"""
DuckDB é€šç”¨è¶‹åŠ¿åˆ†ææ–¹æ³•ï¼ˆé‡æ„ç‰ˆï¼‰
=================================

æä¾›ç‹¬ç«‹çš„ã€å¯å¤ç”¨çš„è¶‹åŠ¿åˆ†ææ–¹æ³•,æ”¯æŒå¯¹ä»»æ„æŒ‡æ ‡(ROICã€ROEã€ROAç­‰)è¿›è¡Œ:
1. åŠ æƒå¹³å‡è®¡ç®—
2. çº¿æ€§å›å½’è¶‹åŠ¿åˆ†æ
3. è¶‹åŠ¿è¿‡æ»¤å’Œè¯„åˆ†è°ƒæ•´
4. è¡Œä¸šå·®å¼‚åŒ–å‚æ•°é…ç½®
"""

import sys
from pathlib import Path
import math
import pandas as pd
import logging
from typing import Union, List, Optional

# orchestrator å·²ç§»è‡³æ ¹ç›®å½•
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent.parent))
# from orchestrator import register_method  <-- Removed
from .duckdb_core import _q, _get_duckdb_module, _init_duckdb_and_source  # ä½¿ç”¨æ–°çš„åˆå¹¶æ¨¡å—
from ..trend.config import (
    INDUSTRY_FILTER_CONFIGS,
    DEFAULT_FILTER_CONFIG,
    ROIIC_INDUSTRY_FILTER_CONFIGS,
    DEFAULT_ROIIC_FILTER_CONFIG,
    get_industry_category,
    get_default_config,
)
from ..trend.trend_analyzer import TrendAnalyzer
from ..trend.trend_settings import TrendAnalyzerConfig
from ..trend.trend_components import (
    ConfigResolver,
    TrendEvaluationResult,
    TrendResultCollector,
    TrendRuleEvaluator,
)
# ğŸ”Œ å¯¼å…¥æ’ä»¶åŒ–æ´¾ç”Ÿå™¨ç³»ç»Ÿ
from ..trend.derivers import find_deriver, list_available_metrics, check_derivable

logger = logging.getLogger(__name__)


def _describe_roiic_spread(spread: float) -> str:
    if not math.isfinite(spread):
        return ""

    if spread >= 10.0:
        return f"ROIIC Spread {spread:.1f}ppï¼šæ‰©å¼ åˆ›é€ å¤§é‡ä»·å€¼"
    if spread >= 3.0:
        return f"ROIIC Spread {spread:.1f}ppï¼šæ‰©å¼ åˆ›é€ ä»·å€¼"
    if spread >= 0.0:
        return f"ROIIC Spread {spread:.1f}ppï¼šåˆšå¥½è¦†ç›–èµ„æœ¬æˆæœ¬"
    if spread > -5.0:
        return f"ROIIC Spread {spread:.1f}ppï¼šæ‰©å¼ å›æŠ¥åå¼±"
    return f"ROIIC Spread {spread:.1f}ppï¼šæ‰©å¼ å¯èƒ½æ¯ç­ä»·å€¼"


def analyze_metric_trend(
    data: Union[str, Path, pd.DataFrame],
    group_cols: Union[str, List[str]],
    metric_name: str,
    prefix: str = "",
    suffix: str = "_trend",
    min_periods: int = 5,
    analyzer_config: Optional[TrendAnalyzerConfig] = None,
) -> pd.DataFrame:
    """
    å¯¹æŒ‡å®šæŒ‡æ ‡è¿›è¡Œé€šç”¨è¶‹åŠ¿åˆ†æ

    æ ¸å¿ƒåŠŸèƒ½:
    1. è®¡ç®—åŠ æƒå¹³å‡(æœ€è¿‘æ•°æ®æƒé‡æ›´é«˜)
    2. çº¿æ€§å›å½’åˆ†æ(æ–œç‡ã€RÂ²ã€på€¼)
    3. è¶‹åŠ¿è¿‡æ»¤(å¯é€‰,æ ¹æ®é…ç½®)
    4. è¯„åˆ†è°ƒæ•´(å¯é€‰,æ ¹æ®é…ç½®)

    Args:
        data: è¾“å…¥æ•°æ®(å¿…é¡»åŒ…å«å¤šæœŸæ•°æ®,æŒ‰æ—¶é—´æ’åº)
        group_cols: åˆ†ç»„åˆ—(å¦‚ 'ts_code', 'industry')
        metric_name: è¦åˆ†æçš„æŒ‡æ ‡å(å¦‚ 'roic', 'roe', 'roa')
        prefix: è¾“å‡ºåˆ—åå‰ç¼€(é»˜è®¤ç©º)
        suffix: è¾“å‡ºåˆ—ååç¼€(é»˜è®¤ '_trend')
        min_periods: æœ€å°‘éœ€è¦çš„æœŸæ•°(é»˜è®¤5)
        analyzer_config: è¶‹åŠ¿åˆ†æå™¨é…ç½®(çª—å£ã€æƒé‡ã€æ¢é’ˆã€å‚è€ƒæŒ‡æ ‡ç­‰)

    Returns:
        DataFrame,åŒ…å«:
        - åŸåˆ†ç»„åˆ—
        - {prefix}{metric_name}_weighted{suffix}: åŠ æƒå¹³å‡å€¼
        - {prefix}{metric_name}_slope{suffix}: è¶‹åŠ¿æ–œç‡
        - {prefix}{metric_name}_r_squared{suffix}: RÂ²
        - {prefix}{metric_name}_latest{suffix}: æœ€æ–°æœŸå€¼
        - {prefix}{metric_name}_penalty{suffix}: æ‰£åˆ†(å¦‚æœå¯ç”¨è¿‡æ»¤)

    Example:
    >>> # åˆ†æROICè¶‹åŠ¿(ä½¿ç”¨é…ç½®ä¸­å¿ƒå‚æ•°)
    >>> df_roic = analyze_metric_trend(
    ...     data='data/polars/5yd_final_industry.csv',
    ...     group_cols='ts_code',
    ...     metric_name='roic'
    ... )
    """

    logger.info("=" * 80)
    logger.info(f"ğŸ” é€šç”¨è¶‹åŠ¿åˆ†æå¯åŠ¨: {metric_name}")
    logger.info("=" * 80)

    # ========== 1. åŠ è½½æ•°æ® ==========
    con, source_sql = _init_duckdb_and_source(data)

    # æ ‡å‡†åŒ–åˆ†ç»„åˆ—
    group_cols_list = [group_cols] if isinstance(group_cols, str) else list(group_cols)

    # æ£€æŸ¥æŒ‡æ ‡åˆ—æ˜¯å¦å­˜åœ¨
    cols_info = con.execute(f"DESCRIBE SELECT * FROM {source_sql}").df()
    all_cols = cols_info['column_name'].tolist()

    # ğŸ”Œ æ’ä»¶åŒ–æŒ‡æ ‡æ´¾ç”Ÿç³»ç»Ÿ
    if metric_name not in all_cols:
        # å°è¯•ä½¿ç”¨æ’ä»¶æ´¾ç”ŸæŒ‡æ ‡
        deriver = find_deriver(metric_name, set(all_cols))

        if deriver:
            logger.info(f"ğŸ”Œ ä½¿ç”¨æ’ä»¶ {deriver.__class__.__name__} æ´¾ç”Ÿ {metric_name}")
            source_sql = deriver.derive(con, source_sql, group_cols_list[0])

            # åˆ·æ–°åˆ—ä¿¡æ¯
            cols_info = con.execute(f"DESCRIBE SELECT * FROM {source_sql}").df()
            all_cols = cols_info['column_name'].tolist()

        # æœ€ç»ˆæ£€æŸ¥ï¼šå¦‚æœä»ç„¶ä¸å­˜åœ¨ï¼Œæä¾›è¯¦ç»†é”™è¯¯
        if metric_name not in all_cols:
            # ä½¿ç”¨ check_derivable è·å–è¯¦ç»†ä¿¡æ¯
            can_derive, missing = check_derivable(metric_name, set(all_cols))

            if missing:
                raise ValueError(
                    f"âŒ æŒ‡æ ‡ '{metric_name}' æ— æ³•æ´¾ç”Ÿï¼Œç¼ºå°‘å¿…éœ€åˆ—: {', '.join(sorted(missing))}\n"
                    f"å½“å‰å¯ç”¨åˆ—: {', '.join(sorted(all_cols))}"
                )
            else:
                available = list_available_metrics()
                raise ValueError(
                    f"âŒ æŒ‡æ ‡ '{metric_name}' ä¸å­˜åœ¨ä¸”æ— å¯ç”¨æ´¾ç”Ÿå™¨ã€‚\n"
                    f"å¯æ´¾ç”ŸæŒ‡æ ‡: {', '.join(available)}\n"
                    f"å½“å‰å¯ç”¨åˆ—: {', '.join(sorted(all_cols))}"
                )

    logger.info(f"åˆ†ç»„åˆ—: {group_cols_list}")
    logger.info(f"åˆ†ææŒ‡æ ‡: {metric_name}")
    _trend_config = get_default_config()
    logger.info(f"åŠ æƒæ–¹æ¡ˆ: {_trend_config.default_weights.tolist()}")

    metric_lower = metric_name.lower()
    default_config = DEFAULT_ROIIC_FILTER_CONFIG if metric_lower == "roiic" else DEFAULT_FILTER_CONFIG
    industry_configs = ROIIC_INDUSTRY_FILTER_CONFIGS if metric_lower == "roiic" else INDUSTRY_FILTER_CONFIGS

    # ========== 2. è§£æè¿‡æ»¤é…ç½® ==========
    base_config = {"enable_filter": True}
    base_config.update(default_config)
    logger.info(f"è¿‡æ»¤åŸºçº¿é…ç½®(é»˜è®¤é˜ˆå€¼): {base_config}")

    # ========== 3. è¯»å–æ•°æ®å¹¶æ’åº ==========
    # æ£€æŸ¥æ˜¯å¦æœ‰ name å’Œ industry åˆ—ï¼ˆç”¨äºè¾“å‡ºï¼‰
    keep_cols = []
    if 'name' in all_cols:
        keep_cols.append('name')
    if 'industry' in all_cols:
        keep_cols.append('industry')

    # æ„å»ºSELECTåˆ—è¡¨
    select_cols = [_q(group_cols_list[0]), _q(metric_name), 'end_date']
    if keep_cols:
        select_cols.extend([_q(col) for col in keep_cols])

    sql_load = f"""
        SELECT {', '.join(select_cols)}
        FROM {source_sql}
        ORDER BY {_q(group_cols_list[0])}, end_date ASC
    """

    df_full = con.execute(sql_load).df()
    logger.info(f"è¾“å…¥æ•°æ®: {len(df_full)} è¡Œ")
    if keep_cols:
        logger.info(f"ä¿ç•™é¢å¤–åˆ—: {keep_cols}")

    # ========== 4. åˆ†ç»„å¤„ç† ==========
    eliminated_count = 0
    config_resolver = ConfigResolver(industry_configs)
    rule_evaluator = TrendRuleEvaluator(logger)
    result_collector = TrendResultCollector()

    grouped = df_full.groupby(group_cols_list[0])

    for group_key, group_df in grouped:
        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        if len(group_df) < min_periods:
            logger.debug(f"è·³è¿‡ {group_key}: æ•°æ®ä¸è¶³{min_periods}æœŸ(å®é™…{len(group_df)}æœŸ)")
            continue

        # ========== æ ¹æ®è¡Œä¸šåŠ¨æ€è°ƒæ•´è¿‡æ»¤å‚æ•° ==========
        current_config, _ = config_resolver.resolve(group_key, base_config, group_df, logger)

        analyzer = TrendAnalyzer(
            group_key=group_key,
            group_df=group_df,
            metric_name=metric_name,
            group_column=group_cols_list[0],
            prefix=prefix,
            suffix=suffix,
            keep_cols=keep_cols,
            logger=logger,
            config=analyzer_config,
        )

        if not analyzer.valid:
            logger.debug(f"è·³è¿‡ {group_key}: {analyzer.error_reason}")
            continue

        trend_vector = analyzer.build_trend_vector()

        if current_config.get('enable_filter'):
            evaluation = rule_evaluator.evaluate(group_key, metric_name, current_config, trend_vector)

            if not evaluation.passes:
                eliminated_count += 1
                continue
        else:
            evaluation = TrendEvaluationResult(
                passes=True,
                elimination_reason="",
                penalty=0.0,
                penalty_details=[],
                bonus_details=[],
                trend_score=100.0,
                auxiliary_notes=[],
            )

        snapshot = analyzer.build_snapshot(evaluation, trend_vector)
        result_row = analyzer.build_result_row(snapshot, current_config.get('enable_filter', False))
        result_collector.add(result_row)

    # ========== 10. æ„å»ºè¾“å‡º DataFrame ==========
    df_result = result_collector.to_dataframe()

    logger.info("\n" + "=" * 80)
    logger.info(f"ğŸ“Š {metric_name} è¶‹åŠ¿åˆ†æå®Œæˆ")
    logger.info("=" * 80)
    logger.info(f"è¾“å…¥åˆ†ç»„æ•°: {grouped.ngroups}")
    logger.info(f"è¾“å‡ºåˆ†ç»„æ•°: {len(df_result)}")

    if base_config.get('enable_filter'):
        logger.info(f"è¿‡æ»¤æ·˜æ±°: {eliminated_count} ç»„")

        # è¡Œä¸šé…ç½®ä½¿ç”¨ç»Ÿè®¡
        usage_stats = config_resolver.usage_stats()
        if usage_stats:
            logger.info(f"\nğŸ­ è¡Œä¸šå·®å¼‚åŒ–å‚æ•°åº”ç”¨:")
            for industry, count in sorted(usage_stats.items(), key=lambda x: -x[1])[:10]:
                ind_config = industry_configs.get(industry, default_config)
                slope_param = ind_config.get('log_severe_decline_slope', ind_config.get('severe_decline_slope', default_config.get('log_severe_decline_slope', -0.30)))
                min_value = ind_config.get('min_latest_value', default_config.get('min_latest_value'))
                logger.info(f"  {industry}: {count}å®¶ (min={min_value}, log_slope={slope_param:.2f})")

    if len(df_result) > 0:
        logger.info("\nğŸ“Š v2.0 è¶‹åŠ¿ç»Ÿè®¡ (Logæ–œç‡):")
        logger.info(f"  å¹³å‡åŠ æƒå€¼:   {df_result[f'{prefix}{metric_name}_weighted{suffix}'].mean():.2f}")
        logger.info(f"  å¹³å‡Logæ–œç‡:  {df_result[f'{prefix}{metric_name}_log_slope{suffix}'].mean():.4f} (CAGR: {df_result[f'{prefix}{metric_name}_cagr{suffix}'].mean()*100:.1f}%)")
        logger.info(f"  å¹³å‡çº¿æ€§æ–œç‡: {df_result[f'{prefix}{metric_name}_slope{suffix}'].mean():.2f} (å¯¹ç…§)")
        logger.info(f"  å¹³å‡RÂ²:       {df_result[f'{prefix}{metric_name}_r_squared{suffix}'].mean():.2f}")

        score_col = f"{prefix}{metric_name}_trend_score{suffix}"
        if score_col in df_result.columns:
            logger.info(f"  å¹³å‡è¶‹åŠ¿è¯„åˆ†: {df_result[score_col].mean():.1f}")

        # æ”¹å–„vsè¡°é€€ (ä½¿ç”¨Logæ–œç‡)
        log_slope_col = f"{prefix}{metric_name}_log_slope{suffix}"
        improving = (df_result[log_slope_col] > 0.10).sum()   # CAGR >10%
        declining = (df_result[log_slope_col] < -0.10).sum()  # CAGR <-10%
        stable = len(df_result) - improving - declining

        logger.info(f"\n  æ”¹å–„è¶‹åŠ¿(æ–œç‡>+1): {improving} ({improving/len(df_result)*100:.1f}%)")
        logger.info(f"  ç¨³å®šè¶‹åŠ¿(æ–œç‡Â±1):  {stable} ({stable/len(df_result)*100:.1f}%)")
        logger.info(f"  ä¸‹æ»‘è¶‹åŠ¿(æ–œç‡<-1): {declining} ({declining/len(df_result)*100:.1f}%)")

        # æ‰£åˆ†ç»Ÿè®¡
    if base_config.get('enable_filter'):
            penalty_col = f"{prefix}{metric_name}_penalty{suffix}"
            penalized = (df_result[penalty_col] > 0).sum()
            if penalized > 0:
                logger.info(f"\n  è¢«æ‰£åˆ†: {penalized} ç»„")
                logger.info(f"  å¹³å‡æ‰£åˆ†: {df_result[df_result[penalty_col]>0][penalty_col].mean():.1f}åˆ†")

    logger.info("=" * 80)

    return df_result


if __name__ == "__main__":
    # æµ‹è¯•é€šç”¨è¶‹åŠ¿åˆ†æ
    import sys
    sys.path.append('src')

    # æµ‹è¯•: ROICè¶‹åŠ¿åˆ†æ
    print("\n" + "=" * 80)
    print("æµ‹è¯•: ROICè¶‹åŠ¿åˆ†æ")
    print("=" * 80)

    df_roic = analyze_metric_trend(
        data='data/polars/5yd_final_industry.csv',
        group_cols='ts_code',
        metric_name='roic',
        prefix='',
        suffix='',
        min_periods=5,
    )

    print("\nROICè¶‹åŠ¿åˆ†æç»“æœ(å‰10):")
    print(df_roic.head(10))
    print(f"\nè¾“å‡ºåˆ—: {df_roic.columns.tolist()}")
