"""
DuckDBç­›é€‰å¼•æ“ v2.0 Enhanced
ç»“åˆv2.0çš„æœ‰æ•ˆæ€§å’Œv3.0çš„ä¼˜ç§€è®¾è®¡

æ ¸å¿ƒæ”¹è¿›ï¼š
1. å¼•å…¥è´¢åŠ¡å®‰å…¨åº•çº¿ç­›é€‰ï¼ˆä¸€ç¥¨å¦å†³ï¼‰
2. å‡çº§å•†ä¸šæ¨¡å¼åˆ†ç±»ï¼ˆ5ç±»ï¼Œå¤šç»´åº¦ï¼‰
3. å¼•å…¥è¡Œä¸šç›¸å¯¹å€¼è¯„åˆ†
4. å®ç°å·®å¼‚åŒ–æƒé‡è°ƒæ•´
5. ä¿ç•™v2çš„ä¸‰å±‚æ¶æ„å’Œä¾‹å¤–è§„åˆ™
"""

import pandas as pd
import logging
import sys
from pathlib import Path
from typing import Union, Dict, List

# orchestrator å·²ç§»è‡³æ ¹ç›®å½•
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent.parent))
from orchestrator.decorators.register import register_method

from .industry_config import (
    UNIVERSAL_RULES_ENHANCED,
    BUSINESS_MODEL_CLASSIFICATION_ENHANCED,
    EXCEPTION_RULES_ENHANCED,
    classify_business_model_enhanced,
    get_company_config_enhanced,
    calculate_quality_score_enhanced,
)

logger = logging.getLogger(__name__)


@register_method(
    engine_name="filter_second_stage_v2",
    component_type="business_engine",
    engine_type="duckdb_v2",
    description="v2.0 ä¸“ä¸šæŠ•èµ„ç­›é€‰ä½“ç³» - 4å±‚æ¶æ„+5ç±»å•†ä¸šæ¨¡å¼+å·®å¼‚åŒ–è¯„åˆ†"
)
def filter_second_stage_v2(
    first_stage_data: Union[str, Path, pd.DataFrame],
    full_data: Union[str, Path, pd.DataFrame] = None,
    latest_end_date: str = "20241231",
    end_date_col: str = "end_date",
) -> pd.DataFrame:
    """
    v2.0 ä¸“ä¸šæŠ•èµ„ç­›é€‰ä½“ç³»

    å››å±‚æ¶æ„ï¼š
    0. è´¢åŠ¡å®‰å…¨åº•çº¿ï¼ˆä¸€ç¥¨å¦å†³ï¼‰- æ–°å¢
    1. é€šç”¨é“å¾‹ï¼ˆè´¨é‡ä¸‹é™ï¼‰
    2. å•†ä¸šæ¨¡å¼åˆ†ç±»ï¼ˆ5ç±»ï¼šè§„æ¨¡/æŠ€æœ¯/å“ç‰Œ/æœåŠ¡/å‘¨æœŸï¼‰- å‡çº§
    3. å·®å¼‚åŒ–ç­›é€‰æ ‡å‡† + ä¾‹å¤–è§„åˆ™

    å‚æ•°:
        first_stage_data: ç¬¬ä¸€æ¬¡ç­›é€‰ç»“æœï¼ˆåŒ…å«5å¹´å¹³å‡å€¼ï¼‰
        full_data: å®Œæ•´çš„5å¹´æ•°æ®ï¼ˆç”¨äºè·å–æœ€æ–°ä¸€æœŸæ•°æ®ï¼Œå¯é€‰ï¼‰
        latest_end_date: æœ€æ–°ä¸€æœŸçš„æ—¥æœŸï¼ˆå¦‚ï¼š20241231ï¼‰
        end_date_col: æ—¥æœŸåˆ—åï¼ˆé»˜è®¤ï¼šend_dateï¼‰

    è¿”å›:
        æœ€ç»ˆç­›é€‰ç»“æœçš„DataFrame
    """
    # ========== æ•°æ®åŠ è½½ ==========
    if isinstance(first_stage_data, (str, Path)):
        df_first = pd.read_csv(first_stage_data)
    else:
        df_first = first_stage_data.copy() if isinstance(first_stage_data, pd.DataFrame) else first_stage_data

    logger.info("=" * 80)
    logger.info("ğŸš€ ä¸“ä¸šæŠ•èµ„ç­›é€‰ä½“ç³» v2.0 Enhanced å¯åŠ¨")
    logger.info("=" * 80)
    logger.info(f"è¾“å…¥ä¼ä¸šæ•°: {len(df_first)}")
    logger.info("")
    logger.info("æ”¹è¿›ç‚¹:")
    logger.info("  âœ… è´¢åŠ¡å®‰å…¨åº•çº¿ç­›é€‰ï¼ˆä¸€ç¥¨å¦å†³ï¼‰")
    logger.info("  âœ… å•†ä¸šæ¨¡å¼åˆ†ç±»å‡çº§ï¼ˆ5ç±»ï¼‰")
    logger.info("  âœ… è¡Œä¸šç›¸å¯¹å€¼è¯„åˆ†")
    logger.info("  âœ… å·®å¼‚åŒ–æƒé‡è°ƒæ•´")

    # ========== Mergeæœ€æ–°æœŸæ•°æ® ==========
    # first_stage_dataåªæœ‰èšåˆåˆ—ï¼ˆ5yd_ts_code_*ï¼‰ï¼Œéœ€è¦mergeåŸå§‹æ•°æ®è·å–æœ€æ–°æœŸæŒ‡æ ‡
    if full_data is not None:
        logger.info("\nğŸ“Š åˆå¹¶æœ€æ–°æœŸæ•°æ®...")

        # åŠ è½½å®Œæ•´æ•°æ®
        if isinstance(full_data, (str, Path)):
            df_full = pd.read_csv(full_data)
        else:
            df_full = full_data.copy() if isinstance(full_data, pd.DataFrame) else full_data

        # è·å–æœ€æ–°æœŸæ•°æ®ï¼ˆæŒ‰end_dateæ’åºï¼‰
        if end_date_col in df_full.columns:
            df_full[end_date_col] = pd.to_datetime(df_full[end_date_col], format='%Y%m%d', errors='coerce')
            # æ¯ä¸ªts_codeå–æœ€æ–°ä¸€æœŸ
            df_latest = df_full.sort_values(end_date_col).groupby('ts_code', as_index=False).last()
            logger.info(f"   æå–æœ€æ–°æœŸæ•°æ®: {len(df_latest)}è¡Œ")
        else:
            # å¦‚æœæ²¡æœ‰æ—¥æœŸåˆ—ï¼Œç›´æ¥å–æ¯ä¸ªts_codeçš„æœ€åä¸€æ¡
            df_latest = df_full.groupby('ts_code', as_index=False).last()
            logger.info(f"   æå–æœ€æ–°æœŸæ•°æ®(æ— æ—¥æœŸåˆ—): {len(df_latest)}è¡Œ")

        # é€‰æ‹©éœ€è¦çš„æœ€æ–°æœŸåˆ—
        latest_cols = ['ts_code', 'name', 'industry', 'roic', 'roe', 'roa', 'debt_to_assets',
                       'current_ratio', 'ocfps', 'eps', 'grossprofit_margin', 'or_yoy',
                       'fixed_assets_ratio', 'total_revenue', 'rd_exp_ratio']
        latest_cols_available = [c for c in latest_cols if c in df_latest.columns]
        df_latest_subset = df_latest[latest_cols_available].copy()

        # Mergeåˆ°first_stage_data
        df_merged = df_first.merge(df_latest_subset, on='ts_code', how='left', suffixes=('', '_latest'))

        # å¤„ç†é‡å¤åˆ—ï¼ˆå¦‚æœæœ‰ï¼‰
        for col in ['name', 'industry']:
            if f'{col}_latest' in df_merged.columns:
                df_merged[col] = df_merged[f'{col}_latest'].fillna(df_merged.get(col, ''))
                df_merged.drop(columns=[f'{col}_latest'], inplace=True)

        logger.info(f"   åˆå¹¶åæ•°æ®: {len(df_merged)}è¡Œ, {len(df_merged.columns)}åˆ—")
        logger.info(f"   âœ… å·²æ·»åŠ æœ€æ–°æœŸæŒ‡æ ‡: {[c for c in latest_cols_available if c not in ['ts_code', 'name', 'industry']]}")
    else:
        logger.warning("   âš ï¸  æœªæä¾›full_dataï¼Œå°†ä½¿ç”¨èšåˆæ•°æ®ä¼°ç®—æœ€æ–°æœŸæŒ‡æ ‡")
        df_merged = df_first.copy()

        # å¦‚æœæ²¡æœ‰æœ€æ–°æœŸæ•°æ®ï¼Œç”¨5å¹´å¹³å‡ä¼°ç®—
        if 'roic' not in df_merged.columns and '5yd_ts_code_roic_avg' in df_merged.columns:
            df_merged['roic'] = df_merged['5yd_ts_code_roic_avg']
        if 'debt_to_assets' not in df_merged.columns:
            df_merged['debt_to_assets'] = 50.0  # é»˜è®¤50%
        if 'current_ratio' not in df_merged.columns:
            df_merged['current_ratio'] = 1.5  # é»˜è®¤1.5

    # è¡¥å……ç¼ºå¤±çš„è´¢åŠ¡ç‰¹å¾å­—æ®µï¼ˆç”¨äºå•†ä¸šæ¨¡å¼åˆ†ç±»ï¼‰
    if 'fixed_assets_ratio' not in df_merged.columns:
        df_merged['fixed_assets_ratio'] = 30.0  # é»˜è®¤å€¼
    if 'total_revenue' not in df_merged.columns:
        df_merged['total_revenue'] = 50.0  # é»˜è®¤50äº¿
    if 'roa' not in df_merged.columns:
        if '5yd_ts_code_roa_avg' in df_merged.columns:
            df_merged['roa'] = df_merged['5yd_ts_code_roa_avg']
        else:
            df_merged['roa'] = df_merged.get('roe', 10) * 0.6  # ä¼°ç®—
    if 'rd_exp_ratio' not in df_merged.columns:
        df_merged['rd_exp_ratio'] = 3.0  # é»˜è®¤ç ”å‘å æ¯”
    if 'ocfps_to_eps_ratio' not in df_merged.columns:
        # è®¡ç®—OCF/EPSæ¯”ç‡
        if 'ocfps' in df_merged.columns and 'eps' in df_merged.columns:
            df_merged['ocfps_to_eps_ratio'] = df_merged['ocfps'] / df_merged['eps'].replace(0, 1)
        elif '5yd_ts_code_ocfps_avg' in df_merged.columns and '5yd_ts_code_eps_avg' in df_merged.columns:
            ocf = df_merged['5yd_ts_code_ocfps_avg']
            eps = df_merged['5yd_ts_code_eps_avg']
            df_merged['ocfps_to_eps_ratio'] = ocf / eps.replace(0, 1)
        else:
            df_merged['ocfps_to_eps_ratio'] = 1.0  # é»˜è®¤å€¼

    current_df = df_merged.copy()
    logger.info(f"\nâœ… æ•°æ®å‡†å¤‡å®Œæˆ: {len(current_df)}è¡Œ, åŒ…å«5å¹´å¹³å‡åˆ—+æœ€æ–°æœŸåˆ—")

    # ========== ç¬¬0å±‚ï¼šè´¢åŠ¡å®‰å…¨åº•çº¿ï¼ˆæ–°å¢ï¼‰==========
    logger.info("")
    logger.info("=" * 80)
    logger.info("ğŸ›¡ï¸  ç¬¬0å±‚ï¼šè´¢åŠ¡å®‰å…¨åº•çº¿ï¼ˆä¸€ç¥¨å¦å†³ï¼‰")
    logger.info("=" * 80)

    safety_baseline = UNIVERSAL_RULES_ENHANCED['financial_safety_baseline']

    if safety_baseline['enabled']:
        before = len(current_df)

        # åº•çº¿1ï¼šæ¯›åˆ©ç‡>10%
        logger.info(f"\nåº•çº¿1: æ¯›åˆ©ç‡ > {safety_baseline['grossprofit_margin_min']}%")
        passed_margin = current_df['5yd_ts_code_grossprofit_margin_avg'] > safety_baseline['grossprofit_margin_min']
        failed = current_df[~passed_margin]
        if len(failed) > 0:
            logger.info(f"  âŒ æ·˜æ±° {len(failed)} å®¶ä¼ä¸šï¼ˆæ— å®šä»·æƒï¼‰")
        current_df = current_df[passed_margin].copy()

        # åº•çº¿2ï¼šè´Ÿå€ºç‡æ£€æŸ¥ï¼ˆéœ€è¦å…ˆåˆ†ç±»ï¼‰
        logger.info(f"\nåº•çº¿2: è´Ÿå€ºç‡æ£€æŸ¥ï¼ˆæŒ‰å•†ä¸šæ¨¡å¼å·®å¼‚åŒ–ï¼‰")

        # å¿«é€Ÿåˆ†ç±»ï¼ˆä»…ç”¨äºåº•çº¿æ£€æŸ¥ï¼‰
        def get_business_type_for_safety(row):
            """å¿«é€Ÿåˆ¤æ–­å•†ä¸šæ¨¡å¼ï¼ˆç”¨äºåº•çº¿æ£€æŸ¥ï¼‰"""
            industry = row.get('industry', '')
            margin = row.get('5yd_ts_code_grossprofit_margin_avg', 0)
            growth = row.get('5yd_ts_code_or_yoy_avg', 0)

            # ç®€åŒ–åˆ¤æ–­é€»è¾‘
            if margin > 60:
                return "å“ç‰Œæº¢ä»·å‹"
            elif growth > 20 and margin > 30:
                return "æŠ€æœ¯å£å’å‹"
            elif growth > 15 and margin > 30:
                return "è½»èµ„äº§æœåŠ¡å‹"
            elif any(ind in industry for ind in ["æ±½è½¦", "ç”µæ°”", "æœºæ¢°", "å…ƒå™¨ä»¶"]):
                return "è§„æ¨¡æ•ˆåº”å‹"
            else:
                return "å‘¨æœŸèµ„æºå‹"

        # ä½¿ç”¨å‘é‡åŒ–æ–¹å¼é¿å…applyé—®é¢˜
        business_types = []
        for _, row in current_df.iterrows():
            business_types.append(get_business_type_for_safety(row))
        current_df['business_type_temp'] = business_types

        # åº”ç”¨å·®å¼‚åŒ–è´Ÿå€ºç‡åº•çº¿
        debt_passed = []
        for idx, row in current_df.iterrows():
            biz_type = row['business_type_temp']
            debt_max = safety_baseline['debt_to_assets_max'].get(biz_type, 65.0)
            actual_debt = row.get('debt_to_assets', 100)
            debt_passed.append(actual_debt <= debt_max)

        current_df['debt_check'] = debt_passed
        failed = current_df[~current_df['debt_check']]
        if len(failed) > 0:
            logger.info(f"  âŒ æ·˜æ±° {len(failed)} å®¶ä¼ä¸šï¼ˆè´Ÿå€ºç‡è¿‡é«˜ï¼‰:")
            for _, row in failed.head(5).iterrows():
                logger.info(f"     â€¢ {row.get('name', row['ts_code'])} ({row['business_type_temp']}): "
                           f"è´Ÿå€ºç‡ {row.get('debt_to_assets', 0):.1f}%")

        current_df = current_df[current_df['debt_check']].copy()
        # å®‰å…¨åˆ é™¤ä¸´æ—¶åˆ—
        cols_to_drop = [c for c in ['business_type_temp', 'debt_check'] if c in current_df.columns]
        if cols_to_drop:
            current_df = current_df.drop(columns=cols_to_drop)

        # åº•çº¿3ï¼šæµåŠ¨æ¯”ç‡æ£€æŸ¥ï¼ˆæŒ‰å•†ä¸šæ¨¡å¼å·®å¼‚åŒ–ï¼‰
        logger.info(f"\nåº•çº¿3: æµåŠ¨æ¯”ç‡æ£€æŸ¥ï¼ˆæŒ‰å•†ä¸šæ¨¡å¼å·®å¼‚åŒ–ï¼‰")

        # ä½¿ç”¨å‘é‡åŒ–æ–¹å¼é¿å…applyé—®é¢˜
        business_types = []
        for _, row in current_df.iterrows():
            business_types.append(get_business_type_for_safety(row))
        current_df['business_type_temp'] = business_types

        # åº”ç”¨å·®å¼‚åŒ–æµåŠ¨æ¯”ç‡åº•çº¿
        cr_passed = []
        for idx, row in current_df.iterrows():
            biz_type = row['business_type_temp']
            cr_min = safety_baseline['current_ratio_min'].get(biz_type, 1.0)
            actual_cr = row.get('current_ratio', 0)
            cr_passed.append(actual_cr >= cr_min)

        current_df['cr_check'] = cr_passed
        failed = current_df[~current_df['cr_check']]
        if len(failed) > 0:
            logger.info(f"  âŒ æ·˜æ±° {len(failed)} å®¶ä¼ä¸šï¼ˆæµåŠ¨æ€§ä¸è¶³ï¼‰:")
            for _, row in failed.head(5).iterrows():
                logger.info(f"     â€¢ {row.get('name', row['ts_code'])} ({row['business_type_temp']}): "
                           f"æµåŠ¨æ¯”ç‡ {row.get('current_ratio', 0):.2f}")

        current_df = current_df[current_df['cr_check']].copy()
        # å®‰å…¨åˆ é™¤ä¸´æ—¶åˆ—
        cols_to_drop = [c for c in ['business_type_temp', 'cr_check'] if c in current_df.columns]
        if cols_to_drop:
            current_df = current_df.drop(columns=cols_to_drop)

        logger.info(f"\nâœ… è´¢åŠ¡å®‰å…¨åº•çº¿ç­›é€‰å®Œæˆ: {len(current_df)} å®¶é€šè¿‡ (æ·˜æ±° {before - len(current_df)} å®¶)")

    step0_count = len(current_df)

    # ========== ç¬¬ä¸€å±‚ï¼šé€šç”¨é“å¾‹ ==========
    logger.info("")
    logger.info("=" * 80)
    logger.info("ğŸ“‹ ç¬¬ä¸€å±‚ï¼šé€šç”¨æŠ•èµ„é“å¾‹ï¼ˆå·´è²ç‰¹æ ¸å¿ƒå‡†åˆ™ï¼‰")
    logger.info("=" * 80)

    # é“å¾‹1ï¼šROICç»å¯¹å€¼ä¸‹é™
    logger.info(f"\né“å¾‹1: ROIC 5å¹´å¹³å‡ > {UNIVERSAL_RULES_ENHANCED['roic_5y_min_absolute']}%")
    before = len(current_df)
    passed_roic = current_df['5yd_ts_code_roic_avg'] > UNIVERSAL_RULES_ENHANCED['roic_5y_min_absolute']
    failed = current_df[~passed_roic]
    if len(failed) > 0:
        logger.info(f"  âŒ æ·˜æ±° {len(failed)} å®¶ä¼ä¸šï¼ˆROICè¿‡ä½ï¼‰")
    current_df = current_df[passed_roic].copy()
    logger.info(f"  âœ… é€šè¿‡: {len(current_df)} å®¶")
    step1_count = len(current_df)

    # é“å¾‹2ï¼šç°é‡‘æµè´¨é‡ï¼ˆå¸¦å¢é•¿æœŸè±å…ï¼‰
    logger.info(f"\né“å¾‹2: ç°é‡‘æµè´¨é‡æ£€æŸ¥ï¼ˆå¸¦å¢é•¿æœŸè±å…ï¼‰")
    logger.info(f"  åŸºå‡†: OCF/EPS > {UNIVERSAL_RULES_ENHANCED['ocfps_to_eps_base'] * 100:.0f}%")

    high_growth_cfg = UNIVERSAL_RULES_ENHANCED['high_growth_exemption']
    if high_growth_cfg['enabled']:
        logger.info(f"  è±å…: å¢é•¿>{high_growth_cfg['growth_threshold']}%æ—¶ï¼ŒOCF/EPS>{high_growth_cfg['ocfps_to_eps_relaxed'] * 100:.0f}%")

    before = len(current_df)
    ocf_passed = []
    high_growth_exempted = 0

    for idx, row in current_df.iterrows():
        ocf_ratio = row.get('ocfps_to_eps_ratio', 0)
        growth = row.get('5yd_ts_code_or_yoy_avg', 0)

        # æ£€æŸ¥æ˜¯å¦é«˜å¢é•¿è±å…
        if high_growth_cfg['enabled'] and growth > high_growth_cfg['growth_threshold']:
            threshold = high_growth_cfg['ocfps_to_eps_relaxed']
            if ocf_ratio >= threshold:
                ocf_passed.append(True)
                high_growth_exempted += 1
            else:
                ocf_passed.append(False)
        else:
            threshold = UNIVERSAL_RULES_ENHANCED['ocfps_to_eps_base']
            ocf_passed.append(ocf_ratio >= threshold)

    current_df['ocf_check'] = ocf_passed
    failed = current_df[~current_df['ocf_check']]
    if len(failed) > 0:
        logger.info(f"  âŒ æ·˜æ±° {len(failed)} å®¶ä¼ä¸šï¼ˆç°é‡‘æµè´¨é‡å·®ï¼‰")

    current_df = current_df[current_df['ocf_check']].copy()
    current_df = current_df.drop(columns=['ocf_check'])

    if high_growth_exempted > 0:
        logger.info(f"  ğŸ’¡ é«˜å¢é•¿è±å…: {high_growth_exempted} å®¶ä¼ä¸š")

    logger.info(f"  âœ… é€šè¿‡: {len(current_df)} å®¶")
    step2_count = len(current_df)

    # ========== ç¬¬äºŒå±‚ï¼šå•†ä¸šæ¨¡å¼åˆ†ç±»ï¼ˆå‡çº§ç‰ˆï¼š5ç±»ï¼‰==========
    logger.info("")
    logger.info("=" * 80)
    logger.info("ğŸ“Š ç¬¬äºŒå±‚ï¼šå•†ä¸šæ¨¡å¼è‡ªåŠ¨åˆ†ç±»ï¼ˆå‡çº§ç‰ˆï¼š5ç±»ï¼‰")
    logger.info("=" * 80)

    # åº”ç”¨å¢å¼ºç‰ˆåˆ†ç±»é€»è¾‘
    classifications = []
    for idx, row in current_df.iterrows():
        category, standards = classify_business_model_enhanced(row.to_dict())
        classifications.append({
            'ts_code': row['ts_code'],
            'category': category,
            'category_name': BUSINESS_MODEL_CLASSIFICATION_ENHANCED[category]['description']
        })

    df_class = pd.DataFrame(classifications)
    current_df = current_df.merge(df_class, on='ts_code', how='left')

    # ç»Ÿè®¡å„ç±»åˆ«ä¼ä¸šæ•°
    logger.info("\nä¼ä¸šåˆ†ç±»ç»“æœ:")
    for cat in ["è§„æ¨¡æ•ˆåº”å‹", "æŠ€æœ¯å£å’å‹", "å“ç‰Œæº¢ä»·å‹", "è½»èµ„äº§æœåŠ¡å‹", "å‘¨æœŸèµ„æºå‹"]:
        cat_df = current_df[current_df['category'] == cat]
        if len(cat_df) > 0:
            cat_name = BUSINESS_MODEL_CLASSIFICATION_ENHANCED[cat]['description']
            logger.info(f"\n  ã€{cat}ã€‘{cat_name}: {len(cat_df)}å®¶")

            # æ˜¾ç¤ºè¡Œä¸šåˆ†å¸ƒ
            industry_counts = cat_df['industry'].value_counts()
            for industry, count in industry_counts.head(3).items():
                logger.info(f"     â€¢ {industry}: {count}å®¶")

            # æ˜¾ç¤ºå…¸å‹ä¼ä¸š
            top_companies = cat_df.nlargest(3, '5yd_ts_code_roic_avg')
            logger.info(f"     å…¸å‹ä¼ä¸š:")
            for _, row in top_companies.iterrows():
                logger.info(f"       - {row.get('name', row['ts_code'])}: "
                           f"ROIC {row.get('5yd_ts_code_roic_avg', 0):.1f}%, "
                           f"å¢é•¿ {row.get('5yd_ts_code_or_yoy_avg', 0):.1f}%")

    step3_count = len(current_df)

    # ========== ç¬¬ä¸‰å±‚ï¼šå·®å¼‚åŒ–ç­›é€‰ ==========
    logger.info("")
    logger.info("=" * 80)
    logger.info("ğŸ¯ ç¬¬ä¸‰å±‚ï¼šå·®å¼‚åŒ–ç­›é€‰æ ‡å‡†ï¼ˆåŸºäºå•†ä¸šæ¨¡å¼ï¼‰")
    logger.info("=" * 80)

    filtered_rows = []
    exemption_logs = []

    for idx, row in current_df.iterrows():
        config_result = get_company_config_enhanced(row.to_dict())
        category = config_result['category']
        standards = config_result['standards']
        exemptions = config_result['exemptions']

        company_name = row.get('name', row['ts_code'])

        # æ£€æŸ¥å„é¡¹æ ‡å‡†ï¼ˆä¸v2ç›¸åŒçš„é€»è¾‘ï¼‰
        checks = []

        # 1. ROIC 5å¹´å¹³å‡
        roic_5y = row.get('5yd_ts_code_roic_avg', 0)
        check_roic_min = roic_5y >= standards['roic_5y_min']
        checks.append(('ROIC_5y', check_roic_min, f"{roic_5y:.1f}% >= {standards['roic_5y_min']}%"))

        # 2. ROICç¨³å®šæ€§
        roic_current = row.get('roic', 0)
        if roic_5y > 0:
            decline_pct = ((roic_5y - roic_current) / roic_5y * 100)
        else:
            decline_pct = 0

        if 'roic_decline' in exemptions:
            check_roic_stability = True
            checks.append(('ROICç¨³å®šæ€§', True, f"ä¸‹æ»‘{decline_pct:.1f}% (è±å…)"))
        else:
            check_roic_stability = decline_pct <= standards['roic_decline_max_pct']
            checks.append(('ROICç¨³å®šæ€§', check_roic_stability,
                          f"ä¸‹æ»‘{decline_pct:.1f}% <= {standards['roic_decline_max_pct']}%"))

        # 3. æ¯›åˆ©ç‡
        margin_5y = row.get('5yd_ts_code_grossprofit_margin_avg', 0)
        check_margin = margin_5y >= standards['margin_5y_min']
        checks.append(('æ¯›åˆ©ç‡', check_margin, f"{margin_5y:.1f}% >= {standards['margin_5y_min']}%"))

        # 4. å¢é•¿ç‡
        growth_5y = row.get('5yd_ts_code_or_yoy_avg', 0)
        if 'growth_rate' in exemptions:
            check_growth = True
            checks.append(('å¢é•¿ç‡', True, f"{growth_5y:.1f}% (è±å…)"))
        else:
            check_growth = growth_5y >= standards['growth_5y_min']
            checks.append(('å¢é•¿ç‡', check_growth, f"{growth_5y:.1f}% >= {standards['growth_5y_min']}%"))

        # 5. è´Ÿå€ºç‡ï¼ˆå·²åœ¨åº•çº¿æ£€æŸ¥ï¼‰
        debt = row.get('debt_to_assets', 100)
        if 'debt_to_assets' in exemptions:
            check_debt = True
            checks.append(('è´Ÿå€ºç‡', True, f"{debt:.1f}% (è±å…)"))
        else:
            check_debt = debt <= standards['debt_to_assets_max']
            checks.append(('è´Ÿå€ºç‡', check_debt, f"{debt:.1f}% <= {standards['debt_to_assets_max']}%"))

        # 6. æµåŠ¨æ¯”ç‡ï¼ˆå·²åœ¨åº•çº¿æ£€æŸ¥ï¼‰
        current_ratio = row.get('current_ratio', 0)
        if 'current_ratio' in exemptions:
            check_cr = True
            checks.append(('æµåŠ¨æ¯”ç‡', True, f"{current_ratio:.2f} (è±å…)"))
        else:
            check_cr = current_ratio >= standards['current_ratio_min']
            checks.append(('æµåŠ¨æ¯”ç‡', check_cr, f"{current_ratio:.2f} >= {standards['current_ratio_min']}"))

        # åˆ¤æ–­æ˜¯å¦é€šè¿‡
        all_passed = all([c[1] for c in checks])

        if all_passed:
            filtered_rows.append(row)

            if len(exemptions) > 0:
                exemption_logs.append({
                    'name': company_name,
                    'category': category,
                    'exemptions': exemptions,
                })

    final_df = pd.DataFrame(filtered_rows)
    logger.info(f"\nâœ… å·®å¼‚åŒ–ç­›é€‰å®Œæˆ: {len(final_df)} å®¶ä¼ä¸šé€šè¿‡")

    # æ˜¾ç¤ºä¾‹å¤–è±å…ä¼ä¸š
    if len(exemption_logs) > 0:
        logger.info(f"\nğŸ’¡ è§¦å‘ä¾‹å¤–è§„åˆ™çš„ä¼ä¸š ({len(exemption_logs)}å®¶):")
        for log in exemption_logs[:5]:
            logger.info(f"  â€¢ {log['name']} ({log['category']})")
            logger.info(f"    è±å…: {', '.join(log['exemptions'])}")

    # ========== è´¨é‡è¯„åˆ†ï¼ˆå¢å¼ºç‰ˆï¼‰==========
    logger.info("")
    logger.info("=" * 80)
    logger.info("â­ è´¨é‡è¯„åˆ†ä½“ç³»ï¼ˆå¢å¼ºç‰ˆï¼šè¡Œä¸šç›¸å¯¹å€¼+å·®å¼‚åŒ–æƒé‡ï¼‰")
    logger.info("=" * 80)

    # è®¡ç®—è¡Œä¸šç»Ÿè®¡ï¼ˆç”¨äºç›¸å¯¹æ’åï¼‰
    industry_stats = {}
    for industry in final_df['industry'].unique():
        industry_df = final_df[final_df['industry'] == industry]
        margins = industry_df['5yd_ts_code_grossprofit_margin_avg'].values

        for idx, row in industry_df.iterrows():
            margin = row['5yd_ts_code_grossprofit_margin_avg']
            percentile = (margins < margin).sum() / len(margins) * 100
            if industry not in industry_stats:
                industry_stats[industry] = {}
            industry_stats[industry]['margin_percentile'] = percentile

    # è®¡ç®—è´¨é‡è¯„åˆ†
    final_df['quality_score'] = final_df.apply(
        lambda row: calculate_quality_score_enhanced(row.to_dict(), industry_stats),
        axis=1
    )
    final_df = final_df.sort_values('quality_score', ascending=False)

    # ç»Ÿè®¡è¯„åˆ†åˆ†å¸ƒ
    score_ranges = [
        (90, 100, "å“è¶Š"),
        (80, 90, "ä¼˜ç§€"),
        (70, 80, "è‰¯å¥½"),
        (60, 70, "åˆæ ¼"),
        (0, 60, "ä¸€èˆ¬"),
    ]

    logger.info("\nè¯„åˆ†åˆ†å¸ƒ:")
    for min_score, max_score, label in score_ranges:
        count = len(final_df[(final_df['quality_score'] >= min_score) & (final_df['quality_score'] < max_score)])
        if count > 0:
            logger.info(f"  {label} ({min_score}-{max_score}åˆ†): {count}å®¶")

    # Top 10ä¼ä¸š
    logger.info(f"\nğŸ† Top 10 é«˜è´¨é‡ä¼ä¸š:")
    for i, (_, row) in enumerate(final_df.head(10).iterrows(), 1):
        logger.info(f"  {i}. {row.get('name', row['ts_code'])} ({row['category']})")
        logger.info(f"     è¯„åˆ†: {row['quality_score']:.0f}åˆ† | "
                   f"ROIC: {row.get('5yd_ts_code_roic_avg', 0):.1f}% | "
                   f"å¢é•¿: {row.get('5yd_ts_code_or_yoy_avg', 0):.1f}% | "
                   f"æ¯›åˆ©: {row.get('5yd_ts_code_grossprofit_margin_avg', 0):.1f}%")

    # ========== æœ€ç»ˆæ€»ç»“ ==========
    logger.info("")
    logger.info("=" * 80)
    logger.info("ğŸ“Š ç­›é€‰æ€»ç»“")
    logger.info("=" * 80)
    logger.info(f"è¾“å…¥ä¼ä¸š: {len(df_first)}")
    logger.info(f"é€šè¿‡åº•çº¿0 (è´¢åŠ¡å®‰å…¨): {step0_count}")
    logger.info(f"é€šè¿‡é“å¾‹1 (ROIC): {step1_count}")
    logger.info(f"é€šè¿‡é“å¾‹2 (ç°é‡‘æµ): {step2_count}")
    logger.info(f"åˆ†ç±»å®Œæˆ: {step3_count}")
    logger.info(f"æœ€ç»ˆé€šè¿‡: {len(final_df)}")
    logger.info(f"æ€»æ·˜æ±°ç‡: {(1 - len(final_df) / len(df_first)) * 100:.1f}%")

    # æŒ‰ç±»åˆ«ç»Ÿè®¡
    logger.info("\nåˆ†ç±»åˆ«ç»Ÿè®¡:")
    for cat in ["è§„æ¨¡æ•ˆåº”å‹", "æŠ€æœ¯å£å’å‹", "å“ç‰Œæº¢ä»·å‹", "è½»èµ„äº§æœåŠ¡å‹", "å‘¨æœŸèµ„æºå‹"]:
        cat_df = final_df[final_df['category'] == cat]
        if len(cat_df) > 0:
            avg_score = cat_df['quality_score'].mean()
            avg_roic = cat_df['5yd_ts_code_roic_avg'].mean()
            logger.info(f"  {cat}: {len(cat_df)}å®¶ (å¹³å‡åˆ†: {avg_score:.1f}, å¹³å‡ROIC: {avg_roic:.1f}%)")

    logger.info("=" * 80)

    # ç›´æ¥è¿”å›DataFrameï¼Œä¸ä¿å­˜æ–‡ä»¶ï¼ˆç”±pipelineçš„store_dataæ­¥éª¤ç»Ÿä¸€å¤„ç†ï¼‰
    return final_df


# ============================================================================
# å¯¹æ¯”åˆ†æå‡½æ•°
# ============================================================================

def compare_v2_vs_enhanced(
    v2_results: pd.DataFrame,
    enhanced_results: pd.DataFrame,
    target_leaders: List[str] = None
) -> Dict:
    """
    å¯¹æ¯”v2å’Œv2_enhancedçš„ç»“æœ

    å‚æ•°:
        v2_results: v2ç­›é€‰ç»“æœ
        enhanced_results: v2_enhancedç­›é€‰ç»“æœ
        target_leaders: ç›®æ ‡é¾™å¤´ä¼ä¸šåˆ—è¡¨

    è¿”å›:
        å¯¹æ¯”ç»Ÿè®¡å­—å…¸
    """
    if target_leaders is None:
        target_leaders = [
            "è´µå·èŒ…å°", "æ’ç‘åŒ»è¯", "çˆ±å°”çœ¼ç§‘", "é•¿æ˜¥é«˜æ–°", "é€šç­–åŒ»ç–—",
            "æ¯”äºšè¿ª", "å®å¾·æ—¶ä»£", "è¿ˆç‘åŒ»ç–—", "è¯æ˜åº·å¾·",
            "åŒèŠ±é¡º", "é‡‘å±±åŠå…¬", "ä¸­èŠ¯å›½é™…",
            "é•¿æ±Ÿç”µåŠ›", "ä¸­å›½ç¥å", "ç´«é‡‘çŸ¿ä¸š",
            "ä¸‰ä¸€é‡å·¥",
        ]

    logger.info("")
    logger.info("=" * 80)
    logger.info("ğŸ“Š v2.0 vs v2.0 Enhanced å¯¹æ¯”åˆ†æ")
    logger.info("=" * 80)

    # åŸºæœ¬ç»Ÿè®¡
    logger.info("\nã€åŸºæœ¬ç»Ÿè®¡ã€‘")
    logger.info(f"v2.0 ä¼ä¸šæ•°: {len(v2_results)}")
    logger.info(f"v2.0 Enhanced ä¼ä¸šæ•°: {len(enhanced_results)}")
    logger.info(f"å˜åŒ–: {len(enhanced_results) - len(v2_results):+d} ({(len(enhanced_results) / len(v2_results) - 1) * 100:+.1f}%)")

    # å¹³å‡æŒ‡æ ‡å¯¹æ¯”
    logger.info("\nã€å¹³å‡æŒ‡æ ‡ã€‘")
    v2_avg_score = v2_results['quality_score'].mean()
    enh_avg_score = enhanced_results['quality_score'].mean()
    logger.info(f"v2.0 å¹³å‡è¯„åˆ†: {v2_avg_score:.1f}")
    logger.info(f"v2.0 Enhanced å¹³å‡è¯„åˆ†: {enh_avg_score:.1f} ({enh_avg_score - v2_avg_score:+.1f})")

    v2_avg_roic = v2_results['5yd_ts_code_roic_avg'].mean()
    enh_avg_roic = enhanced_results['5yd_ts_code_roic_avg'].mean()
    logger.info(f"v2.0 å¹³å‡ROIC: {v2_avg_roic:.1f}%")
    logger.info(f"v2.0 Enhanced å¹³å‡ROIC: {enh_avg_roic:.1f}% ({enh_avg_roic - v2_avg_roic:+.1f}%)")

    # é¾™å¤´è¦†ç›–å¯¹æ¯”
    logger.info("\nã€é¾™å¤´ä¼ä¸šè¦†ç›–ã€‘")
    v2_covered = []
    enh_covered = []

    for leader in target_leaders:
        in_v2 = len(v2_results[v2_results['name'].str.contains(leader, na=False)]) > 0
        in_enh = len(enhanced_results[enhanced_results['name'].str.contains(leader, na=False)]) > 0

        if in_v2:
            v2_covered.append(leader)
        if in_enh:
            enh_covered.append(leader)

    logger.info(f"v2.0 è¦†ç›–: {len(v2_covered)}/{len(target_leaders)} ({len(v2_covered)/len(target_leaders)*100:.0f}%)")
    logger.info(f"v2.0 Enhanced è¦†ç›–: {len(enh_covered)}/{len(target_leaders)} ({len(enh_covered)/len(target_leaders)*100:.0f}%)")

    # æ–°å¢å’Œæµå¤±
    new_leaders = set(enh_covered) - set(v2_covered)
    lost_leaders = set(v2_covered) - set(enh_covered)

    if new_leaders:
        logger.info(f"\nâœ… æ–°å¢è¦†ç›– ({len(new_leaders)}å®¶): {', '.join(new_leaders)}")
    if lost_leaders:
        logger.info(f"âŒ å¤±å»è¦†ç›– ({len(lost_leaders)}å®¶): {', '.join(lost_leaders)}")

    logger.info("=" * 80)

    return {
        'v2_count': len(v2_results),
        'enhanced_count': len(enhanced_results),
        'v2_coverage': len(v2_covered) / len(target_leaders),
        'enhanced_coverage': len(enh_covered) / len(target_leaders),
        'new_leaders': list(new_leaders),
        'lost_leaders': list(lost_leaders),
    }
