"""
æ•°æ®å¼•æ“ - Pandaså®ç°
==================

ä¸“é—¨è´Ÿè´£æ•°æ®æ¸…ç†å·¥ä½œï¼šå»é‡ã€æ£€éªŒã€æ ‡å‡†åŒ–ç­‰åŸºç¡€æ•°æ®å¤„ç†åŠŸèƒ½
"""

import sys
from pathlib import Path
import logging
import pandas as pd
import numpy as np
import re
from typing import Dict, Any, List, Union, Optional

# orchestrator å·²ç§»è‡³æ ¹ç›®å½•
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent.parent))
from orchestrator import register_method
from .schema_utils import ensure_columns

logger = logging.getLogger(__name__)

@register_method(
    engine_name="store",
    component_type="data_engine",
    engine_type="pandas",
    description="Pandasæ•°æ®å­˜å‚¨ - ä¿å­˜DataFrameåˆ°CSV"
)
def store(data: Optional[pd.DataFrame] = None,
         path: str = "",
         format: str = "csv",
         append_mode: bool = False,
         **kwargs) -> pd.DataFrame:
    """Pandaså¼•æ“é€šç”¨å­˜å‚¨æ–¹æ³• - æ”¯æŒå¤šç§æ ¼å¼å’Œæ¨¡å¼"""
    logger.info(f"Pandaså¼•æ“ä¿å­˜æ•°æ®åˆ°: {path}")
    logger.info(f"ğŸ” Storeå‡½æ•°æ¥æ”¶çš„æ•°æ®ç±»å‹: {type(data)}")

    # å‚æ•°éªŒè¯
    if data is None:
        raise ValueError("storeæ–¹æ³•éœ€è¦è¾“å…¥æ•°æ®")
    if not path:
        raise ValueError("å¿…é¡»æŒ‡å®šå­˜å‚¨è·¯å¾„")

    try:
        if not isinstance(data, pd.DataFrame):
            logger.warning(f"æ•°æ®ç±»å‹ä¸æ˜¯DataFrame: {type(data)}ï¼Œè·³è¿‡å­˜å‚¨")
            return data

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        Path(path).parent.mkdir(parents=True, exist_ok=True)

        # æ ¹æ®æ ¼å¼ä¿å­˜
        if format.lower() == "csv":
            mode = "a" if append_mode else "w"
            header = not append_mode or not Path(path).exists()
            data.to_csv(path, index=False, encoding='utf-8', mode=mode, header=header)
        elif format.lower() == "excel":
            data.to_excel(path, index=False)
        elif format.lower() == "parquet":
            data.to_parquet(path, index=False)
        elif format.lower() == "json":
            data.to_json(path, orient='records', force_ascii=False, indent=2)
        else:
            logger.warning(f"ä¸æ”¯æŒçš„æ ¼å¼: {format}ï¼Œä½¿ç”¨CSVæ ¼å¼")
            data.to_csv(path, index=False, encoding='utf-8')

        logger.info(f"Pandaså¼•æ“æˆåŠŸä¿å­˜ {len(data)} è¡Œæ•°æ®åˆ°: {path} (æ ¼å¼: {format})")
        logger.info(f"ğŸ” Storeå‡½æ•°è¿”å›çš„æ•°æ®ç±»å‹: {type(data)}")
        return data  # è¿”å›åŸæ•°æ®ä»¥ä¾›ç®¡é“ç»§ç»­ä½¿ç”¨

    except Exception as e:
        logger.error(f"Pandaså¼•æ“ä¿å­˜æ•°æ®å¤±è´¥: {e}")
        return data  # å³ä½¿ä¿å­˜å¤±è´¥ä¹Ÿè¿”å›åŸæ•°æ®


@register_method(
    engine_name="clean_financial_data",
    component_type="data_engine",
    engine_type="pandas",
    description="è´¢åŠ¡æ•°æ®æ ‡å‡†åŒ–æ¸…æ´— - è½¬æ¢å•ä½ã€å¤„ç†ç¼ºå¤±å€¼ã€æ ‡å‡†åŒ–æ ¼å¼"
)
def clean_financial_data(data: Optional[Union[pd.DataFrame, str]] = None,
                        file_path: Optional[str] = None,
                        output_path: Optional[str] = None,
                        report_path: Optional[str] = None,
                        validate_accounting: bool = True,
                        **kwargs) -> pd.DataFrame:
    """
    è´¢åŠ¡æ•°æ®æ ‡å‡†åŒ–æ¸…æ´—ä¸»å‡½æ•°

    Args:
        data: DataFrameæˆ–CSVæ–‡ä»¶è·¯å¾„æˆ–None
        file_path: å¯é€‰çš„æ–‡ä»¶è·¯å¾„å‚æ•°
        output_path: æ¸…æ´—åæ•°æ®ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        report_path: æ¸…æ´—æŠ¥å‘Šä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        validate_accounting: æ˜¯å¦éªŒè¯ä¼šè®¡æ’ç­‰å¼
        **kwargs: å…¶ä»–å‚æ•°

    Returns:
        æ¸…æ´—åçš„DataFrame
    """
    logger.info("ğŸ§¹ å¼€å§‹è´¢åŠ¡æ•°æ®æ ‡å‡†åŒ–æ¸…æ´—")
    logger.info(f"ğŸ” è¾“å…¥æ•°æ®ç±»å‹: {type(data)}")

    try:
        # 1. æ™ºèƒ½æ•°æ®åŠ è½½å’ŒéªŒè¯
        if data is not None and isinstance(data, pd.DataFrame):
            # å¦‚æœæ˜¯DataFrameï¼Œç›´æ¥å¤åˆ¶
            df = data.copy()
            logger.info("ğŸ“‹ ä»ç®¡é“æ¥æ”¶DataFrameæ•°æ®")
        elif data is not None and isinstance(data, str):
            # å¦‚æœæ˜¯æ–‡ä»¶è·¯å¾„å­—ç¬¦ä¸²ï¼Œä»æ–‡ä»¶åŠ è½½
            df = pd.read_csv(data)
            logger.info(f"ğŸ“ ä»ä¼ å…¥è·¯å¾„åŠ è½½æ•°æ®: {data}")
        elif file_path:
            # ä½¿ç”¨file_pathå‚æ•°åŠ è½½
            df = pd.read_csv(file_path)
            logger.info(f"ï¿½ ä»file_pathåŠ è½½æ•°æ®: {file_path}")
        else:
            raise ValueError("å¿…é¡»æä¾›dataï¼ˆDataFrameæˆ–æ–‡ä»¶è·¯å¾„ï¼‰æˆ–file_pathå‚æ•°")

        logger.info(f"ğŸ“Š åŸå§‹æ•°æ®: {df.shape[0]}è¡Œ Ã— {df.shape[1]}åˆ—")

        # 2. åŸºç¡€æ¸…æ´—æµç¨‹
        df = _standardize_column_names(df)
        df = _convert_currency_to_numeric(df)
        df = _handle_boolean_false_values(df)
        df = _handle_missing_values(df)
        df = _standardize_time_index(df)

        # 3. æ•°æ®éªŒè¯
        if validate_accounting:
            df = _validate_accounting_equations(df)

        # 4. å¼‚å¸¸å€¼å¤„ç†
        df = _detect_and_handle_outliers(df)

        # 5. ä¿å­˜æ¸…æ´—åæ•°æ®
        if output_path:
            df.to_csv(output_path, index=False, encoding='utf-8')
            logger.info(f"ğŸ’¾ æ¸…æ´—åæ•°æ®å·²ä¿å­˜: {output_path}")

        # 6. ç”Ÿæˆå¹¶ä¿å­˜æ¸…æ´—æŠ¥å‘Š
        if report_path:
            summary = financial_data_summary(df)
            import json
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ“‹ æ¸…æ´—æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

        logger.info(f"âœ… æ•°æ®æ¸…æ´—å®Œæˆ: {df.shape[0]}è¡Œ Ã— {df.shape[1]}åˆ—")
        return df

    except Exception as e:
        logger.error(f"âŒ æ•°æ®æ¸…æ´—å¤±è´¥: {e}")
        raise


def _standardize_column_names(df: pd.DataFrame) -> pd.DataFrame:
    """æ ‡å‡†åŒ–åˆ—å - ç®€åŒ–ä¸­æ–‡åˆ—å"""
    logger.info("ğŸ“ æ ‡å‡†åŒ–åˆ—å")

    # æ ¸å¿ƒè´¢åŠ¡æŒ‡æ ‡æ˜ å°„
    column_mapping = {
        'æŠ¥å‘ŠæœŸ': 'period',
        '*æ‰€æœ‰è€…æƒç›Šï¼ˆæˆ–è‚¡ä¸œæƒç›Šï¼‰åˆè®¡': 'total_equity',
        '*èµ„äº§åˆè®¡': 'total_assets',
        '*è´Ÿå€ºåˆè®¡': 'total_liabilities',
        '*å½’å±äºæ¯å…¬å¸æ‰€æœ‰è€…æƒç›Šåˆè®¡': 'shareholders_equity',
        'æµåŠ¨èµ„äº§': 'current_assets',
        'è´§å¸èµ„é‡‘': 'cash_and_equivalents',
        'äº¤æ˜“æ€§é‡‘èèµ„äº§': 'trading_securities',
        'åº”æ”¶ç¥¨æ®åŠåº”æ”¶è´¦æ¬¾': 'accounts_receivable_total',
        'åº”æ”¶è´¦æ¬¾': 'accounts_receivable',
        'å­˜è´§': 'inventory',
        'å›ºå®šèµ„äº§åˆè®¡': 'fixed_assets_total',
        'çŸ­æœŸå€Ÿæ¬¾': 'short_term_debt',
        'é•¿æœŸå€Ÿæ¬¾': 'long_term_debt',
        'å®æ”¶èµ„æœ¬ï¼ˆæˆ–è‚¡æœ¬ï¼‰': 'share_capital',
        'èµ„æœ¬å…¬ç§¯': 'capital_surplus',
        'æœªåˆ†é…åˆ©æ¶¦': 'retained_earnings'
    }

    # é‡å‘½åæ ¸å¿ƒåˆ—
    df = df.rename(columns=column_mapping)

    # ä¸ºå…¶ä»–åˆ—ç”Ÿæˆè‹±æ–‡å
    for col in df.columns:
        if col not in column_mapping.values() and col in column_mapping.keys():
            continue
        elif 'ï¼š' in col:
            # å¤„ç†å­é¡¹ç›®ï¼Œå¦‚ "å…¶ä¸­ï¼šåº”æ”¶ç¥¨æ®"
            df = df.rename(columns={col: f"sub_{col.split('ï¼š')[1]}"})

    logger.info(f"ğŸ“ åˆ—åæ ‡å‡†åŒ–å®Œæˆï¼Œæ ¸å¿ƒæŒ‡æ ‡: {len(column_mapping)} ä¸ª")
    return df


def _convert_currency_to_numeric(df: pd.DataFrame) -> pd.DataFrame:
    """è½¬æ¢è´§å¸å­—ç¬¦ä¸²ä¸ºæ•°å€¼"""
    logger.info("ğŸ’° è½¬æ¢è´§å¸å•ä½ä¸ºæ•°å€¼")

    def convert_currency_value(value):
        """è½¬æ¢å•ä¸ªè´§å¸å€¼"""
        if pd.isna(value) or value == '':
            return np.nan

        if isinstance(value, (int, float)):
            return value

        if not isinstance(value, str):
            return np.nan

        # ç§»é™¤ç©ºæ ¼
        value = str(value).strip()

        # å¤„ç†è´Ÿæ•°
        is_negative = value.startswith('-')
        if is_negative:
            value = value[1:]

        # æå–æ•°å­—å’Œå•ä½
        if 'äº¿' in value:
            number = re.findall(r'[\d.]+', value)
            if number:
                result = float(number[0]) * 100000000  # äº¿ = 1e8
            else:
                return np.nan
        elif 'ä¸‡' in value:
            number = re.findall(r'[\d.]+', value)
            if number:
                result = float(number[0]) * 10000  # ä¸‡ = 1e4
            else:
                return np.nan
        else:
            # å°è¯•ç›´æ¥è½¬æ¢ä¸ºæ•°å­—
            try:
                result = float(value)
            except:
                return np.nan

        return -result if is_negative else result

    # è¯†åˆ«éœ€è¦è½¬æ¢çš„åˆ—ï¼ˆåŒ…å«è´§å¸å•ä½çš„åˆ—ï¼‰
    currency_columns = []
    for col in df.columns:
        if col in ['period']:  # è·³è¿‡æ—¶é—´åˆ—
            continue
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è´§å¸å•ä½
        sample_values = df[col].dropna().astype(str).head(5)
        if any('äº¿' in str(val) or 'ä¸‡' in str(val) for val in sample_values):
            currency_columns.append(col)

    logger.info(f"ğŸ’° å‘ç°è´§å¸åˆ—: {len(currency_columns)} ä¸ª")

    # è½¬æ¢è´§å¸åˆ—
    for col in currency_columns:
        original_type = df[col].dtype
        df[col] = df[col].apply(convert_currency_value)
        logger.debug(f"  {col}: {original_type} â†’ float64")

    return df


def _handle_boolean_false_values(df: pd.DataFrame) -> pd.DataFrame:
    """å¤„ç†å¸ƒå°”Falseå€¼ - è½¬æ¢ä¸ºNaNæˆ–0"""
    logger.info("ğŸ”„ å¤„ç†å¸ƒå°”Falseå€¼")

    false_count = 0
    for col in df.columns:
        if col == 'period':  # è·³è¿‡æ—¶é—´åˆ—
            continue

        # ç»Ÿè®¡Falseå€¼
        false_mask = df[col].astype(str) == 'False'
        col_false_count = false_mask.sum()

        if col_false_count > 0:
            false_count += col_false_count
            # å°†Falseè½¬æ¢ä¸ºNaNï¼Œå…ˆè½¬æ¢ä¸ºobjectç±»å‹é¿å…è­¦å‘Š
            df[col] = df[col].astype('object')
            df.loc[false_mask, col] = np.nan
            logger.debug(f"  {col}: {col_false_count} ä¸ªFalseå€¼è½¬æ¢ä¸ºNaN")

    logger.info(f"ğŸ”„ æ€»è®¡å¤„ç† {false_count} ä¸ªFalseå€¼")
    return df


def _handle_missing_values(df: pd.DataFrame) -> pd.DataFrame:
    """å¤„ç†ç¼ºå¤±å€¼ - ä½¿ç”¨å‰å‘å¡«å……å’Œ0å¡«å……ç­–ç•¥"""
    logger.info("ğŸ•³ï¸ å¤„ç†ç¼ºå¤±å€¼")

    # è®¡ç®—ç¼ºå¤±å€¼ç»Ÿè®¡
    missing_before = df.isnull().sum().sum()

    # å¯¹äºè´¢åŠ¡æ•°æ®ï¼Œä¸åŒç±»å‹æŒ‡æ ‡ä½¿ç”¨ä¸åŒç­–ç•¥
    asset_liability_cols = [col for col in df.columns if any(keyword in col.lower()
                           for keyword in ['asset', 'liability', 'debt', 'equity', 'cash'])]

    # èµ„äº§è´Ÿå€ºç±»ï¼šä½¿ç”¨å‰å‘å¡«å……
    for col in asset_liability_cols:
        if col in df.columns:
            df[col] = df[col].ffill()

    # å…¶ä»–æ•°å€¼åˆ—ï¼šå¡«å……0ï¼ˆè¡¨ç¤ºè¯¥å¹´åº¦æ— æ­¤é¡¹ç›®ï¼‰
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if col not in asset_liability_cols:
            df[col] = df[col].fillna(0)

    missing_after = df.isnull().sum().sum()
    logger.info(f"ğŸ•³ï¸ ç¼ºå¤±å€¼å¤„ç†: {missing_before} â†’ {missing_after}")

    return df


def _standardize_time_index(df: pd.DataFrame) -> pd.DataFrame:
    """æ ‡å‡†åŒ–æ—¶é—´ç´¢å¼•"""
    logger.info("ğŸ“… æ ‡å‡†åŒ–æ—¶é—´ç´¢å¼•")

    if 'period' in df.columns:
        # è½¬æ¢ä¸ºdatetime
        df['period'] = pd.to_datetime(df['period'], format='%Y', errors='coerce')

        # è®¾ç½®ä¸ºç´¢å¼•
        df = df.set_index('period').sort_index()

        logger.info(f"ğŸ“… æ—¶é—´ç´¢å¼•è®¾ç½®å®Œæˆ: {df.index.min()} - {df.index.max()}")

    return df


def _validate_accounting_equations(df: pd.DataFrame) -> pd.DataFrame:
    """éªŒè¯ä¼šè®¡æ’ç­‰å¼: èµ„äº§ = è´Ÿå€º + æ‰€æœ‰è€…æƒç›Š"""
    logger.info("âš–ï¸ éªŒè¯ä¼šè®¡æ’ç­‰å¼")

    if all(col in df.columns for col in ['total_assets', 'total_liabilities', 'total_equity']):
        # è®¡ç®—å·®é¢
        df['accounting_diff'] = df['total_assets'] - (df['total_liabilities'] + df['total_equity'])

        # è®¡ç®—ç›¸å¯¹è¯¯å·®
        df['accounting_error_pct'] = (df['accounting_diff'] / df['total_assets'] * 100).round(2)

        # ç»Ÿè®¡éªŒè¯ç»“æœ
        tolerance = 0.01  # 1%å®¹å¿åº¦
        valid_count = (abs(df['accounting_error_pct']) <= tolerance).sum()
        total_count = len(df)

        logger.info(f"âš–ï¸ ä¼šè®¡ç­‰å¼éªŒè¯: {valid_count}/{total_count} æ¡è®°å½•åœ¨å®¹å¿èŒƒå›´å†…")

        # æŠ¥å‘Šå¼‚å¸¸è®°å½•
        invalid_records = df[abs(df['accounting_error_pct']) > tolerance]
        if len(invalid_records) > 0:
            logger.warning(f"âš ï¸ å‘ç° {len(invalid_records)} æ¡è®°å½•å­˜åœ¨ä¼šè®¡ç­‰å¼åå·®")
            for idx, row in invalid_records.iterrows():
                logger.warning(f"  {idx}: åå·® {row['accounting_error_pct']:.2f}%")

    return df


def _detect_and_handle_outliers(df: pd.DataFrame) -> pd.DataFrame:
    """æ£€æµ‹å’Œå¤„ç†å¼‚å¸¸å€¼"""
    logger.info("ğŸ” æ£€æµ‹å¼‚å¸¸å€¼")

    numeric_cols = df.select_dtypes(include=[np.number]).columns
    outlier_count = 0

    for col in numeric_cols:
        if col.endswith('_diff') or col.endswith('_pct'):  # è·³è¿‡è®¡ç®—åˆ—
            continue

        # ä½¿ç”¨IQRæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1

        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)) & df[col].notna()
        outlier_count += outliers.sum()

        if outliers.sum() > 0:
            logger.warning(f"ğŸ” {col}: å‘ç° {outliers.sum()} ä¸ªå¼‚å¸¸å€¼")

    logger.info(f"ğŸ” å¼‚å¸¸å€¼æ£€æµ‹å®Œæˆï¼Œæ€»è®¡å‘ç°: {outlier_count} ä¸ª")
    return df


@register_method(
    engine_name="financial_data_summary",
    component_type="data_engine",
    engine_type="pandas",
    description="ç”Ÿæˆè´¢åŠ¡æ•°æ®æ¸…æ´—æŠ¥å‘Š"
)
def financial_data_summary(data: pd.DataFrame, output_path: Optional[str] = None) -> Dict[str, Any]:
    """ç”Ÿæˆè´¢åŠ¡æ•°æ®æ¸…æ´—æŠ¥å‘Š"""
    logger.info("ğŸ“Š ç”Ÿæˆæ•°æ®æ¸…æ´—æŠ¥å‘Š")

    try:
        summary = {
            "æ•°æ®æ¦‚è§ˆ": {
                "æ€»è¡Œæ•°": int(len(data)),
                "æ€»åˆ—æ•°": int(len(data.columns)),
                "æ—¶é—´è·¨åº¦": f"{data.index.min()} è‡³ {data.index.max()}" if hasattr(data.index, 'min') else "æœªè®¾ç½®æ—¶é—´ç´¢å¼•",
                "æ•°æ®ç±»å‹åˆ†å¸ƒ": {str(k): int(v) for k, v in data.dtypes.value_counts().items()}
            },
            "æ•°æ®è´¨é‡": {
                "ç¼ºå¤±å€¼æ€»æ•°": int(data.isnull().sum().sum()),
                "ç¼ºå¤±å€¼æ¯”ä¾‹": f"{(data.isnull().sum().sum() / (len(data) * len(data.columns)) * 100):.2f}%",
                "æ•°å€¼åˆ—æ•°é‡": int(len(data.select_dtypes(include=[np.number]).columns)),
                "å®Œæ•´è®°å½•æ•°": int(len(data.dropna()))
            },
            "è´¢åŠ¡æŒ‡æ ‡ç»Ÿè®¡": {}
        }

        # æ ¸å¿ƒè´¢åŠ¡æŒ‡æ ‡ç»Ÿè®¡
        key_metrics = ['total_assets', 'total_liabilities', 'total_equity', 'cash_and_equivalents']
        for metric in key_metrics:
            if metric in data.columns:
                col_data = data[metric].dropna()
                summary["è´¢åŠ¡æŒ‡æ ‡ç»Ÿè®¡"][metric] = {
                    "æœ€å°å€¼": float(col_data.min()),
                    "æœ€å¤§å€¼": float(col_data.max()),
                    "å¹³å‡å€¼": float(col_data.mean()),
                    "æ ‡å‡†å·®": float(col_data.std()),
                    "æœ‰æ•ˆæ•°æ®ç‚¹": len(col_data)
                }

        # ä¼šè®¡ç­‰å¼éªŒè¯ç»“æœ
        if 'accounting_error_pct' in data.columns:
            valid_records = (abs(data['accounting_error_pct']) <= 1.0).sum()
            summary["ä¼šè®¡éªŒè¯"] = {
                "éªŒè¯é€šè¿‡è®°å½•": int(valid_records),
                "éªŒè¯é€šè¿‡ç‡": f"{(valid_records / len(data) * 100):.2f}%",
                "å¹³å‡åå·®": f"{data['accounting_error_pct'].abs().mean():.4f}%"
            }

        # ä¿å­˜æŠ¥å‘Š
        if output_path:
            import json
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ“Š æ¸…æ´—æŠ¥å‘Šå·²ä¿å­˜: {output_path}")

        logger.info("ğŸ“Š æ•°æ®æ¸…æ´—æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        return summary

    except Exception as e:
        logger.error(f"âŒ ç”Ÿæˆæ¸…æ´—æŠ¥å‘Šå¤±è´¥: {e}")
        raise


@register_method(
    engine_name="join_and_summarize",
    component_type="data_engine",
    engine_type="pandas",
    description="å¤šè¾“å…¥ç¤ºä¾‹ï¼šåˆå¹¶ä¸¤ä¸ª DataFrame å¹¶è¾“å‡ºåŒç»“æœ(dict è§¦å‘å¤šè¾“å‡ºæ‹†è§£)"
)
@ensure_columns(required_columns=["total_assets"], output_keys=["merged", "stats"], strict=False)
def join_and_summarize(inputs: List[pd.DataFrame] = None,
                       how: str = 'inner',
                       on: Optional[str] = None,
                       limit: int = 0) -> Dict[str, Any]:
    """ç¤ºä¾‹ï¼šå±•ç¤ºå¤šè¾“å…¥ + dict å¤šè¾“å‡º + schema æ ¡éªŒã€‚

    Args:
        inputs: æ¥è‡ªä¸Šæ¸¸çš„å¤šä¸ª DataFrameï¼ˆengine å·²æ³¨å…¥ï¼‰
        how: åˆå¹¶æ–¹å¼
        on: æŒ‡å®š join é”®ï¼ˆè‹¥æœªæä¾›åˆ™å°è¯•å…¬å…±åˆ—ï¼‰
        limit: å¯é€‰æˆªæ–­è¡Œæ•°ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    Returns:
        {"merged": DataFrame, "stats": {åˆ—/è¡Œä¿¡æ¯}}
    """
    logger.info("ğŸ”— join_and_summarize: å¼€å§‹å¤„ç†å¤šè¾“å…¥")
    if not inputs or len(inputs) < 2:
        raise ValueError("join_and_summarize éœ€è¦è‡³å°‘ä¸¤ä¸ªè¾“å…¥ DataFrame")
    left, right = inputs[0], inputs[1]
    # è‡ªåŠ¨é€‰æ‹© join åˆ—
    if on is None:
        common = [c for c in left.columns if c in right.columns]
        if not common:
            raise ValueError("æœªæ‰¾åˆ°å…¬å…±åˆ—ç”¨äº joinï¼Œå¯é€šè¿‡å‚æ•° on æŒ‡å®š")
        on = common[0]
        logger.info(f"ğŸ”‘ è‡ªåŠ¨é€‰æ‹©å…¬å…±åˆ— '{on}' ä½œä¸º join é”®")
    merged = left.merge(right, how=how, on=on, suffixes=("_l", "_r"))
    if limit and limit > 0:
        merged = merged.head(limit)
    stats = {
        'rows': int(len(merged)),
        'cols': int(len(merged.columns)),
        'columns_sample': merged.columns[:10].tolist(),
        'join_key': on,
        'inputs_shapes': [list(df.shape) for df in inputs[:2]]
    }
    logger.info(f"ğŸ”— join_and_summarize å®Œæˆ: {stats['rows']} è¡Œ, {stats['cols']} åˆ—")
    return {"merged": merged, "stats": stats}


@register_method(
    engine_name="double_split_demo",
    component_type="data_engine",
    engine_type="pandas",
    description="ç¤ºä¾‹ï¼šè¿”å›ä¸¤ä¸ª DataFrame (tuple å¤šè¾“å‡º)"
)
def double_split_demo(data: pd.DataFrame, top: int = 5, sample: int = 5):
    """æ¼”ç¤ºæ— éœ€ dictï¼Œç›´æ¥è¿”å› (head_df, tail_df) ä¹Ÿå¯é€šè¿‡ outputs æ˜ å°„ã€‚

    Args:
        data: ä¸Šæ¸¸ DataFrameï¼ˆå¼•æ“è‡ªåŠ¨æ³¨å…¥ï¼‰
        top: å–å‰å¤šå°‘è¡Œ
        sample: ä»åéƒ¨éšæœºæŠ½æ ·å¤šå°‘è¡Œ
    Returns:
        (head_df, tail_sample_df)
    """
    if not isinstance(data, pd.DataFrame):
        raise ValueError("double_split_demo éœ€è¦ DataFrame è¾“å…¥")
    head_part = data.head(top).copy()
    tail_part = data.tail(max(sample, 1)).sample(min(sample, len(data.tail(max(sample,1)))), random_state=42).copy()
    head_part['__subset'] = 'head'
    tail_part['__subset'] = 'tail_sample'
    return head_part, tail_part