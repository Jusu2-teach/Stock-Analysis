"""
Polarsæ•°æ®å¼•æ“ - é«˜æ€§èƒ½æ•°æ®å¤„ç†å¼•æ“
æä¾›å­˜å‚¨ã€å­—æ®µæ˜ å°„å’Œå»é‡åŠŸèƒ½
"""
import sys
from pathlib import Path
import polars as pl
import yaml
from typing import Optional, Dict, Any, Union, List
import logging
import re
import fnmatch

# orchestrator å·²ç§»è‡³æ ¹ç›®å½•
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent.parent))
# from orchestrator import register_method  <-- Removed

logger = logging.getLogger(__name__)

def _load_mapping() -> Dict[str, str]:
    """åŠ è½½è´¢åŠ¡æŒ‡æ ‡æ˜ å°„é…ç½®"""
    try:
        config_path = Path(__file__).parent / "config.yaml"
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
            return config.get('financial_indicators', {})
    except Exception as e:
        logger.warning(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}, ä½¿ç”¨ç©ºæ˜ å°„")
        return {}


import polars as pl
import logging

logger = logging.getLogger(__name__)


def _infer_format(path_obj: Path, pattern: str | None) -> str:
    inferred = "csv"
    if path_obj.is_file():
        suffix = path_obj.suffix.lower()
        if suffix in (".csv", ".parquet", ".json"):
            inferred = suffix.removeprefix('.')
    elif pattern and any(pattern.endswith(ext) for ext in (".csv", ".parquet", ".json")):
        for ext in ("parquet", "csv", "json"):
            if pattern.endswith(ext):
                inferred = ext
                break
    return inferred

def _build_file_list(path_obj: Path, read_format: str, pattern: str | None, recursive: bool) -> tuple[list[Path], str]:
    if pattern:
        glob_pattern = pattern
    else:
        ext_map = {'csv': '*.csv', 'parquet': '*.parquet', 'json': '*.json'}
        glob_pattern = ext_map.get(read_format, '*')
    if recursive:
        files = list(path_obj.rglob(glob_pattern))
    else:
        files = list(path_obj.glob(glob_pattern))
    return [f for f in files if f.is_file()], glob_pattern

def _apply_exclude(files: list[Path], exclude: Union[str, list[str], tuple[str, ...], None]) -> list[Path]:
    if not exclude:
        return files
    patterns: list[str] = []
    if isinstance(exclude, str):
        if ',' in exclude:
            patterns.extend([p.strip() for p in exclude.split(',') if p.strip()])
        else:
            patterns.append(exclude.strip())
    else:
        patterns.extend([p.strip() for p in list(exclude) if isinstance(p, str) and p.strip()])
    if not patterns:
        return files
    before = len(files)
    filtered = [f for f in files if not any(fnmatch.fnmatch(f.name, pat) for pat in patterns)]
    removed = before - len(filtered)
    if removed:
        logger.info(f"æ–‡ä»¶æ’é™¤: patterns={patterns} ç§»é™¤={removed}")
    return filtered


def _read_single_file(p: Path, fmt: str, lazy: bool, kwargs: dict[str, Any], log_each: bool, ignore_errors: bool) -> Optional[Union[pl.DataFrame, pl.LazyFrame]]:
    try:
        if lazy and fmt in {"csv", "parquet"}:
            df_local = pl.scan_csv(p, **kwargs) if fmt == "csv" else pl.scan_parquet(p, **kwargs)
        elif fmt == "csv":
            df_local = pl.read_csv(p, **kwargs)
        elif fmt == "parquet":
            df_local = pl.read_parquet(p, **kwargs)
        elif fmt == "json":
            if lazy:
                logger.warning("json ä¸æ”¯æŒ scanï¼Œé€€å› eager è¯»å–")
            df_local = pl.read_json(p, **kwargs)
        else:
            logger.error(f"ä¸æ”¯æŒçš„æ ¼å¼: {fmt}")
            return None
        if log_each and not isinstance(df_local, pl.LazyFrame):
            logger.info(f"è¯»å–: {p.name} -> {df_local.height}è¡Œ x {df_local.width}åˆ—")
        elif log_each:
            logger.info(f"æ‰«æ: {p.name} -> LazyFrame")
        return df_local
    except Exception as ex:
        if ignore_errors:
            logger.warning(f"è¯»å–å¤±è´¥å·²è·³è¿‡ {p}: {ex}")
            return None
        raise

def _unify_schema(dfs: list[Union[pl.DataFrame, pl.LazyFrame]], fill_value, strict: bool) -> list[Union[pl.DataFrame, pl.LazyFrame]]:
    if not dfs:
        return dfs
    try:
        all_cols = []
        col_set = set()
        # ä¿æŒç¬¬ä¸€æ¬¡å‡ºç°é¡ºåº
        for d in dfs:
            for c in d.columns:
                if c not in col_set:
                    col_set.add(c)
                    all_cols.append(c)
        base_cols = dfs[0].columns
        ordered = list(base_cols) + [c for c in all_cols if c not in base_cols]
        aligned = []
        for d in dfs:
            d_cols = set(d.columns)
            missing = [c for c in ordered if c not in d_cols]
            if missing and strict:
                raise ValueError(f"strict_schema=True åˆ—ä¸ä¸€è‡´: ç¼ºå¤± {missing}")
            if missing:
                d = d.with_columns([pl.lit(fill_value).alias(m) for m in missing])
            d = d.select([pl.col(c) for c in ordered])
            aligned.append(d)
        return aligned
    except Exception as ex:
        logger.warning(f"Schema å¯¹é½å¤±è´¥(å¿½ç•¥ unify_schema): {ex}")
        return dfs

def _normalize_sort_spec(spec: Union[str, list[str], tuple[str, ...], None]) -> list[str]:
    if not spec:
        return []
    if isinstance(spec, str):
        return [p.strip() for p in spec.split(',') if p.strip()]
    return [p.strip() for p in list(spec) if isinstance(p, str) and p.strip()]

def _parse_sort_tokens(tokens: list[str]) -> tuple[list[str], list[bool]]:
    cols: list[str] = []
    descs: list[bool] = []
    for token in tokens:
        desc = False
        raw = token
        if raw.startswith('-') and len(raw) > 1:
            desc = True
            raw = raw[1:]
        if ':' in raw:
            name, mode = [x.strip() for x in raw.split(':', 1)]
            raw = name
            if mode.lower() in ("desc", "descending", "d"):
                desc = True
            elif mode.lower() in ("asc", "ascending", "a"):
                desc = False
        cols.append(raw)
        descs.append(desc)
    return cols, descs

def _apply_sort(merged: Union[pl.DataFrame, pl.LazyFrame],
                sort_by,
                sort_desc,
                coerce_dates: bool) -> tuple[Union[pl.DataFrame, pl.LazyFrame], list[str]]:
    tokens = _normalize_sort_spec(sort_by)
    if not tokens:
        logger.debug("æœªæŒ‡å®šæ’åºåˆ—æˆ–æ’åºåˆ—è¡¨ä¸ºç©º, è·³è¿‡æ’åº")
        return merged, []
    cols, descs = _parse_sort_tokens(tokens)
    # sort_desc æ˜¾å¼è¦†ç›–
    if cols:
        if isinstance(sort_desc, bool):
            descs = [bool(sort_desc)] * len(cols)
        else:
            tmp = list(sort_desc)
            if len(tmp) < len(cols):
                tmp += [False] * (len(cols) - len(tmp))
            descs = [bool(x) for x in tmp[:len(cols)]]
    existing_map = {c: c for c in merged.columns}
    kept_cols: list[str] = []
    kept_desc: list[bool] = []
    missing: list[str] = []
    for c, d in zip(cols, descs):
        if c in existing_map:
            kept_cols.append(existing_map[c])
            kept_desc.append(d)
        else:
            missing.append(c)
    if not kept_cols:
        logger.warning(f"æ’åº: æ‰€æœ‰æŒ‡å®šåˆ—ä¸å­˜åœ¨ -> {cols} (è·³è¿‡æ’åº)")
        return merged, []
    if coerce_dates:
        date_targets = [c for c in kept_cols if 'date' in c.lower()]
        casts = []
        for coln in date_targets:
            try:
                if merged.schema.get(coln) == pl.Utf8:
                    casts.append(pl.col(coln).str.strptime(pl.Date, strict=False).alias(coln))
            except Exception as ce:
                logger.debug(f"æ—¥æœŸè§£æè·³è¿‡ {coln}: {ce}")
        if casts:
            try:
                merged = merged.with_columns(casts)
            except Exception as ce2:
                logger.debug(f"æ—¥æœŸåˆ—æ‰¹é‡è§£æå¤±è´¥(å¿½ç•¥): {ce2}")
    try:
        merged = merged.sort(by=kept_cols, descending=kept_desc)
        logger.info(
            f"æ’åºå®Œæˆ: åˆ—={kept_cols} é™åº={kept_desc} ç¼ºå¤±åˆ—={missing if missing else 'None'}"
        )
        if missing:
            logger.warning(f"æ’åº: ä»¥ä¸‹åˆ—æœªæ‰¾åˆ° -> {missing}")
    except Exception as ex:
        logger.warning(f"æ’åºå¤±è´¥(å¿½ç•¥): {ex}")
        logger.debug(f"å¤±è´¥ä¸Šä¸‹æ–‡ sort_cols={kept_cols} desc={kept_desc}")
    return merged, kept_cols

def load_data(data: Optional[pl.DataFrame] = None,
              file_path: str = None,
              pattern: str | None = None,
              exclude: Union[str, list[str], tuple[str, ...], None] = None,
              lazy: bool = False,
              sort_by: Union[str, list[str], tuple[str, ...], None] = "end_date",
              sort_desc: Union[bool, list[bool], tuple[bool, ...]] = False,
              deduplicate: bool = True,
              dedup_keys: Union[list[str], tuple[str, ...]] = ("ts_code", "end_date"),
              dedup_strategy: str = "first"  # first|last|latest_ann_date
              ) -> Optional[Union[pl.DataFrame, pl.LazyFrame]]:
    """æ•°æ®åŠ è½½å…¥å£ (æ”¯æŒå•æ–‡ä»¶ / ç›®å½•å¤šæ–‡ä»¶)ã€‚å†…éƒ¨å·²æ‹†åˆ†å¤šæ­¥éª¤å‡½æ•°ä»¥æå‡å¯ç»´æŠ¤æ€§ã€‚

    å‚æ•°è¯´æ˜ä¸åŸç‰ˆæœ¬ä¸€è‡´ï¼ŒåŠŸèƒ½ç­‰ä»·ã€‚
    """
    # å†…éƒ¨é…ç½®ï¼ˆåç»­å¯æŠ½è±¡ä¸º dataclass é…ç½®ï¼‰
    FORMAT: str | None = None
    RECURSIVE = False
    LIMIT = 0
    COERCE_DATES = True
    UNIFY_SCHEMA = True
    FILL_MISSING_WITH = None
    STRICT_SCHEMA = False
    IGNORE_ERRORS = True  # æ˜¯å¦å¿½ç•¥å•æ–‡ä»¶è¯»å–é”™è¯¯
    LOG_EACH = True

    kwargs: dict[str, Any] = {}
    if not file_path:
        logger.error("å¿…é¡»æŒ‡å®š file_path")
        return None
    path_obj = Path(file_path)
    if not path_obj.exists():
        logger.error(f"è·¯å¾„ä¸å­˜åœ¨: {file_path}")
        return None

    read_format = (FORMAT or _infer_format(path_obj, pattern)).lower()

    # å•æ–‡ä»¶ç›´æ¥è¯»å–
    if path_obj.is_file():
        df = _read_single_file(path_obj, read_format, lazy, kwargs, LOG_EACH, IGNORE_ERRORS)
        if df is not None and not isinstance(df, pl.LazyFrame):
            logger.info(f"æˆåŠŸåŠ è½½æ•°æ®: {path_obj} ({df.height}è¡Œ, {df.width}åˆ—)")
        return df
    if not path_obj.is_dir():
        logger.error(f"file_path æ—¢ä¸æ˜¯æ–‡ä»¶ä¹Ÿä¸æ˜¯ç›®å½•: {file_path}")
        return None

    files, glob_pattern = _build_file_list(path_obj, read_format, pattern, RECURSIVE)
    files = _apply_exclude(files, exclude)
    if LIMIT and LIMIT > 0:
        files = files[:LIMIT]
    if not files:
        logger.warning(f"ç›®å½•æœªåŒ¹é…åˆ°ä»»ä½•æ–‡ä»¶: {file_path} pattern={glob_pattern}")
        return None
    original_count = len(files)
    logger.info(f"ç›®å½•æ¨¡å¼: å¾…è¯»å–æ–‡ä»¶æ•°={len(files)} (åŸå§‹={original_count}) pattern={glob_pattern} format={read_format} lazy={lazy}")

    # è¯»å–
    dfs: list[Union[pl.DataFrame, pl.LazyFrame]] = []
    for fp in files:
        dfl = _read_single_file(fp, read_format, lazy, kwargs, LOG_EACH, IGNORE_ERRORS)
        if dfl is not None:
            dfs.append(dfl)
    if not dfs:
        logger.error("æ‰€æœ‰æ–‡ä»¶è¯»å–å¤±è´¥æˆ–ä¸ºç©º")
        return None

    # Schema å¯¹é½
    if UNIFY_SCHEMA:
        dfs = _unify_schema(dfs, FILL_MISSING_WITH, STRICT_SCHEMA)

    try:
        merged = pl.concat(dfs, how="vertical_relaxed")
    except Exception as ex:
        logger.error(f"åˆå¹¶å¤±è´¥: {ex}")
        return None

    # å»é‡ï¼ˆå¦‚æœé…ç½®ï¼‰åœ¨æ’åºå‰æ‰§è¡Œ
    if deduplicate and dedup_keys:
        missing_keys = [k for k in dedup_keys if k not in merged.columns]
        if missing_keys:
            logger.warning(f"å»é‡è·³è¿‡: ç¼ºå¤±å…³é”®åˆ— {missing_keys}")
        else:
            try:
                merged = _remove_duplicates(merged, list(dedup_keys), dedup_strategy)
            except Exception as de:
                logger.warning(f"å†…éƒ¨å»é‡å¤±è´¥(å¿½ç•¥): {de}")

    merged, used_sort_cols = _apply_sort(merged, sort_by, sort_desc, COERCE_DATES)

    if isinstance(merged, pl.LazyFrame):
        logger.info(f"åˆå¹¶å®Œæˆ (LazyFrame): æ–‡ä»¶æ•°={len(dfs)} åˆ—æ•°(ä¼°è®¡)={len(merged.columns)} sort_by={used_sort_cols if used_sort_cols else 'N/A'}")
        if not lazy:
            collected = merged.collect()
            logger.info(f"å·² collect -> è¡Œæ•°={collected.height} åˆ—æ•°={collected.width}")
            return collected
        return merged
    else:
        logger.info(f"åˆå¹¶å®Œæˆ: æ–‡ä»¶æ•°={len(dfs)} æ€»è¡Œæ•°={merged.height} åˆ—æ•°={merged.width} (æ’åºåˆ—={used_sort_cols if used_sort_cols else 'N/A'}) lazy={lazy}")
        return merged

def store(data: Optional[pl.DataFrame] = None,
          file_path: str = None,
          format: str = "parquet",
          **kwargs) -> Optional[pl.DataFrame]:
    """
    çº¯ Polars æ•°æ®å†™å‡ºå‡½æ•°ã€‚

    å‚æ•°:
        data: å¿…é¡»æ˜¯ pl.DataFrame
        file_path: ç›®æ ‡æ–‡ä»¶è·¯å¾„
        format: parquet | csv | json
        **kwargs: ä¼ é€’ç»™åº•å±‚å†™å‡ºæ–¹æ³•çš„é™„åŠ å‚æ•°

    è¿”å›:
        åŸå§‹ pl.DataFrame (ä¾¿äºé“¾å¼è°ƒç”¨) æˆ– None
    """
    if data is None:
        logger.warning("æ²¡æœ‰æ•°æ®éœ€è¦å­˜å‚¨")
        return None
    # å…¼å®¹ pandas.DataFrame è‡ªåŠ¨è½¬æ¢
    if not isinstance(data, pl.DataFrame):
        try:
            import pandas as pd  # type: ignore
            if isinstance(data, pd.DataFrame):
                logger.info("store: æ£€æµ‹åˆ° pandas.DataFrame, è‡ªåŠ¨è½¬æ¢ä¸º Polars")
                try:
                    data = pl.from_pandas(data)
                except Exception as conv_e:
                    logger.error(f"pandas->Polars è½¬æ¢å¤±è´¥: {conv_e}")
                    return None
            else:
                logger.error("store ä»…æ”¯æŒ Polars æˆ– pandas DataFrame")
                return None
        except Exception as ie:
            logger.error(f"pandas å…¼å®¹è½¬æ¢å¤±è´¥: {ie}")
            return None
    if file_path is None:
        logger.error("å¿…é¡»æŒ‡å®šæ–‡ä»¶è·¯å¾„")
        return None

    try:
        dst = Path(file_path)
        dst.parent.mkdir(parents=True, exist_ok=True)
        fmt = format.lower()
        if fmt == "parquet":
            data.write_parquet(dst, **kwargs)
        elif fmt == "csv":
            data.write_csv(dst, **kwargs)
        elif fmt == "json":
            data.write_json(dst, **kwargs)
        else:
            logger.error(f"ä¸æ”¯æŒçš„æ ¼å¼: {format}")
            return None
        logger.info(f"æ•°æ®å·²ä¿å­˜åˆ°: {dst} (format={fmt}) rows={data.height}")
        return data
    except Exception as e:
        logger.error(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")
        return None

def filter_mapped_columns(data: Optional[pl.DataFrame] = None,
                          include_all_mapped: bool = True,  # é¢„ç•™å‚æ•°ï¼Œå½“å‰ä»…ç­›é€‰å­˜åœ¨åˆ—
                          show_schema: bool = True,
                          **kwargs) -> Optional[pl.DataFrame]:
    """
    æ ¹æ®é…ç½®æ˜ å°„ç­›é€‰åˆ—ï¼Œåªä¿ç•™æ˜ å°„ä¸­å®šä¹‰çš„å­—æ®µï¼Œå¹¶æŠŠä¸­æ–‡å­—æ®µåè¡Œä½œä¸ºç¬¬ä¸€æ¡æ•°æ®è¡Œæ’å…¥ã€‚

    Args:
        data: è¾“å…¥çš„Polars DataFrame
        include_all_mapped: æ˜¯å¦åŒ…å«æ‰€æœ‰æ˜ å°„çš„åˆ—ï¼ˆå³ä½¿æ•°æ®ä¸­ä¸å­˜åœ¨ï¼‰
        show_schema: æ˜¯å¦æ˜¾ç¤ºå­—æ®µçš„ä¸­æ–‡å«ä¹‰è¯´æ˜
        **kwargs: å…¶ä»–å‚æ•°

    Returns:
    Optional[pl.DataFrame]: ç­›é€‰åçš„DataFrameï¼ˆå†™å‡ºCSVå: ç¬¬1è¡Œä¸ºè‹±æ–‡è¡¨å¤´, ç¬¬2è¡Œä¸ºä¸­æ–‡å­—æ®µå, ç¬¬3+è¡Œä¸ºæ•°æ®ï¼‰
    """
    if data is None:
        logger.warning("æ²¡æœ‰è¾“å…¥æ•°æ®")
        return None

    mapping = _load_mapping()
    if not mapping:
        logger.warning("æ²¡æœ‰åŠ è½½åˆ°æ˜ å°„é…ç½®ï¼Œè¿”å›åŸæ•°æ®")
        return data

    try:
        # è·å–æ˜ å°„ä¸­å®šä¹‰çš„å­—æ®µ
        mapped_columns = list(mapping.keys())
        logger.info(f"ğŸ” é…ç½®æ–‡ä»¶ä¸­çš„æ˜ å°„å­—æ®µ: {mapped_columns}")

        # ç­›é€‰æ•°æ®ä¸­å­˜åœ¨çš„æ˜ å°„å­—æ®µ
        existing_columns = [col for col in mapped_columns if col in data.columns]
        logger.info(f"ğŸ¯ æ•°æ®ä¸­å­˜åœ¨çš„æ˜ å°„å­—æ®µ: {existing_columns}")

        if not existing_columns:
            logger.warning("æ•°æ®ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ˜ å°„å­—æ®µ")
            return data

        # ç­›é€‰åˆ—
        filtered_data = data.select(existing_columns)
        logger.info(f"ğŸ”§ ç­›é€‰åæ•°æ®ç»´åº¦: {filtered_data.height}è¡Œ x {filtered_data.width}åˆ—")

        # å°†æ‰€æœ‰åˆ—è½¬ä¸ºUtf8ï¼Œé¿å…æ’å…¥ä¸­æ–‡è¡Œæ—¶ç±»å‹å†²çª
        try:
            filtered_data = filtered_data.select([
                pl.col(c).cast(pl.Utf8).alias(c) for c in filtered_data.columns
            ])
            logger.debug("åˆ—ç±»å‹ç»Ÿä¸€ä¸ºUtf8ä¾¿äºæ’å…¥ä¸­æ–‡è¯´æ˜è¡Œ")
        except Exception as cast_err:
            logger.warning(f"åˆ—ç±»å‹è½¬æ¢Utf8å¤±è´¥(ä¸å½±å“ç»§ç»­): {cast_err}")

        # å¦‚æœç¬¬ä¸€æ¡æ•°æ®è¡Œå·²ç»æ˜¯ä¸­æ–‡(æ£€æµ‹å«ä¸­æ–‡å­—ç¬¦) åˆ™ä¸é‡å¤æ’å…¥
        already_chinese = False
        if filtered_data.height > 0:
            first_val = str(filtered_data[0, existing_columns[0]])
            if re.search(r'[\u4e00-\u9fff]', first_val):
                already_chinese = True
        if not already_chinese:
            chinese_names = [mapping.get(col, col) for col in existing_columns]
            chinese_row_dict = {col: cname for col, cname in zip(existing_columns, chinese_names)}
            logger.info(f"æ’å…¥ä¸­æ–‡å­—æ®µåè¡Œ: {chinese_row_dict}")
            chinese_row = pl.DataFrame([chinese_row_dict])
            filtered_data = pl.concat([chinese_row, filtered_data], how="vertical_relaxed")
            logger.info("ä¸­æ–‡å­—æ®µåè¡Œæ’å…¥å®Œæˆ (ä½œä¸ºæ–‡ä»¶ç¬¬äºŒè¡Œ)")
        else:
            logger.debug("æ£€æµ‹åˆ°å·²æœ‰ä¸­æ–‡è¯´æ˜è¡Œï¼Œè·³è¿‡æ’å…¥")

        logger.info(f"å…±ä¿ç•™ {len(existing_columns)} ä¸ªæ˜ å°„å­—æ®µ: {existing_columns}")

        if show_schema:
            logger.info("å­—æ®µæ˜ å°„è¯´æ˜ (è‹±æ–‡ -> ä¸­æ–‡):")
            for col in existing_columns:
                chinese_name = mapping.get(col, "æœªçŸ¥")
                logger.info(f"  {col} -> {chinese_name}")

        return filtered_data
    except Exception as e:
        logger.error(f"å­—æ®µç­›é€‰å¤±è´¥: {e}")
        return data

def _remove_duplicates(
    df: pl.DataFrame,
    key_columns: list,
    strategy: str = "first"
) -> pl.DataFrame:
    if df is None or df.height == 0:
        return df
    miss = [c for c in key_columns if c not in df.columns]
    if miss:
        raise ValueError(f"å…³é”®å­—æ®µç¼ºå¤±: {miss}")
    if strategy == "latest_ann_date":
        if "ann_date" not in df.columns:
            raise ValueError("latest_ann_date ç­–ç•¥éœ€è¦ ann_date åˆ—")
        try:
            df = df.sort(by=key_columns + ["ann_date"], descending=[False]*len(key_columns) + [True])
        except Exception as se:
            logger.debug(f"latest_ann_date é¢„æ’åºå¤±è´¥(å¿½ç•¥): {se}")
        keep = "first"
    elif strategy in ("first", "last"):
        keep = strategy
    else:
        raise ValueError(f"æœªçŸ¥å»é‡ç­–ç•¥: {strategy}")
    before = df.height
    try:
        try:
            df_u = df.unique(subset=key_columns, keep=keep, maintain_order=True)
        except TypeError:
            df_u = df.unique(subset=key_columns, keep=keep)
        try:
            df_u = df_u.sort(by=key_columns, descending=[False]*len(key_columns))
        except Exception:
            pass
        removed = before - df_u.height
        if removed > 0:
            logger.info(f"å†…éƒ¨å»é‡: strategy={strategy} keys={key_columns} ç§»é™¤={removed} å‰©ä½™={df_u.height}")
        else:
            logger.info(f"å†…éƒ¨å»é‡: æ— é‡å¤ strategy={strategy}")
        return df_u
    except Exception as e:
        logger.warning(f"å†…éƒ¨å»é‡å¤±è´¥(è¿”å›åŸæ•°æ®): {e}")
        return df


def _load_filter_industries() -> List[str]:
    """åŠ è½½ config.yaml ä¸­çš„ filter_industry åˆ—è¡¨"""
    try:
        config_path = Path(__file__).parent / "config.yaml"
        with open(config_path, 'r', encoding='utf-8') as f:
            cfg = yaml.safe_load(f) or {}
            lst = cfg.get('filter_industry') or []
            if not isinstance(lst, list):
                logger.warning("filter_industry ä¸æ˜¯åˆ—è¡¨ç±»å‹ï¼Œå¿½ç•¥")
                return []
            return [str(x).strip() for x in lst if str(x).strip()]
    except Exception as e:
        logger.warning(f"åŠ è½½è¡Œä¸šè¿‡æ»¤åˆ—è¡¨å¤±è´¥: {e}")
        return []


def _read_table_auto(path_str: str,
                     strict: bool = False,
                     label: str = "æ•°æ®æ–‡ä»¶") -> Optional[pl.DataFrame]:
    """ç»Ÿä¸€çš„è¡¨æ ¼è¯»å–é€»è¾‘ (csv/parquet è‡ªåŠ¨å°è¯•)

    Args:
        path_str: æ–‡ä»¶è·¯å¾„
        strict: ä¸¥æ ¼æ¨¡å¼, True æ—¶å¤±è´¥æŠ›å‡ºå¼‚å¸¸
        label: æ—¥å¿—ä¸­æ˜¾ç¤ºçš„æ ‡ç­¾ (ä¾‹å¦‚: åŸºç¡€ä¿¡æ¯ / è´¢åŠ¡æ•°æ®)

    Returns:
        pl.DataFrame or None
    """
    p = Path(path_str)
    if not p.exists():
        msg = f"{label}æ–‡ä»¶ä¸å­˜åœ¨: {path_str}"
        logger.error(msg)
        if strict:
            raise RuntimeError(msg)
        return None
    try:
        suf = p.suffix.lower()
        if suf == '.parquet':
            return pl.read_parquet(p)
        if suf == '.csv':
            return pl.read_csv(p)
        # æœªçŸ¥åç¼€: ä¾æ¬¡å°è¯• parquet -> csv
        try:
            return pl.read_parquet(p)
        except Exception:
            return pl.read_csv(p)
    except Exception as e:
        msg = f"è¯»å–{label}å¤±è´¥: {e}"
        logger.error(msg)
        if strict:
            raise RuntimeError(msg)
        return None


def filter_industry(data: Optional[pl.DataFrame] = None,
                    financial_path: str = "data/polars/final_concat.parquet",
                    basic_info_path: str = "data/stack_basic.csv",
                    industries: Optional[List[str]] = None,
                    join_type: str = "inner",
                    keep_financial: bool = True,
                    keep_columns: Optional[List[str]] = None,
                    strict: bool = False) -> Optional[pl.DataFrame]:
    """
    Args:
        data: å¯é€‰ï¼Œè‹¥ä¼ å…¥åˆ™ä¸å†ä» financial_path è¯»å–ï¼›åº”åŒ…å« ts_code åˆ—
    financial_path: å…¨é‡åˆå¹¶è´¢åŠ¡æ•°æ®è·¯å¾„ (parquet/csv çš†å¯)
    basic_info_path: stock_basic å¯¼å‡ºçš„åŸºç¡€æ–‡ä»¶ (CSV/Parquet å‡å¯, å« ts_code,name,industry)
        industries: ç›´æ¥æŒ‡å®šè¡Œä¸šç™½åå•ï¼›è‹¥ä¸º None åˆ™ä» config.yaml çš„ filter_industry è¯»å–
        join_type: è¿æ¥ç±»å‹ (inner / left)
        keep_financial: æ˜¯å¦ä¿ç•™è´¢åŠ¡æ•°æ®çš„å…¶ä½™åˆ—ï¼›False æ—¶ä»…è¾“å‡º ts_code,name,industry
    keep_columns: å¯é€‰æ‰‹åŠ¨æŒ‡å®šæœ€ç»ˆåˆ—é¡ºåºï¼ˆè‹¥ä¸ºç©ºè‡ªåŠ¨æ¨æ–­ï¼‰

    Returns:
        è¿‡æ»¤åçš„ Polars DataFrame

    strict: ä¸º True æ—¶ï¼Œå…³é”®æ–‡ä»¶ç¼ºå¤±æˆ–è¯»å–å¤±è´¥ç›´æ¥æŠ›å‡º RuntimeErrorï¼Œé˜²æ­¢åç»­èŠ‚ç‚¹è¯¯ç”¨æ—§æ•°æ®
    """
    # 1. è¡Œä¸šç™½åå•
    if industries is None:
        industries = _load_filter_industries()
    if not industries:
        logger.warning("æœªè·å–åˆ°ä»»ä½•è¡Œä¸šç™½åå•(industries)ï¼Œè¿”å›ç©º")
        return None
    industries_set = set(industries)
    logger.info(f"è¡Œä¸šç™½åå•: {industries}")

    # 2. è¯»å–åŸºç¡€ä¿¡æ¯æ˜ å°„
    basic_df = _read_table_auto(basic_info_path, strict=strict, label="åŸºç¡€ä¿¡æ¯")
    if basic_df is None:
        return None
    expected_cols = {"ts_code", "name", "industry"}
    missing_basic = expected_cols - set(basic_df.columns)
    if missing_basic:
        logger.warning(f"åŸºç¡€ä¿¡æ¯ç¼ºå¤±åˆ— {missing_basic}ï¼Œå¯ç”¨åˆ—: {basic_df.columns}")
    # è¿‡æ»¤è¡Œä¸š
    if "industry" in basic_df.columns:
        basic_df = basic_df.filter(pl.col("industry").is_in(list(industries_set)))
    else:
        msg = "ç¼ºå°‘è¡Œä¸šåˆ— 'industry'ï¼Œæ— æ³•æŒ‰è¡Œä¸šè¿‡æ»¤ï¼Œè¿”å›ç©º"
        logger.warning(msg)
        if strict:
            raise RuntimeError(msg)
        return None
    if basic_df.is_empty():
        logger.warning("æŒ‰è¡Œä¸šè¿‡æ»¤ååŸºç¡€ä¿¡æ¯ä¸ºç©º")
        return basic_df
    logger.info(f"åŸºç¡€ä¿¡æ¯è¡Œä¸šè¿‡æ»¤åè¡Œæ•°: {basic_df.height}")

    # 2.1 æ°¸ä¹…æ€§æ’é™¤ *ST ç­‰ç‰¹æ®Šå¤„ç†å…¬å¸ï¼ˆé€€å¸‚/é£é™©é¢„è­¦ï¼‰
    if "name" in basic_df.columns:
        before_st = basic_df.height
        try:
            basic_df = basic_df.filter(~pl.col("name").str.contains(r"\\*ST"))
            removed_st = before_st - basic_df.height
            if removed_st > 0:
                logger.info(f"æ’é™¤ *ST å…¬å¸: ç§»é™¤ {removed_st} è¡Œ (å‰©ä½™ {basic_df.height})")
        except Exception as ste:
            logger.warning(f"æ’é™¤ *ST æ­¥éª¤å¤±è´¥(å¿½ç•¥): {ste}")

    # 3. è´¢åŠ¡æ•°æ®è¯»å–/å‡†å¤‡
    if data is None:
        fin_df = _read_table_auto(financial_path, strict=strict, label="è´¢åŠ¡æ•°æ®")
        if fin_df is None:
            return None
    else:
        fin_df = data
    if "ts_code" not in fin_df.columns:
        msg = "è´¢åŠ¡æ•°æ®ç¼ºå°‘ ts_code åˆ—"
        logger.error(msg)
        if strict:
            raise RuntimeError(msg)
        return None

    # 4. ç”Ÿæˆéœ€è¦è¿æ¥çš„å”¯ä¸€ ts_code é›†åˆ (æå‡ join æ€§èƒ½)
    uniq_codes = fin_df.select("ts_code").unique()
    logger.info(f"è´¢åŠ¡æ•°æ®å”¯ä¸€è‚¡ç¥¨æ•°: {uniq_codes.height}")

    # 5. è¿æ¥åŸºç¡€ä¿¡æ¯ (é¢„å…ˆè¿‡æ»¤è¡Œä¸šåçš„ basic_df)
    how = "inner" if join_type not in ("left", "outer") else "left"
    join_df = uniq_codes.join(basic_df, on="ts_code", how=how)
    logger.info(f"è¡Œä¸šæ˜ å°„ join åè¡Œæ•°: {join_df.height}")
    if join_df.is_empty():
        logger.warning("Join åæ— åŒ¹é…è‚¡ç¥¨ï¼Œè¿”å›ç©º")
        return join_df

    # 6. åˆå¹¶å›è´¢åŠ¡æ•°æ®ï¼ˆå¯é€‰ï¼‰
    if keep_financial:
        merged = join_df.join(fin_df, on="ts_code", how="inner")
    else:
        merged = join_df

    # 7. åˆ—é¡ºåºæ•´ç†: ts_code, name, industry æ”¾æœ€å‰
    front_cols = [c for c in ["ts_code", "name", "industry"] if c in merged.columns]
    other_cols = [c for c in merged.columns if c not in front_cols]
    final_cols = keep_columns if keep_columns else front_cols + other_cols
    merged = merged.select(final_cols)
    logger.info(f"æœ€ç»ˆè¿‡æ»¤åè¡Œæ•°={merged.height} åˆ—æ•°={merged.width}")

    return merged


def compute_hand_metrics(data: Optional[pl.DataFrame] = None,
                         tax_rate: float = 0.25,
                         cast_double: bool = True,
                         inplace: bool = False,
                         strict: bool = False) -> Optional[pl.DataFrame]:
    """ä¸ºè¾“å…¥æ•°æ®è¿½åŠ  5 ä¸ªæ‰‹å·¥è®¡ç®—æŒ‡æ ‡åˆ—:

    1. roe_cal = profit_dedt / bps
    2. roa_cal = netprofit_margin * assets_turn
    3. roic_cal = op_income / (bps * assets_to_eqt)
    4. fcf_margin_ps = fcff_ps / total_revenue_ps
    5. roic_tax = ebit * (1 - tax_rate) / invest_capital

    ä»…åœ¨ä¾èµ–åˆ—å…¨éƒ¨å­˜åœ¨æ—¶æ‰è¿½åŠ è¯¥åˆ—ï¼›ç¼ºå¤±æ—¶è·³è¿‡å¹¶è®°å½•æ—¥å¿—ã€‚

    å‚æ•°:
        data: è¾“å…¥ Polars DataFrame
        tax_rate: ç¨ç‡ (ç”¨äº roic_tax)
        cast_double: æ˜¯å¦å°†å‚ä¸è®¡ç®—çš„åˆ—ç»Ÿä¸€å°è¯•ä¸º Float64 (é˜²æ­¢å­—ç¬¦ä¸²åˆ—å¯¼è‡´ç»“æœ NULL)
        inplace: True æ—¶åŸåœ°åœ¨åŒä¸€ DataFrame ä¸Šè¿½åŠ å¹¶è¿”å›ï¼›False æ—¶å¤åˆ¶ä¸€ä»½
        strict: True æ—¶è‹¥æ‰€æœ‰æ‰‹å·¥åˆ—å‡æœªç”Ÿæˆåˆ™æŠ›å‡ºå¼‚å¸¸
    """
    if data is None:
        logger.warning("æ²¡æœ‰è¾“å…¥æ•°æ® data=None")
        return None
    if not isinstance(data, pl.DataFrame):
        logger.error("compute_hand_metrics ä»…æ”¯æŒ Polars DataFrame")
        return None
    df = data if inplace else data.clone()

    existing = set(df.columns)
    deps = {
        'roe_cal': ['profit_dedt', 'bps'],
        'roa_cal': ['netprofit_margin', 'assets_turn'],
        'roic_cal': ['op_income', 'bps', 'assets_to_eqt'],
        'fcf_margin_ps': ['fcff_ps', 'total_revenue_ps'],
        'roic_tax': ['ebit', 'invest_capital'],
    }
    produced = []
    def all_exists(cols: list[str]) -> bool:
        return all(c in existing for c in cols)
    def col_expr(name: str):
        c = pl.col(name)
        return c.cast(pl.Float64) if cast_double else c

    try:
        if all_exists(deps['roe_cal']):
            df = df.with_columns(
                (pl.when(col_expr('bps') > 0)
                 .then(col_expr('profit_dedt') / pl.when(col_expr('bps') == 0).then(None).otherwise(col_expr('bps')))
                 .otherwise(None)
                 ).alias('roe_cal')
            )
            produced.append('roe_cal')
        else:
            logger.info("è·³è¿‡ roe_cal: ç¼ºå¤±åˆ— %s", [c for c in deps['roe_cal'] if c not in existing])

        if all_exists(deps['roa_cal']):
            df = df.with_columns((col_expr('netprofit_margin') * col_expr('assets_turn')).alias('roa_cal'))
            produced.append('roa_cal')
        else:
            logger.info("è·³è¿‡ roa_cal: ç¼ºå¤±åˆ— %s", [c for c in deps['roa_cal'] if c not in existing])

        if all_exists(deps['roic_cal']):
            df = df.with_columns(
                (pl.when((col_expr('bps') > 0) & (col_expr('assets_to_eqt') != 0))
                 .then(col_expr('op_income') / pl.when((col_expr('bps') * col_expr('assets_to_eqt')) == 0).then(None).otherwise(col_expr('bps') * col_expr('assets_to_eqt')))
                 .otherwise(None)
                 ).alias('roic_cal')
            )
            produced.append('roic_cal')
        else:
            logger.info("è·³è¿‡ roic_cal: ç¼ºå¤±åˆ— %s", [c for c in deps['roic_cal'] if c not in existing])

        if all_exists(deps['fcf_margin_ps']):
            df = df.with_columns(
                (pl.when((col_expr('total_revenue_ps').is_not_null()) & (col_expr('total_revenue_ps') != 0))
                 .then(col_expr('fcff_ps') / pl.when(col_expr('total_revenue_ps') == 0).then(None).otherwise(col_expr('total_revenue_ps')))
                 .otherwise(None)
                 ).alias('fcf_margin_ps')
            )
            produced.append('fcf_margin_ps')
        else:
            logger.info("è·³è¿‡ fcf_margin_ps: ç¼ºå¤±åˆ— %s", [c for c in deps['fcf_margin_ps'] if c not in existing])

        if all_exists(deps['roic_tax']):
            df = df.with_columns(
                (pl.when(col_expr('invest_capital') > 0)
                 .then(col_expr('ebit') * (1 - tax_rate) / pl.when(col_expr('invest_capital') == 0).then(None).otherwise(col_expr('invest_capital')))
                 .otherwise(None)
                 ).alias('roic_tax')
            )
            produced.append('roic_tax')
        else:
            logger.info("è·³è¿‡ roic_tax: ç¼ºå¤±åˆ— %s", [c for c in deps['roic_tax'] if c not in existing])
    except Exception as e:
        logger.error(f"æ‰‹å·¥æŒ‡æ ‡è®¡ç®—è¿‡ç¨‹å¼‚å¸¸: {e}")
        if strict:
            raise

    if not produced:
        msg = "æœªç”Ÿæˆä»»ä½•æ‰‹å·¥æŒ‡æ ‡åˆ—"
        if strict:
            raise RuntimeError(msg)
        logger.warning(msg)
    else:
        logger.info(f"è¿½åŠ æ‰‹å·¥æŒ‡æ ‡åˆ—: {produced}")
    return df
