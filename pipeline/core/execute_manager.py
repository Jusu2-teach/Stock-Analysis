#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""ExecuteManager (ç²¾ç®€ç‰ˆ)

èŒè´£ï¼š
1. è§£æ YAML steps -> è§„èŒƒåŒ–å†…éƒ¨ StepSpec & æ‹“æ‰‘æ’åº
2. æ„å»ºè‡ªåŠ¨ Kedro é£æ ¼èŠ‚ç‚¹é…ç½®ï¼ˆå»¶è¿Ÿå¼•æ“ç»‘å®šç”± MethodHandle è´Ÿè´£ï¼‰
3. è°ƒç”¨æ··åˆæ‰§è¡Œ FlowExecutor (Prefect åŒ…è£… Kedro)
4. æ±‡æ€»è¾“å‡º / ç¼“å­˜ / è¡€ç¼˜ / æŒ‡æ ‡

å·²ç§»é™¤ï¼šå†å²å¤šå¼•æ“æ¨¡å¼ / æ—§ CLI å…¼å®¹å±‚ / å†—ä½™ run/debug æ¥å£
"""
import sys
import os
import re
import yaml
import hashlib
from pathlib import Path
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Set
from collections import defaultdict, deque
import logging
from datetime import datetime

# è·¯å¾„æ³¨å…¥
sys.path.append(str(Path(__file__).parent.parent.parent / "src"))
# orchestrator å·²ç§»è‡³æ ¹ç›®å½•,æ·»åŠ åˆ°æœç´¢è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))
try:
    # ä½¿ç”¨æ–°ç‰ˆ v4.0 orchestrator facade (å·²ç§»è‡³æ ¹ç›®å½•)
    from orchestrator import AStockOrchestrator
except ImportError as e:  # pragma: no cover
    print(f"âŒ å¯¼å…¥æ–°ç‰ˆ orchestrator å¤±è´¥: {e}")
    raise


from pipeline.core.services.config_service import ConfigService, StepSpec, StepOutput
from pipeline.core.services.result_assembler import ResultAssembler
from pipeline.core.services.runtime_param_service import RuntimeParamService
from pipeline.core.services.flow_executor import FlowExecutor
from pipeline.core.services.cache_stats_service import CacheStatsService

class ExecuteManager:
    """æ ¸å¿ƒæ‰§è¡Œç®¡ç†å™¨ (Hybrid-only)

    åŠŸèƒ½èšç„¦ï¼š
    - è§£æ YAML steps -> ç”Ÿæˆè‡ªåŠ¨ kedro é£æ ¼èŠ‚ç‚¹æè¿°
    - è§£æè·¨æ­¥å¼•ç”¨ (steps.<step>.outputs.parameters.<name>)
    - ç»Ÿä¸€ç» PrefectEngine (å†…éƒ¨å°è£… KedroEngine) æ‰§è¡Œ
    - æä¾›ç¼“å­˜/è½¯å¤±è´¥/è¡€ç¼˜/æŒ‡æ ‡ ç»“æœå¯¹ä¸Šå±‚æš´éœ²
    - å»é™¤å…¨éƒ¨å†å²ç‹¬ç«‹ Kedro / å…¶å®ƒå¼•æ“æ¨¡å¼ä»£ç 
    """

    REF_PATTERN = re.compile(r"^steps\.(?P<step>[^.]+)\.outputs\.parameters\.(?P<param>[^.]+)$")

    def __init__(self, config_path: Optional[str] = None, orchestrator: 'AStockOrchestrator' = None):
        self.logger = logging.getLogger("AStockExecuteManager")
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
            self.logger.addHandler(handler)
        self.logger.setLevel(logging.INFO)
        self.config_path = config_path
        self.config: Optional[Dict[str, Any]] = None
        # åªä¿ç•™ä¸€ä¸ª orchestrator å®ä¾‹ï¼ˆå¯å¤–éƒ¨æ³¨å…¥ï¼Œæœªæä¾›åˆ™æœ¬åœ°å®ä¾‹åŒ–ï¼‰
        self.orchestrator = orchestrator if orchestrator is not None else AStockOrchestrator()
        # å…¨å±€å¼•ç”¨/æ³¨å†Œè¡¨
        self.reference_values: Dict[str, Any] = {}
        self.global_registry: Dict[str, Any] = {}
        self.reference_to_hash: Dict[str, str] = {}
        self.steps: Dict[str, StepSpec] = {}
        self.execution_order: List[str] = []
        # services (åˆ†å±‚åçš„èŒè´£æ‹†åˆ†)
        self._config_service = ConfigService(self)
        self._result_assembler = ResultAssembler(self)
        self._runtime_param_service = RuntimeParamService(self)
        self._flow_executor = FlowExecutor(self)
    # å·²ç§»é™¤è‡ªåŠ¨å¤šè¾“å…¥æ¨æ–­æœåŠ¡ï¼Œä¿æŒæ˜¾å¼å‚æ•°/inputs åˆ—è¡¨é£æ ¼
        self._cache_stats_service = CacheStatsService(self)
        # å°è¯•æ’ä»¶è‡ªåŠ¨å‘ç° (å¯é€‰ç›®å½• pipeline/plugins)
        try:
            from pipeline.core.services.hook_manager import HookManager
            import importlib, pkgutil, pathlib
            plugins_dir = Path(__file__).parent.parent / 'plugins'
            if plugins_dir.is_dir():
                # æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡ç¦ç”¨æ’ä»¶: PIPELINE_DISABLE_PLUGINS=log,prometheus_plugin
                disable_raw = (Path.cwd() / '.pipeline_disable_plugins').read_text(encoding='utf-8').strip() if (Path.cwd() / '.pipeline_disable_plugins').exists() else ''
                disable_env = os.getenv('PIPELINE_DISABLE_PLUGINS', '')
                disabled = {x.strip() for x in (disable_env + ',' + disable_raw).split(',') if x.strip()}
                for m in pkgutil.iter_modules([str(plugins_dir)]):
                    if m.name in disabled:
                        self.logger.info(f"ğŸš« è·³è¿‡æ’ä»¶(è¢«ç¦ç”¨): {m.name}")
                        continue
                    try:
                        mod = importlib.import_module(f'pipeline.plugins.{m.name}')
                        if hasattr(mod, 'register'):
                            mod.register(HookManager.get())
                            self.logger.info(f"ğŸ”Œ å·²åŠ è½½æ’ä»¶: {m.name}")
                    except Exception as e:  # pragma: no cover
                        self.logger.warning(f"æ’ä»¶åŠ è½½å¤±è´¥ {m.name}: {e}")
        except Exception:
            pass

    # ------------------------------------------------------------------
    # é…ç½®è§£æ
    # ------------------------------------------------------------------
    def load_config(self, path: Optional[str] = None) -> Dict[str, Any]:
        path = path or self.config_path
        if not path:
            raise ValueError("æœªæä¾›é…ç½®è·¯å¾„")
        return self._config_service.load_config(path)

    def _hash_reference(self, ref: str) -> str:  # å¤–éƒ¨æœåŠ¡ä»ä¼šç”¨
        return hashlib.md5(ref.encode('utf-8')).hexdigest()[:16]

    def rebuild_after_filter(self):
        """åœ¨ steps è¿‡æ»¤åé‡å»ºå†…éƒ¨ç»“æ„ (parse + topo)ã€‚"""
        self.steps.clear()
        self.execution_order.clear()
        # ç›´æ¥è°ƒç”¨æœåŠ¡å†…éƒ¨æ–¹æ³•
        self._config_service._parse_steps()
        self._config_service._compute_execution_order()

    # ------------------------------------------------------------------
    # æ‰§è¡Œ
    # ------------------------------------------------------------------
    def execute_pipeline(self, engine: str | None = None) -> Dict[str, Any]:
        """ç»Ÿä¸€å…¥å£ï¼šä»…ä¿ç•™ Hybrid (Prefect+Kedro) æ¨¡å¼ã€‚engine å‚æ•°å¿½ç•¥ã€‚"""
        if self.config is None:
            self.load_config()
        # æ„å»º auto pipeline ä¾›æ··åˆå¼•æ“å†…éƒ¨ä½¿ç”¨
        auto_info = self._build_auto_kedro_config()
        result = self._flow_executor.run(auto_info)
        # ç»Ÿä¸€æ ‡è¯†æ¨¡å¼
        result['mode'] = 'hybrid'
        return result

    # ------------------ Introspection (for CLI status/engines) ---------
    def get_available_engines(self) -> Dict[str, Any]:
        """è¿”å›å½“å‰æ³¨å†Œä¸­å¿ƒçš„ç»„ä»¶/æ–¹æ³•/å¼•æ“å…ƒæ•°æ® (ä¾› CLI status / engines).

        ç»“æ„:
        {
          'components': ['compA', 'compB', ...],
          'methods': {
              'component::engine_type::method': {
                  'component_type': ..., 'engine_type': ..., 'engine_name': ..., 'description': ...
              },
              ...
          }
        }
        """
        try:
            registry = self.orchestrator.registry
            methods = registry.list_methods()
            components = list(registry.index.by_component.keys())
            return {
                'components': components,
                'methods': methods
            }
        except Exception as e:  # pragma: no cover
            self.logger.warning(f"get_available_engines failed: {e}")
            return {'components': [], 'methods': {}}

    # ------------------ è‡ªåŠ¨æ„å»º kedro pipeline é…ç½® ------------------
    def _dataset_name(self, step: str, output: str) -> str:
        # ç»Ÿä¸€æ•°æ®é›†å‘½åï¼šstep__output (é¿å…ç‚¹å·ï¼Œæ›´é€‚åˆæŸäº›åç«¯)
        return f"{step}__{output}".replace('-', '_')

    def _build_auto_kedro_config(self) -> Dict[str, Any]:
        return self._config_service.build_auto_nodes()

    # å•ç‹¬ Kedro æ¨¡å¼å·²ç§»é™¤

    # ------------------ Prefect è¿è¡Œ ------------------
    # _run_prefect å·²æ‹†åˆ†åˆ° FlowExecutor

    # ------------------ ä» catalog æ³¨å†Œè¾“å‡º ------------------
    # _register_catalog_outputs é€»è¾‘å·²åˆå¹¶è¿› ResultAssembler.register_catalog

    # ä¾›å¤–éƒ¨å¼•æ“åœ¨èŠ‚ç‚¹è¿è¡Œæ—¶è§£æå‚æ•°å¼•ç”¨ï¼ˆç›´æ¥å§”æ´¾ serviceï¼‰
    def resolve_runtime_params_for_engine(self, params: Dict[str, Any]) -> Dict[str, Any]:  # ä¿ç•™æœ€å°å…¥å£
        return self._runtime_param_service.resolve(params)


    # å…¼å®¹æ—§æ¥å£ run / debug_registry å·²ç§»é™¤ï¼Œå¤–éƒ¨åº”ç»Ÿä¸€ä½¿ç”¨ execute_pipeline()

    # ------------------ å·¥å…·æ–¹æ³• ------------------
    @staticmethod
    def clear_cache(cache_dir: str = '.pipeline/cache') -> None:
        """æ¸…é™¤æŒä¹…åŒ–ç¼“å­˜ç›®å½• (æµ‹è¯•æˆ–è°ƒè¯•ä½¿ç”¨)ã€‚"""
        import shutil, os
        try:
            if os.path.isdir(cache_dir):
                shutil.rmtree(cache_dir)
        except Exception:  # pragma: no cover
            pass


# å…¥å£
def main():  # pragma: no cover
    import argparse
    parser = argparse.ArgumentParser(description="ExecuteManager Runner")
    parser.add_argument('-c', '--config', required=True, help='Pipeline YAML è·¯å¾„')
    parser.add_argument('--log', default='INFO')
    args = parser.parse_args()
    logging.basicConfig(level=getattr(logging, args.log.upper(), logging.INFO))
    mgr = ExecuteManager(args.config)
    mgr.load_config()
    result = mgr.execute_pipeline()
    print(result.get('status'), 'executed_steps=', result.get('executed_steps'))


if __name__ == '__main__':  # pragma: no cover
    main()