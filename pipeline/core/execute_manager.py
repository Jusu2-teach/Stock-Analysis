#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""ExecuteManager - Pipeline æ‰§è¡Œç®¡ç†å™¨

èŒè´£ï¼š
1. è§£æ YAML steps -> è§„èŒƒåŒ–å†…éƒ¨ StepSpec & æ‹“æ‰‘æ’åº
2. æ„å»ºè‡ªåŠ¨ Kedro é£æ ¼èŠ‚ç‚¹é…ç½®ï¼ˆå»¶è¿Ÿå¼•æ“ç»‘å®šç”± MethodHandle è´Ÿè´£ï¼‰
3. è°ƒç”¨æ··åˆæ‰§è¡Œ FlowExecutor (Prefect åŒ…è£… Kedro)
4. æ±‡æ€»è¾“å‡º / ç¼“å­˜ / è¡€ç¼˜ / æŒ‡æ ‡
"""
import sys
import os
import re
import yaml
import hashlib
from pathlib import Path
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Set
from collections import defaultdict, deque
import logging
from datetime import datetime

# è·¯å¾„æ³¨å…¥
sys.path.append(str(Path(__file__).parent.parent.parent / "src"))
# orchestrator å·²ç§»è‡³æ ¹ç›®å½•,æ·»åŠ åˆ°æœç´¢è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))
try:
    # ä½¿ç”¨æ–°ç‰ˆ v4.0 orchestrator facade (å·²ç§»è‡³æ ¹ç›®å½•)
    from orchestrator import AStockOrchestrator
except ImportError as e:  # pragma: no cover
    print(f"âŒ å¯¼å…¥æ–°ç‰ˆ orchestrator å¤±è´¥: {e}")
    raise


from pipeline.core.context import PipelineContext
from pipeline.core.services.config_service import ConfigService
from pipeline.core.services.result_assembler import ResultAssembler
from pipeline.core.services.runtime_param_service import RuntimeParamService
from pipeline.core.services.flow_executor import FlowExecutor
from pipeline.core.services.cache_stats_service import CacheStatsService

class ExecuteManager:
    """Pipeline æ‰§è¡Œç®¡ç†å™¨ï¼ˆHybrid æ¨¡å¼ï¼‰

    åŠŸèƒ½ï¼š
    - è§£æ YAML steps -> ç”Ÿæˆ Kedro é£æ ¼èŠ‚ç‚¹æè¿°
    - è§£æè·¨æ­¥å¼•ç”¨ (steps.<step>.outputs.parameters.<name>)
    - é€šè¿‡ PrefectEngine (å†…éƒ¨å°è£… KedroEngine) æ‰§è¡Œ
    - æä¾›ç¼“å­˜/è½¯å¤±è´¥/è¡€ç¼˜/æŒ‡æ ‡ç»“æœ
    """

    REF_PATTERN = re.compile(r"^steps\.(?P<step>[^.]+)\.outputs\.parameters\.(?P<param>[^.]+)$")

    def __init__(self, config_path: Optional[str] = None, orchestrator: 'AStockOrchestrator' = None):
        self.logger = logging.getLogger("AStockExecuteManager")
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
            self.logger.addHandler(handler)
        self.logger.setLevel(logging.INFO)

        self.config_path = config_path
        self.orchestrator = orchestrator if orchestrator is not None else AStockOrchestrator()

        # Pipeline æ‰§è¡Œä¸Šä¸‹æ–‡ï¼ˆå…±äº«çŠ¶æ€ï¼‰
        self.ctx = PipelineContext()

        # æœåŠ¡å±‚ï¼ˆèŒè´£åˆ†ç¦» - å…¨éƒ¨åŸºäº Contextï¼‰
        self._config_service = ConfigService(self.ctx, self.logger)
        self._result_assembler = ResultAssembler(self.ctx, self.logger)
        self._runtime_param_service = RuntimeParamService(self.ctx, self.logger)
        self._cache_stats_service = CacheStatsService(self.logger)
        self._flow_executor = FlowExecutor(
            self.ctx,
            self._result_assembler,
            self._cache_stats_service,
            self.logger
        )
        # æ’ä»¶è‡ªåŠ¨å‘ç°
        self._load_plugins()

    # ------------------------------------------------------------------
    # é…ç½®è§£æ
    # ------------------------------------------------------------------
    def _load_plugins(self):
        """åŠ è½½ pipeline/plugins ç›®å½•ä¸‹çš„æ’ä»¶"""
        from pipeline.core.services.hook_manager import HookManager
        import importlib
        import pkgutil

        plugins_dir = Path(__file__).parent.parent / 'plugins'
        if not plugins_dir.is_dir():
            return

        # è·å–ç¦ç”¨æ’ä»¶åˆ—è¡¨
        disabled = {x.strip() for x in os.getenv('PIPELINE_DISABLE_PLUGINS', '').split(',') if x.strip()}
        disable_file = Path.cwd() / '.pipeline_disable_plugins'
        if disable_file.exists():
            disabled.update(x.strip() for x in disable_file.read_text(encoding='utf-8').split(',') if x.strip())

        # åŠ è½½æ’ä»¶
        for module_info in pkgutil.iter_modules([str(plugins_dir)]):
            if module_info.name in disabled:
                self.logger.info(f"ğŸš« è·³è¿‡æ’ä»¶: {module_info.name}")
                continue

            try:
                mod = importlib.import_module(f'pipeline.plugins.{module_info.name}')
                if hasattr(mod, 'register'):
                    mod.register(HookManager.get())
                    self.logger.info(f"ğŸ”Œ å·²åŠ è½½æ’ä»¶: {module_info.name}")
            except Exception as e:
                self.logger.warning(f"æ’ä»¶åŠ è½½å¤±è´¥ {module_info.name}: {e}")

    # ------------------------------------------------------------------
    # é…ç½®è§£æ
    # ------------------------------------------------------------------
    def load_config(self, path: Optional[str] = None) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶ï¼ˆå­˜å‚¨åœ¨ context ä¸­ï¼‰"""
        path = path or self.config_path
        if not path:
            raise ValueError("æœªæä¾›é…ç½®è·¯å¾„")
        return self._config_service.load_config(path)

    def rebuild_after_filter(self):
        """åœ¨ steps è¿‡æ»¤åé‡å»ºå†…éƒ¨ç»“æ„ (parse + topo)ã€‚"""
        self.ctx.clear_steps()
        # ç›´æ¥è°ƒç”¨æœåŠ¡å†…éƒ¨æ–¹æ³•
        self._config_service._parse_steps()
        self._config_service._compute_execution_order()

    # ------------------------------------------------------------------
    # æ‰§è¡Œ
    # ------------------------------------------------------------------
    def execute_pipeline(self) -> Dict[str, Any]:
        """æ‰§è¡Œ Pipelineï¼ˆHybrid æ¨¡å¼ï¼šPrefect + Kedroï¼‰

        Returns:
            æ‰§è¡Œç»“æœå­—å…¸ï¼ŒåŒ…å« status/executed_steps/outputs/metrics ç­‰
        """
        if not self.ctx.config:
            self.load_config()

        auto_info = self._build_auto_kedro_config()
        result = self._flow_executor.run(auto_info, manager=self)
        result['mode'] = 'hybrid'
        return result

    # ------------------ Introspection ---------
    def get_available_engines(self) -> Dict[str, Any]:
        """è¿”å›æ³¨å†Œä¸­å¿ƒçš„ç»„ä»¶/æ–¹æ³•/å¼•æ“å…ƒæ•°æ®ï¼ˆä¾› CLI ä½¿ç”¨ï¼‰"""
        registry = self.orchestrator.registry
        return {
            'components': list(registry.index.by_component.keys()),
            'methods': registry.list_methods()
        }

    # ------------------ å†…éƒ¨é…ç½®æ„å»º ------------------
    def _dataset_name(self, step: str, output: str) -> str:
        """ç”Ÿæˆæ•°æ®é›†åç§°ï¼ˆå§”æ‰˜ç»™ contextï¼‰"""
        return self.ctx.dataset_name(step, output)

    def _build_auto_kedro_config(self) -> Dict[str, Any]:
        """æ„å»ºè‡ªåŠ¨ Kedro èŠ‚ç‚¹é…ç½®"""
        return self._config_service.build_auto_nodes()

    def resolve_runtime_params_for_engine(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """è§£æè¿è¡Œæ—¶å‚æ•°å¼•ç”¨ï¼ˆä¾›å¼•æ“è°ƒç”¨ï¼‰"""
        return self._runtime_param_service.resolve(params)

    # ------------------ å·¥å…·æ–¹æ³• ------------------
    @staticmethod
    def clear_cache(cache_dir: str = '.pipeline/cache') -> None:
        """æ¸…é™¤æŒä¹…åŒ–ç¼“å­˜ç›®å½•"""
        import shutil
        if os.path.isdir(cache_dir):
            shutil.rmtree(cache_dir)