import inspect
import time
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Callable, Tuple, Union, Protocol

import pandas as pd


class CacheStrategy(Protocol):
    def hit(self, outputs: List[str], global_catalog: Dict[str, Any]) -> bool: ...
    def record(self, outputs: List[str], produced: Dict[str, Any], global_catalog: Dict[str, Any]): ...


class SimplePresenceCache:
    """æœ€ç®€å•ç¼“å­˜ï¼šæ‰€æœ‰è®¡åˆ’è¾“å‡ºå­˜åœ¨å³è§†ä¸ºå‘½ä¸­"""
    def hit(self, outputs: List[str], global_catalog: Dict[str, Any]) -> bool:
        return bool(outputs) and all(o in global_catalog for o in outputs)
    def record(self, outputs: List[str], produced: Dict[str, Any], global_catalog: Dict[str, Any]):
        for k, v in produced.items():
            global_catalog[k] = v


@dataclass
class InputSpec:
    name: str
    alias: Optional[str] = None
    required: bool = False
    kind: str = "dataset"  # dataset | param | artifact | model


@dataclass
class OutputSpec:
    name: str
    source_key: Optional[str] = None  # ç”¨äº dict é‡æ˜ å°„
    primary: bool = False
    kind: str = "dataset"


@dataclass
class NodeIOConfig:
    step_name: str
    inputs: List[InputSpec] = field(default_factory=list)
    outputs: List[OutputSpec] = field(default_factory=list)
    primary_output: Optional[str] = None
    strict_schema: bool = False
    cache_strategy: CacheStrategy = field(default_factory=SimplePresenceCache)


@dataclass
class ResolvedInputs:
    ordered: List[Any] = field(default_factory=list)         # ä½ç½®å‚æ•°é¡ºåº
    mapping: Dict[str, Any] = field(default_factory=dict)    # æŒ‰åç§°æ˜ å°„
    aggregated: List[Any] = field(default_factory=list)      # è‡ªåŠ¨èšåˆ inputs
    aggregated_map: Dict[str, Any] = field(default_factory=dict)


@dataclass
class CapturedOutputs:
    produced: Dict[str, Any] = field(default_factory=dict)
    primary_output: Optional[str] = None
    tuple_result: Tuple[Any, ...] = tuple()


class IOManager:
    """é›†ä¸­å¼ I/O è§£æä¸ç»‘å®šç®¡ç†å™¨

    ç›®æ ‡ï¼š
    1. æŠ½ç¦»ç¡¬ç¼–ç çš„å¤šè¾“å…¥/å¤šè¾“å‡ºå¤„ç†é€»è¾‘
    2. ç»Ÿä¸€æ”¯æŒ Argo é£æ ¼çš„ *å‚æ•°* ä¸ *æ•°æ®é›†/å·¥ä»¶* åŒºåˆ†ï¼ˆé¢„ç•™ï¼‰
    3. æ”¯æŒå¤šç§è¿”å›ç±»å‹ï¼šå•å¯¹è±¡ / tuple / dict
    4. primary_output è£å‰ªç­–ç•¥é›†ä¸­ç®¡ç†
    5. æœªæ¥å¯æ’æ‹”ç­–ç•¥ï¼šå‘½åè§„èŒƒã€åŠ¨æ€é€‰æ‹©å™¨ã€Jinja æ¨¡æ¿ã€ç¼“å­˜ç­–ç•¥ã€ç‰©åŒ–å±‚æ’ä»¶
    """

    def __init__(self, global_catalog: Dict[str, Any], logger, strict_pipeline: bool = False):
        self.global_catalog = global_catalog
        self.logger = logger
        self.strict_pipeline = strict_pipeline

    # ----------------------------------------------------------------------------
    # æ„å»ºé…ç½®å¯¹è±¡ï¼ˆä»åŸå§‹ node_config æå–ï¼‰
    # ----------------------------------------------------------------------------
    def build_config(self, node_config: Dict[str, Any]) -> NodeIOConfig:
        step_name = node_config.get('name') or node_config.get('id') or 'unknown_step'

        raw_inputs = node_config.get('inputs') or []
        input_specs: List[InputSpec] = []
        for item in raw_inputs:
            if isinstance(item, dict):
                input_specs.append(InputSpec(
                    name=item.get('name'),
                    alias=item.get('alias'),
                    required=bool(item.get('required', False)),
                    kind=item.get('kind', 'dataset')
                ))
            else:
                input_specs.append(InputSpec(name=item))

        raw_outputs = node_config.get('outputs') or []
        output_specs: List[OutputSpec] = []
        # å…¼å®¹åˆ—è¡¨ + å«å­—å…¸ + åˆæˆ parameter æ•°æ®é›† (name: <step>__param__<param>)
        for item in raw_outputs:
            if isinstance(item, dict):
                name = item.get('name')
                kind = item.get('kind', 'dataset')
                src = item.get('source_key') or item.get('from')
                output_specs.append(OutputSpec(name=name, source_key=src, primary=False, kind=kind))
            else:
                name = item
                kind = 'dataset'
                if isinstance(name, str) and '__param__' in name:
                    kind = 'parameter'
                output_specs.append(OutputSpec(name=name, primary=False, kind=kind))
        # primary è§£æ
        primary_decl = node_config.get('primary_output')
        if primary_decl:
            for spec in output_specs:
                spec.primary = (spec.name == primary_decl)
        else:
            # é€‰æ‹©ç¬¬ä¸€ä¸ª dataset è¾“å‡ºä¸º primary
            for spec in output_specs:
                if spec.kind == 'dataset':
                    spec.primary = True
                    break
        primary = next((s.name for s in output_specs if s.primary), None)

        cfg = NodeIOConfig(
            step_name=step_name,
            inputs=input_specs,
            outputs=output_specs,
            primary_output=primary,
            strict_schema=False
        )
        return cfg

    # ----------------------------------------------------------------------------
    # è¾“å…¥è§£æï¼šæ ¹æ®ä¸Šæ¸¸ä¼ å…¥ *args (Kedro runtime) ä¸ node_config æè¿°æ„é€  ResolvedInputs
    # ----------------------------------------------------------------------------
    def resolve_inputs(self, cfg: NodeIOConfig, raw_args: Tuple[Any, ...]) -> ResolvedInputs:
        resolved = ResolvedInputs()
        declared_names = [i.alias or i.name for i in cfg.inputs]
        for idx, val in enumerate(raw_args):
            key = declared_names[idx] if idx < len(declared_names) else f"_arg{idx}"
            resolved.mapping[key] = val
            resolved.ordered.append(val)
        if len(resolved.mapping) > 1:
            resolved.aggregated = list(resolved.mapping.values())
            resolved.aggregated_map = dict(resolved.mapping)
        elif len(resolved.mapping) == 1:
            pass
        return resolved

    # ----------------------------------------------------------------------------
    # å‚æ•°ç»‘å®šï¼šæ ¹æ®å‡½æ•°ç­¾åå’Œå·²è§£æè¾“å…¥ç”Ÿæˆæœ€ç»ˆè°ƒç”¨å‚æ•°
    # ----------------------------------------------------------------------------
    def bind_call_params(self, callable_obj: Callable, base_params: Dict[str, Any], resolved: ResolvedInputs, previous_result: Any = None) -> Dict[str, Any]:
        target = self._unwrap(callable_obj)
        sig = inspect.signature(target)
        params = dict(base_params)
        strict_mode = (str(self.strict_pipeline).lower() == 'true') or (str(
            # å…è®¸é€šè¿‡ç¯å¢ƒå˜é‡æ˜¾å¼å¼€å¯ä¸¥æ ¼æ¨¡å¼ (ä¸æ³¨å…¥ä»»ä½•éšå¼åˆ«å)
            __import__('os').getenv('ASTOCK_STRICT_PARAMS', '0')
        ) == '1')

        # é“¾å¼ç»“æœä¼ é€’ï¼ˆä»…é™å¤šæ–¹æ³•é“¾åœºæ™¯ï¼‰:
        # æ—§é€»è¾‘: æ³¨å…¥ data/df/dataset ä¸‰ç§åˆ«åå¹¶ fallback data
        # æ–°é€»è¾‘: åªåœ¨éä¸¥æ ¼æ¨¡å¼ä¸‹ï¼Œä¸” YAML æœªæ˜¾å¼æä¾›ç›¸åº”å‚æ•°æ—¶ï¼Œå°è¯•â€œæ™ºèƒ½æ¨æ–­ä¸€ä¸ªæœ€å¯èƒ½çš„å•ä¸€å½¢å‚â€å†æ³¨å…¥ï¼›
        # ä¸¥æ ¼æ¨¡å¼: å®Œå…¨ä¸è‡ªåŠ¨æ³¨å…¥ï¼Œå¿…é¡»æ˜¾å¼åœ¨ YAML å†™å‡ºã€‚
        if previous_result is not None and not strict_mode:
            # æ‰¾å°šæœªæä¾›ä¸”æ— é»˜è®¤å€¼çš„ POSITIONAL_OR_KEYWORD å½¢å‚é›†åˆ
            candidate_params = [
                (n, p) for n, p in sig.parameters.items()
                if n not in params
                and p.kind in (p.POSITIONAL_ONLY, p.POSITIONAL_OR_KEYWORD)
            ]
            # è‹¥åªæœ‰ 1 ä¸ªæœªèµ‹å€¼ä¸”æ— é»˜è®¤å€¼çš„å‚æ•° -> æ³¨å…¥é“¾å¼ç»“æœ
            filtered = [c for c in candidate_params if c[1].default is c[1].empty]
            if len(filtered) == 1:
                pname = filtered[0][0]
                params[pname] = previous_result
            # å¦åˆ™è‹¥æ˜¾å¼å­˜åœ¨ "data" å‚æ•°åå°šæœªæä¾›ï¼Œå¸¸è§çº¦å®šä¼˜å…ˆ
            elif 'data' in sig.parameters and 'data' not in params:
                params['data'] = previous_result
            # å…¶å®ƒæƒ…å†µï¼šä¸æ³¨å…¥ï¼Œä¿æŒæ˜¾å¼ç­–ç•¥
        # ä¸¥æ ¼æ¨¡å¼ä¸‹ previous_result å®Œå…¨ä¸æ³¨å…¥ï¼›ç”¨æˆ·éœ€åœ¨ YAML ç”¨å¼•ç”¨æ˜¾å¼ä¼ é€’

        # æŒ‰åç§°åŒ¹é… declared inputs
        for name, val in resolved.mapping.items():
            if name in sig.parameters and name not in params:
                params[name] = val

        # ç§»é™¤: å•è¾“å…¥åˆ«å(data/df/dataset)è‡ªåŠ¨æ³¨å…¥ã€‚æ˜¾å¼å³ä¸€åˆ‡ã€‚

        # å¤šè¾“å…¥ï¼šæä¾›èšåˆ forms
        if resolved.aggregated and 'inputs' in sig.parameters:
            params.setdefault('inputs', resolved.aggregated)
        if resolved.aggregated_map and 'inputs_map' in sig.parameters:
            params.setdefault('inputs_map', resolved.aggregated_map)

        # å¦‚æœå‡½æ•°æ¥å— **kwargs åˆ™å…è®¸é™„åŠ ï¼›å¦åˆ™è£å‰ª
        has_var_kw = any(p.kind == p.VAR_KEYWORD for p in sig.parameters.values())
        if not has_var_kw:
            params = {k: v for k, v in params.items() if k in sig.parameters}
        return params

    # ----------------------------------------------------------------------------
    # è¾“å‡ºæ•è·ï¼šæ ¹æ® node_config è§„èŒƒåŒ– raw_result (dict/tuple/å•å€¼)
    # ----------------------------------------------------------------------------
    def capture_outputs(self, cfg: NodeIOConfig, raw_result: Any) -> CapturedOutputs:
        cap = CapturedOutputs(primary_output=cfg.primary_output)
        output_names = [o.name for o in cfg.outputs]
        if not output_names:
            return cap
        # dict ç»“æœï¼šè¿›è¡Œé”®æ˜ å°„ï¼ˆé€‚é… dataset + parameterï¼‰
        if isinstance(raw_result, dict) and len(output_names) > 0:
            mapped_vals: List[Any] = []
            raw_keys = list(raw_result.keys())
            used_keys: set = set()

            def infer_key(out_nm: str) -> Optional[str]:
                # 1) æ˜ç¡®æ˜ å°„è¡¨
                # legacy output_key_map å·²ç§»é™¤
                # 2) åŒå
                if out_nm in raw_result:
                    return out_nm
                # 3) å»é™¤å¸¸è§è¯­ä¹‰åç¼€å†åŒ¹é…
                suffixes = ["full", "only", "part", "data", "df", "dataset", "stats", "main"]
                base = out_nm
                for _ in range(2):
                    if '_' in base:
                        tail = base.rsplit('_', 1)[-1]
                        if tail in suffixes:
                            base = base.rsplit('_', 1)[0]
                        else:
                            break
                if base != out_nm and base in raw_result:
                    return base
                # 4) å›é€€ï¼šæŒ‰æœªä½¿ç”¨é¡ºåºæŒ‘é€‰
                for k in raw_keys:
                    if k not in used_keys:
                        return k
                return None

            for spec in cfg.outputs:
                out_name = spec.name
                # ä¼˜å…ˆçº§ï¼šspec.source_key > output_key_map > æ¨æ–­
                if spec.source_key and spec.source_key in raw_result:
                    source_key = spec.source_key
                elif False:  # ä¿ç•™å ä½ï¼Œoutput_key_map å·²åˆ é™¤
                    source_key = None
                else:
                    source_key = infer_key(out_name)

                val = raw_result.get(source_key) if source_key else None
                if val is None and (not source_key or source_key not in raw_result):
                    self.logger.warning(
                        f"ğŸ”‘ è‡ªåŠ¨æ˜ å°„æœªæ‰¾åˆ°åˆé€‚é”® -> è¾“å‡º {out_name} (æ¨æ–­æº: {source_key}) ä½¿ç”¨ None; åŸå§‹å¯ç”¨é”®: {raw_keys}")
                else:
                    used_keys.add(source_key)

                cap.produced[out_name] = val
                # ä»… dataset è¾“å‡ºè¿›å…¥ tuple_resultï¼Œparameter ä¸å‚ä¸ Kedro node output åºåˆ—
                if spec.kind == 'dataset':
                    mapped_vals.append(val)
            cap.tuple_result = tuple(mapped_vals) if mapped_vals else tuple()

        # åºåˆ—ç»“æœï¼ˆlist/tupleï¼‰æ˜ å°„åˆ°å£°æ˜çš„å¤šä¸ªè¾“å‡º
        elif isinstance(raw_result, (list, tuple)) and len(output_names) > 1:
            mapped_vals: List[Any] = []
            for idx, out_name in enumerate(output_names):
                if idx < len(raw_result):
                    val = raw_result[idx]
                else:
                    self.logger.warning(f"ğŸ”¢ å¤šè¾“å‡ºä½ç½® {idx} ä¸å­˜åœ¨ï¼Œä½¿ç”¨ None")
                    val = None
                cap.produced[out_name] = val
                # ä»ç„¶åªå°† dataset è¾“å‡ºç»„è£…åˆ° tuple_result
                spec_kind = next((s.kind for s in cfg.outputs if s.name == out_name), 'dataset')
                if spec_kind == 'dataset':
                    mapped_vals.append(val)
            cap.tuple_result = tuple(mapped_vals)

        else:
            # å•è¾“å‡ºï¼šæ•´ä¸ª raw_result èµ‹ç»™ç¬¬ä¸€ä¸ªï¼ˆå¯èƒ½æ˜¯ dataset æˆ– parameterï¼‰
            first = output_names[0]
            cap.produced[first] = raw_result
            if any(s.kind == 'dataset' for s in cfg.outputs):
                # ä»…å½“å­˜åœ¨ dataset è¾“å‡ºæ—¶æ‰æ„é€  tuple_result ä¾› Kedro ä½¿ç”¨
                ds_first = next((s.name for s in cfg.outputs if s.kind == 'dataset'), first)
                # è‹¥ first ä¸æ˜¯ datasetï¼Œä¸”å­˜åœ¨ dataset è¾“å‡ºï¼Œtuple_result å–è¯¥ dataset çš„å€¼ï¼ˆæ­¤æ—¶æ‰¾ä¸åˆ°åˆ™ Noneï¼‰
                val_for_tuple = cap.produced.get(ds_first) if ds_first == first else None
                cap.tuple_result = (val_for_tuple,) if ds_first else tuple()
            else:
                cap.tuple_result = tuple()  # å…¨æ˜¯ parameter æ—¶è¿”å›ç©º tuple ç»™ Kedro
        return cap

    # ----------------------------------------------------------------------------
    # æ‘˜è¦å·¥å…·
    # ----------------------------------------------------------------------------
    def summarize(self, name: str, obj: Any) -> Dict[str, Any]:
        summary = {'name': name, 'type': type(obj).__name__}
        try:
            if isinstance(obj, pd.DataFrame):
                summary.update({'rows': int(len(obj)), 'cols': int(len(obj.columns)), 'columns_sample': obj.columns[:10].tolist()})
            elif isinstance(obj, (list, tuple)):
                summary['length'] = len(obj)
            elif isinstance(obj, dict):
                summary['keys'] = list(obj.keys())[:15]
        except Exception as e:
            summary['error'] = f'summary_failed: {e}'
        return summary

    # primary_policy åŠŸèƒ½å·²åˆ é™¤ï¼šä¸å†éœ€è¦è¿è¡ŒæœŸè¾“å…¥è£å‰ª

    # --- Prefect external input ingestion ---
    def ingest_prefect_inputs(self, step: str, declared_inputs: List[str], task_inputs: Dict[str, Any], logger):
        if not declared_inputs:
            return
        for name in declared_inputs:
            if name in task_inputs:
                self.global_catalog[name] = task_inputs[name]
            else:
                if name not in self.global_catalog:
                    logger.warning(f"[IOManager][Prefect] Step '{step}' ç¼ºå¤±å£°æ˜è¾“å…¥ä¸”å…¨å±€ä¸å­˜åœ¨: {name}")
                else:
                    logger.info(f"[IOManager][Prefect] Step '{step}' ä½¿ç”¨å…¨å±€ç¼“å­˜è¾“å…¥: {name}")

    # ----------------------------------------------------------------------------
    # Helper
    # ----------------------------------------------------------------------------
    def _unwrap(self, func: Callable) -> Callable:
        original = func
        depth = 0
        while hasattr(original, '__wrapped__') and depth < 10:
            original = getattr(original, '__wrapped__')
            depth += 1
        return original

    # ---------------- Schema & è¾“å‡º Key æ ¡éªŒæå– ----------------
    def validate_input_schema(self, schema_meta: Dict[str, Any], call_params: Dict[str, Any], strict: bool):
        if not schema_meta or not schema_meta.get('required_columns'):
            return
        required_cols = schema_meta.get('required_columns') or []
        candidate_dfs: List[pd.DataFrame] = []
        for v in call_params.values():
            if isinstance(v, pd.DataFrame):
                candidate_dfs.append(v)
            elif isinstance(v, (list, tuple)) and v and all(isinstance(x, pd.DataFrame) for x in v):
                candidate_dfs.extend(v)
        uniq = []
        seen = set()
        for df in candidate_dfs:
            oid = id(df)
            if oid not in seen:
                uniq.append(df)
                seen.add(oid)
        if not uniq and required_cols and strict:
            raise ValueError("Schemaä¸¥æ ¼æ¨¡å¼: æœªæ‰¾åˆ°å¯æ ¡éªŒDataFrame")
        missing_any = []
        for df in uniq[:2]:
            miss = [c for c in required_cols if c not in df.columns]
            if miss:
                missing_any.extend(miss)
        if missing_any:
            msg = f"Schemaç¼ºåˆ—: {sorted(set(missing_any))} (æœŸæœ›:{required_cols})"
            if strict:
                raise ValueError(msg)
            else:
                self.logger.warning(msg)

    def validate_output_schema(self, schema_meta: Dict[str, Any], result: Any, strict: bool, method_name: str):
        if not schema_meta or not schema_meta.get('output_keys') or not isinstance(result, dict):
            return
        expected_keys = schema_meta.get('output_keys') or []
        missing = [k for k in expected_keys if k not in result]
        if missing:
            msg = f"Schemaè¾“å‡ºç¼ºå¤±({method_name}): {missing} (æœŸæœ›:{expected_keys})"
            if strict:
                raise ValueError(msg)
            else:
                self.logger.warning(msg)


# æœªæ¥æ‰©å±•ç‚¹å ä½ï¼šå‘½åç­–ç•¥ / å˜é‡æ¸²æŸ“ / åˆ†åŒºæ•°æ®é›† / ç‰ˆæœ¬æ§åˆ¶ / ç¼“å­˜ç­–ç•¥æ³¨å†Œè¡¨
## (å·²ç§»é™¤å ä½) IONamingStrategy åˆ é™¤ï¼šç²¾ç®€ä»£ç åŸºçº¿
