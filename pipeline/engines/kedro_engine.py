import logging
import inspect
from typing import Dict, Any, List, Optional, Callable
import time
import pandas as pd
from pathlib import Path
import json
import pickle
import hashlib
from kedro.pipeline import Pipeline, node
from kedro.io import DataCatalog, MemoryDataset
from kedro.runner import SequentialRunner

from pipeline.io.io_manager import IOManager
from pipeline.core.services.hook_manager import HookManager
import traceback

class KedroEngine:
    def __init__(self, execute_manager):
        self.execute_manager = execute_manager
        self.logger = execute_manager.logger
        self.pipelines = {}
        self.data_catalog = None
        self.global_catalog = {}
        # Phase3: lineage & metrics containers
        self.node_metrics: Dict[str, Dict[str, Any]] = {}
        self.lineage: Dict[str, Dict[str, Any]] = {  # step -> {inputs: [], outputs: []}
            # filled dynamically
        }
        self.dataset_producers: Dict[str, str] = {}  # dataset -> step
    # primary è¾“å‡ºè£å‰ªç›¸å…³çŠ¶æ€å·²ç§»é™¤
        # Fingerprint / caching (Phase3 enhancement)
        self.dataset_fingerprints: Dict[str, str] = {}  # dataset -> fingerprint string
        self.node_signatures: Dict[str, str] = {}  # step -> last execution signature
        # Persistent cache control
        self.enable_persist = True
        try:
            opts = (execute_manager.ctx.config or {}).get('pipeline', {}).get('__options__', {}) or {}
            cache_opts = opts.get('cache', {}) if isinstance(opts.get('cache'), dict) else {}
            # é»˜è®¤å¼€å¯ï¼Œæ˜¾å¼ persist=False å¯å…³é—­
            self.enable_persist = cache_opts.get('persist', True)
        except Exception:
            pass
        self.cache_base = Path('.pipeline/cache')
        self.cache_datasets_dir = self.cache_base / 'datasets'
        if self.enable_persist:
            try:
                self._load_persistent_cache()
            except Exception as e:
                self.logger.warning(f"âš ï¸ æŒä¹…åŒ–ç¼“å­˜åŠ è½½å¤±è´¥(å¿½ç•¥ç»§ç»­): {e}")
        self.logger.info("Kedroå¼•æ“åˆå§‹åŒ–æˆåŠŸ")
        self._initialize_global_catalog()

    def _initialize_global_catalog(self):
        try:
            self.data_catalog = DataCatalog()
            self.global_catalog = {}
            self.logger.info("Kedroæ•°æ®ç›®å½•åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            self.logger.error(f"æ•°æ®ç›®å½•åˆå§‹åŒ–å¤±è´¥: {e}")

    def build_all_pipelines(self, config: Dict[str, Any]):
        kedro_pipelines = config.get("pipeline", {}).get("kedro_pipelines", {})
        for pipeline_name, pipeline_config in kedro_pipelines.items():
            try:
                self.create_pipeline(pipeline_name, pipeline_config)
            except Exception as e:
                self.logger.error(f"ç®¡é“æ„å»ºå¤±è´¥ {pipeline_name}: {e}")

    def create_pipeline(self, pipeline_name: str, config: Dict[str, Any]) -> Pipeline:
        nodes = []
        for node_config in config.get("nodes", []):
            try:
                kedro_node = self._create_kedro_node(node_config)
                if kedro_node:
                    nodes.append(kedro_node)
            except Exception as e:
                self.logger.error(f"èŠ‚ç‚¹åˆ›å»ºå¤±è´¥: {e}")
                continue
        pipeline = Pipeline(nodes, tags={pipeline_name})
        self.pipelines[pipeline_name] = pipeline
        self.logger.info(f"Kedroç®¡é“åˆ›å»ºæˆåŠŸ: {pipeline_name}")
        return pipeline

    def _create_kedro_node(self, node_config: Dict[str, Any]):
        # æ™ºèƒ½åŒ–é…ç½®æ ¼å¼ï¼šcomponent + engine + method
        component = node_config.get("component")
        engine = node_config.get("engine")
        method = node_config.get("method")  # æ”¯æŒå­—ç¬¦ä¸²æˆ–æ•°ç»„
        method_handles = node_config.get("handles")  # Mode5: å¤šæ–¹æ³•å¥æŸ„åˆ—è¡¨

        if not component or not engine:
            self.logger.error(f"èŠ‚ç‚¹é…ç½®å¿…é¡»åŒ…å« component, engine: {node_config.get('name')}")
            return None

        if not method:
            self.logger.error(f"èŠ‚ç‚¹é…ç½®å¿…é¡»åŒ…å« method: {node_config.get('name')}")
            return None
        # ---------- I/O manager æ„å»º ----------
        step_name = node_config.get('name') or node_config.get('id') or (node_config.get('outputs') or ['unknown_step'])[0]
        io_manager = IOManager(self.global_catalog, self.logger, strict_pipeline=bool(self.execute_manager.ctx.config.get('pipeline', {}).get('__strict_schema__')) if self.execute_manager.ctx.config else False)
        io_cfg = io_manager.build_config(node_config)

        def execute_node(*args, **kwargs):
            upstream_map = {}
            start_ts = time.perf_counter()
            try:
                # æ–¹æ³•é“¾åˆ—è¡¨ï¼ˆåœ¨ä»»ä½•ä¾èµ–å…¶å†…å®¹çš„é€»è¾‘ä¹‹å‰å®šä¹‰ï¼‰
                method_list = method if isinstance(method, list) else [method]

                base_params = {**node_config.get("parameters", {}), **kwargs}

                # --- åŸºäº KedroEngine å…¨å±€ catalog çš„å¼•ç”¨è§£æ ---
                def _resolve_refs_via_catalog(obj):
                    em = self.execute_manager
                    # ä½¿ç”¨ config_service çš„ REF_PATTERN
                    pattern = getattr(em._config_service, 'REF_PATTERN', None)
                    def walk(v):
                        if isinstance(v, dict) and '__ref__' in v:
                            ref = v['__ref__']
                            h = v.get('hash')
                            try:
                                m = pattern.match(ref) if pattern else None
                            except Exception:
                                m = None
                            if m:
                                step_id = m.group('step')
                                out_id = m.group('param')
                                ds_name = f"{step_id}__{out_id}".replace('-', '_')
                                # å…ˆä» global_catalog å–ï¼Œå…¶æ¬¡å°è¯• data_catalog.load
                                if ds_name in self.global_catalog:
                                    val = self.global_catalog[ds_name]
                                else:
                                    loaded = False
                                    val = None
                                    try:
                                        if self.data_catalog and ds_name in getattr(self.data_catalog, '_data_sets', {}):
                                            val = self.data_catalog.load(ds_name)
                                            self.global_catalog[ds_name] = val
                                            loaded = True
                                    except Exception:
                                        pass
                                    if not loaded and val is None:
                                        raise ValueError(f"catalog ä¸­æœªæ‰¾åˆ°æ•°æ®é›†: {ds_name} (ref={ref})")
                                # æ³¨å†Œåˆ° context å¼•ç”¨è¡¨ï¼Œä¿è¯åç»­é€šç”¨è§£æå¯å‘½ä¸­
                                try:
                                    rhash = em._config_service._hash_reference(ref)
                                    em.ctx.reference_to_hash.setdefault(ref, rhash)
                                    em.ctx.reference_values[ref] = val
                                    em.ctx.global_registry[rhash] = val
                                except Exception:
                                    pass
                                return val
                        if isinstance(v, list):
                            return [walk(x) for x in v]
                        if isinstance(v, dict):
                            return {k: walk(val) for k, val in v.items() if k != '__ref__'}
                        return v
                    return {k: walk(val) for k, val in obj.items()}

                try:
                    base_params = _resolve_refs_via_catalog(base_params)
                except Exception as e:
                    self.logger.error(f"å‚æ•°å¼•ç”¨è§£æå¤±è´¥(step={step_name}, catalogé˜¶æ®µ): {e}")
                    raise

                # ç»Ÿä¸€é€šè¿‡ runtime_param_service è§£æåŠ¨æ€å‚æ•°å¼•ç”¨
                try:
                    base_params = self.execute_manager.resolve_runtime_params_for_engine(base_params)
                except Exception as e:
                    self.logger.error(f"å‚æ•°å¼•ç”¨è§£æå¤±è´¥(step={step_name}, runtimeé˜¶æ®µ): {e}")
                    raise

                # å·²ç§»é™¤è‡ªåŠ¨è¾“å…¥èšåˆé€»è¾‘ï¼Œç›´æ¥ä½¿ç”¨ base_params
                # è§£æè¾“å…¥ï¼ˆä¸å†æ‰§è¡Œ primary_policy è£å‰ªï¼‰
                resolved_inputs = io_manager.resolve_inputs(io_cfg, args)
                upstream_map = dict(resolved_inputs.mapping)
                applied_inputs = getattr(resolved_inputs, '_applied_inputs', list(upstream_map.keys()))
                trimmed_inputs = getattr(resolved_inputs, '_trimmed_inputs', [])
                # é¢„å…ˆæ”¶é›†è®¡åˆ’è¾“å‡º (dataset)
                planned_outputs = [o.name for o in io_cfg.outputs if o.kind == 'dataset']
                # è®¡ç®—å½“å‰èŠ‚ç‚¹ç­¾å (æ–¹æ³•é“¾ + å‚æ•° + ä¸Šæ¸¸æŒ‡çº¹)
                upstream_fps = []
                for in_name, in_val in upstream_map.items():
                    fp = self._fingerprint_object(in_val)
                    upstream_fps.append(f"{in_name}:{fp}")
                param_items = sorted(base_params.items())
                # ä½¿ç”¨ MethodHandle.predict_signature() é¢„æµ‹æ¯ä¸ªæ–¹æ³•çš„å®ç°æŒ‡çº¹ (engine:version:priority)
                method_meta_parts = []
                try:
                    # method_handles åœ¨ _create_kedro_node æ„å»ºæ—¶å·²å†™å…¥ node_config
                    handle_map = {}
                    if method_handles:
                        for h in method_handles:
                            handle_map[getattr(h, 'method', None)] = h
                    for m in method_list:
                        h = handle_map.get(m)
                        if h is not None:
                            method_meta_parts.append(h.predict_signature(self.execute_manager.orchestrator))
                        else:
                            method_meta_parts.append(f"{m}@unknown:unknown:0")
                    method_meta_str = ';'.join(method_meta_parts)
                except Exception:
                    method_meta_str = ';'.join(f"{m}@unknown:unknown:0" for m in method_list)
                signature_components = ["|".join(method_list), method_meta_str, str(param_items), "|".join(sorted(upstream_fps))]
                node_signature = "#".join(signature_components)

                # --- ç¼“å­˜åˆ¤å®šè°ƒè¯•ä¿¡æ¯ (æ”¹ä¸º DEBUG çº§åˆ«ï¼Œé¿å…ç»ˆç«¯å™ªéŸ³) ---
                self.logger.debug(
                    "[CACHE CHECK] step=%s outputs=%s loaded=%s last_sig=%s new_sig=%s",
                    step_name,
                    planned_outputs,
                    {o: o in self.global_catalog for o in planned_outputs},
                    self.node_signatures.get(step_name),
                    node_signature,
                )

                # ç¼“å­˜å‘½ä¸­ï¼šè¾“å‡ºå­˜åœ¨ ä¸” ç­¾åç›¸åŒ -> è·³è¿‡æ‰§è¡Œ
                last_sig = self.node_signatures.get(step_name)
                # TTL å¤±æ•ˆåˆ¤å®š: å…è®¸åœ¨ step é…ç½®ä¸­åŠ å…¥ cache_ttl(seconds)
                ttl_expired = False
                try:
                    step_cfg_search = None
                    for s in (self.execute_manager.ctx.config.get('pipeline', {}).get('steps') or []):
                        if isinstance(s, dict) and s.get('name') == step_name:
                            step_cfg_search = s
                            break
                    if step_cfg_search and 'cache_ttl' in step_cfg_search:
                        ttl = step_cfg_search.get('cache_ttl')
                        if isinstance(ttl, (int, float)) and ttl > 0:
                            # ç®€å•å®ç°: datasets_index.json ä¸­æ— å•ç‹¬æ—¶é—´æˆ³, ä½¿ç”¨ç­¾åæ–‡ä»¶ä¿®æ”¹æ—¶é—´
                            sig_file = self.cache_base / 'node_signatures.json'
                            if sig_file.exists():
                                import time as _t, os
                                age = _t.time() - os.path.getmtime(sig_file)
                                if age > ttl:
                                    ttl_expired = True
                except Exception:
                    pass
                if planned_outputs and all(o in self.global_catalog for o in planned_outputs) and last_sig == node_signature and not ttl_expired:
                    self.logger.info(f"ğŸ§© Cache hit: {step_name} (signature matched) -> skip execution")
                    duration = time.perf_counter() - start_ts
                    # è¡¥å†™ primary æ ‡è®°ï¼ˆç¼“å­˜å‘½ä¸­ä¹Ÿéœ€è¦ï¼‰
                    primary_out = node_config.get('primary_output') or (planned_outputs[0] if planned_outputs else None)
                    for ds in planned_outputs:
                        self.dataset_producers.setdefault(ds, step_name)
                    self.node_metrics[step_name] = {
                        'duration_sec': duration,
                        'outputs': [io_manager.summarize(o, self.global_catalog[o]) for o in planned_outputs],
                        'cached': True,
                        'signature': node_signature
                    }
                    self.lineage[step_name] = {
                        'inputs': list(upstream_map.keys()),
                        'applied_inputs': applied_inputs,
                        'trimmed_inputs': trimmed_inputs,
                        'applied_input_count': len(applied_inputs),
                        'trimmed_input_count': len(trimmed_inputs),
                        'outputs': planned_outputs,
                        'primary_output': node_config.get('primary_output') or (planned_outputs[0] if planned_outputs else None),
                        'cached': True,
                        'signature': node_signature,
                        # primary_policy å·²ç§»é™¤
                        'duration_sec': duration
                    }
                    for ds in planned_outputs:
                        self.dataset_producers.setdefault(ds, step_name)
                    try:
                        HookManager.get().emit('on_cache_hit', step_name, self.node_metrics[step_name])
                    except Exception:
                        pass
                    return tuple(self.global_catalog[o] for o in planned_outputs) if len(planned_outputs) > 1 else (self.global_catalog[planned_outputs[0]],)
                # ç”± IOManager å†³å®šç»‘å®šç­–ç•¥

                def _unwrap(func: Callable) -> Callable:
                    original = func
                    depth = 0
                    while hasattr(original, '__wrapped__') and depth < 10:
                        original = getattr(original, '__wrapped__')
                        depth += 1
                    return original

                def build_call_params(reg_callable: Callable, prev_result, user_params) -> Dict[str, Any]:
                    return io_manager.bind_call_params(reg_callable, user_params, resolved_inputs, previous_result=prev_result)

                # method_list å·²åœ¨ä¸Šæ–¹å®šä¹‰
                # å¦‚æœå­˜åœ¨æ—§ç­¾åä¸”è¾“å‡ºå­˜åœ¨ä½†ç­¾åä¸åŒï¼Œè¾“å‡º diff è¯´æ˜
                if planned_outputs and all(o in self.global_catalog for o in planned_outputs) and last_sig and last_sig != node_signature and not ttl_expired:
                    self._log_cache_diff(step_name, last_sig, node_signature, upstream_fps)
                if ttl_expired:
                    self.logger.info(f"â° Cache TTL expired for step={step_name}, å¼ºåˆ¶é‡ç®—")
                self.logger.info(f"ğŸ”„ æ‰§è¡Œæ–¹æ³•åºåˆ—: {method_list}")
                try:
                    HookManager.get().emit('before_node', step_name, {
                        'planned_outputs': planned_outputs,
                        'inputs': list(upstream_map.keys()),
                        'signature': node_signature
                    })
                except Exception:
                    pass
                result = None
                for idx, method_name in enumerate(method_list):
                    self.logger.info(f"  âš¡ æ‰§è¡Œæ–¹æ³• {idx+1}/{len(method_list)}: {method_name}")
                    # è‹¥å­˜åœ¨å¥æŸ„åˆ—è¡¨ä¸” engine å ä½ç¬¦ï¼ŒæŒ‰æ–¹æ³•å•ç‹¬è§£æ
                    effective_engine = engine
                    if method_handles and engine in ('<auto:deferred>', '<handle:auto>'):
                        # åŒ¹é…å½“å‰æ–¹æ³•çš„ handleï¼ˆä¸€ä¸ªæ–¹æ³•ä¸€ä¸ªï¼‰
                        mh = None
                        for h in method_handles:
                            if getattr(h, 'method', None) == method_name:
                                mh = h
                                break
                        if mh is None:
                            raise ValueError(f"æœªæ‰¾åˆ°æ–¹æ³•å¥æŸ„: {step_name}.{method_name}")
                        try:
                            # é¢„æµ‹å€¼ï¼ˆç”¨äºä¸€è‡´æ€§æ ¡éªŒï¼‰
                            predicted = None
                            try:
                                predicted_sig = mh.predict_signature(self.execute_manager.orchestrator)
                                # æ ¼å¼ method@engine:version:priority
                                if '@' in predicted_sig and ':' in predicted_sig:
                                    mpart, rest = predicted_sig.split('@',1)
                                    eng_part = rest.split(':',1)[0]
                                    predicted = eng_part
                            except Exception:
                                predicted = None
                            resolved_engine = mh.resolve(self.execute_manager.orchestrator)
                            effective_engine = resolved_engine
                            if predicted and predicted != resolved_engine:
                                self.logger.warning(f"âš ï¸ å¥æŸ„é¢„æµ‹ä¸å®é™…è§£æä¸ä¸€è‡´: step={step_name} method={method_name} predicted={predicted} actual={resolved_engine}")
                            self.logger.info(f"ğŸ§® å¥æŸ„è§£æå¼•æ“: step={step_name} method={method_name} -> {resolved_engine}")
                        except Exception as re:
                            raise ValueError(f"MethodHandle å¼•æ“è§£æå¤±è´¥: {step_name}.{method_name} - {re}") from re
                    # é€šè¿‡æ–°ç´¢å¼•ç»“æ„è§£ææ³¨å†Œï¼š component -> method -> engine -> registration
                    idx_bucket = self.execute_manager.orchestrator.registry.index.by_component.get(component, {})
                    method_bucket = idx_bucket.get(method_name, {}) if idx_bucket else {}
                    registration = method_bucket.get(effective_engine)
                    if not registration:
                        raise ValueError(f"æœªæ³¨å†Œçš„æ–¹æ³•: {component}::{effective_engine}::{method_name}")
                    callable_obj = registration.callable
                    # æ•è·å‡½æ•°å‚æ•°åä¾›åç»­ auto ç­–ç•¥åˆ¤å®š
                    try:
                        sig = inspect.signature(io_manager._unwrap(callable_obj))
                        # ä¸´æ—¶æŠŠå‚æ•°åé›†åˆæŒ‚åœ¨ resolved_inputs ä¸Šï¼ˆåç»­ IOManager å¯åˆ©ç”¨ï¼‰
                        setattr(resolved_inputs, 'sig_param_names', list(sig.parameters.keys()))
                    except Exception:
                        setattr(resolved_inputs, 'sig_param_names', [])
                    schema_meta = getattr(callable_obj, '__schema__', None)
                    strict_pipeline = bool(self.execute_manager.ctx.config.get('pipeline', {}).get('__strict_schema__')) if self.execute_manager.ctx.config else False
                    strict_schema = bool(schema_meta.get('strict')) if isinstance(schema_meta, dict) else False
                    effective_strict = strict_pipeline or strict_schema
                    call_params = build_call_params(callable_obj, result, base_params)
                    # -------- è¾“å…¥åˆ—ä¸¥æ ¼æ ¡éªŒ (å§”æ‰˜ IOManager) --------
                    if schema_meta and schema_meta.get('required_columns'):
                        io_manager.validate_input_schema(schema_meta, call_params, effective_strict)
                    result = self.execute_manager.orchestrator.execute_with_engine(
                        component_type=component,
                        engine_type=effective_engine,
                        method_name=method_name,
                        **call_params
                    )
                    # -------- è¾“å‡ºé”®ä¸¥æ ¼æ ¡éªŒ (å§”æ‰˜ IOManager) --------
                    if schema_meta and schema_meta.get('output_keys') and isinstance(result, dict):
                        io_manager.validate_output_schema(schema_meta, result, effective_strict, method_name)
                    self.logger.info(f"  âœ… æ–¹æ³• {method_name} æ‰§è¡Œå®Œæˆ")
                self.logger.info(f"ğŸ¯ æ–¹æ³•åºåˆ—æ‰§è¡Œå®Œæˆï¼Œå…± {len(method_list)} ä¸ªæ–¹æ³•")

                outputs = [o.name for o in io_cfg.outputs if o.kind == 'dataset']
                produced_dataset_names: List[str] = []
                # æ”¶é›† parameter è¾“å‡ºåç§°
                parameter_outputs = [o.name for o in io_cfg.outputs if o.kind == 'parameter']
                if outputs:
                    captured = io_manager.capture_outputs(io_cfg, result)
                    for on, val in captured.produced.items():
                        self.global_catalog[on] = val
                        # ä»…è®°å½• dataset è¾“å‡ºï¼ˆparameter è¾“å‡ºä»ä¿ç•™åœ¨ global_catalog å¯è¢«å¼•ç”¨ï¼‰
                        spec = next((s for s in io_cfg.outputs if s.name == on), None)
                        if spec and spec.kind == 'dataset':
                            produced_dataset_names.append(on)
                    # parameter è¾“å‡ºæ‘˜è¦
                    param_summary = {pn: io_manager.summarize(pn, self.global_catalog.get(pn)) for pn in parameter_outputs if pn in self.global_catalog}
                    final = captured.tuple_result
                    duration = time.perf_counter() - start_ts
                    self.node_metrics[step_name] = {
                        'duration_sec': duration,
                        'outputs': [io_manager.summarize(on, self.global_catalog[on]) for on in produced_dataset_names],
                        'parameters': param_summary,
                        'cached': False,
                        'signature': node_signature
                    }
                    self.lineage[step_name] = {
                        'inputs': list(upstream_map.keys()),
                        'applied_inputs': applied_inputs,
                        'trimmed_inputs': trimmed_inputs,
                        'applied_input_count': len(applied_inputs),
                        'trimmed_input_count': len(trimmed_inputs),
                        'outputs': produced_dataset_names,
                        'parameters_produced': parameter_outputs,
                        'parameters_used': node_config.get('param_inputs', []),
                        'primary_output': captured.primary_output if captured.primary_output else (produced_dataset_names[0] if produced_dataset_names else None),
                        'cached': False,
                        'signature': node_signature,
                        # primary_policy å·²ç§»é™¤
                        'duration_sec': duration
                    }
                    for ds in produced_dataset_names:
                        self.dataset_producers[ds] = step_name
                        # è®°å½•è¾“å‡ºæŒ‡çº¹
                        self.dataset_fingerprints[ds] = self._fingerprint_object(self.global_catalog[ds])
                        # --- å³æ—¶å‘ ExecuteManager æ³¨å†Œå¼•ç”¨ (æ”¯æŒåç»­èŠ‚ç‚¹å‚æ•°è§£æ) ---
                        if '__' in ds:
                            try:
                                step_id, out_id = ds.split('__', 1)
                                ref = f"steps.{step_id}.outputs.parameters.{out_id}"
                                em = self.execute_manager
                                rhash = em._hash_reference(ref)
                                em.reference_to_hash.setdefault(ref, rhash)
                                em.reference_values[ref] = self.global_catalog[ds]
                                em.global_registry[rhash] = self.global_catalog[ds]
                            except Exception:
                                pass
                    # parameter è¾“å‡ºåŒæ ·æ³¨å†Œï¼ˆåç§°ä¸å« __ï¼Œä½†å¯ç›´æ¥æ„é€ å¼•ç”¨ï¼‰
                    for pn in parameter_outputs:
                        if pn in self.global_catalog:
                            try:
                                ref = f"steps.{step_name}.outputs.parameters.{pn}"
                                em = self.execute_manager
                                rhash = em._hash_reference(ref)
                                em.reference_to_hash.setdefault(ref, rhash)
                                em.reference_values[ref] = self.global_catalog[pn]
                                em.global_registry[rhash] = self.global_catalog[pn]
                            except Exception:
                                pass
                    # è®°å½•èŠ‚ç‚¹ç­¾å
                    self.node_signatures[step_name] = node_signature
                    # æŒä¹…åŒ–èŠ‚ç‚¹ä¸æ•°æ®é›†ï¼ˆå¢é‡ï¼‰
                    self._persist_node_state(produced_dataset_names)
                    try:
                        HookManager.get().emit('after_node', step_name, {
                            'duration_sec': duration,
                            'produced': produced_dataset_names,
                            'signature': node_signature,
                            'cached': False,
                        }, self.node_metrics[step_name])
                    except Exception:
                        pass
                    return final
                # æ—  outputsï¼ˆå…œåº•ï¼‰
                duration = time.perf_counter() - start_ts
                self.node_metrics[step_name] = {'duration_sec': duration, 'outputs': [], 'cached': False, 'signature': node_signature}
                self.lineage[step_name] = {
                    'inputs': list(upstream_map.keys()),
                    'applied_inputs': applied_inputs,
                    'trimmed_inputs': trimmed_inputs,
                    'applied_input_count': len(applied_inputs),
                    'trimmed_input_count': len(trimmed_inputs),
                    'outputs': [],
                    'primary_output': None,
                    'cached': False,
                    'signature': node_signature,
                    # primary_policy å·²ç§»é™¤
                    'duration_sec': duration
                }
                self.node_signatures[step_name] = node_signature
                self._persist_node_state([])
                # Kedro æœŸæœ›æ—  outputs çš„èŠ‚ç‚¹è¿”å› None/()ï¼Œé¿å… DataFrame ç­‰è¢«è¯¯åˆ¤
                try:
                    HookManager.get().emit('after_node', step_name, {
                        'duration_sec': duration,
                        'produced': [],
                        'signature': node_signature,
                        'cached': False,
                    }, self.node_metrics[step_name])
                except Exception:
                    pass
                return None
            except Exception as e:
                # å¤±è´¥èŠ‚ç‚¹ lineage & metrics è®°å½•ï¼ˆå³ä¾¿ soft-fail ä¸Šå±‚åæ‰ï¼Œä¹Ÿä¿ç•™ç—•è¿¹ï¼‰
                try:
                    duration = time.perf_counter() - start_ts if start_ts else None
                except Exception:
                    duration = None
                self.node_metrics[step_name] = {
                    'duration_sec': duration,
                    'outputs': [],
                    'parameters': {},
                    'cached': False,
                    'error': str(e)
                }
                if step_name not in self.lineage:
                    self.lineage[step_name] = {
                        'inputs': list(upstream_map.keys()),
                        'applied_inputs': applied_inputs,
                        'trimmed_inputs': trimmed_inputs,
                        'applied_input_count': len(applied_inputs),
                        'trimmed_input_count': len(trimmed_inputs),
                        'outputs': [],
                        'parameters_produced': [],
                        'parameters_used': node_config.get('param_inputs', []),
                        'primary_output': None,
                        'failed': True,
                        'error': str(e),
                        'cached': False,
                        'signature': None,
                        # primary_policy å·²ç§»é™¤
                        'duration_sec': duration
                    }
                # å¤±è´¥å¿«ç…§
                try:
                    fail_dir = Path('.pipeline') / 'failures'
                    fail_dir.mkdir(parents=True, exist_ok=True)
                    snapshot = {
                        'step': step_name,
                        'error': str(e),
                        'traceback': traceback.format_exc(limit=8),
                        'methods': method_list,
                        'parameters': base_params,
                        'inputs': list(upstream_map.keys()),
                        'signature': self.node_metrics.get(step_name, {}).get('signature'),
                    }
                    (fail_dir / f'{step_name}.json').write_text(json.dumps(snapshot, ensure_ascii=False, indent=2), encoding='utf-8')
                except Exception:
                    pass
                try:
                    HookManager.get().emit('after_node', step_name, {
                        'duration_sec': duration,
                        'produced': [],
                        'signature': self.node_metrics.get(step_name, {}).get('signature'),
                        'cached': False,
                        'error': str(e),
                        'failed': True,
                    }, self.node_metrics.get(step_name, {}))
                    HookManager.get().emit('on_failure', step_name, {
                        'error': str(e)
                    })
                except Exception:
                    pass
                self.logger.error(f"èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥: {e}")
                raise

        # ä»…æŠŠ dataset ç±»å‹è¾“å‡ºæ³¨å†Œç»™ Kedroï¼Œparameter è¾“å‡ºåªæ”¾å…¥å…¨å±€ç›®å½•ä¸å ä½è§£æ
        try:
            io_manager_tmp = IOManager(self.global_catalog, self.logger)
            io_cfg_tmp = io_manager_tmp.build_config(node_config)
            kedro_outputs = [spec.name for spec in io_cfg_tmp.outputs if spec.kind == 'dataset']
        except Exception:
            raw_outs = node_config.get("outputs", [])
            kedro_outputs = [o for o in raw_outs if isinstance(o, str)]

        # ç¡®ä¿ node_config å†…éƒ¨ outputs æ›¿æ¢ä¸ºä»… dataset åˆ—è¡¨ï¼ˆé¿å… Kedro å…ˆè¯»å–åŸå§‹ dict åˆ—è¡¨æ—¶æŠ¥é”™ï¼‰
        node_config['outputs'] = kedro_outputs

        return node(
            func=execute_node,
            inputs=node_config.get("inputs", []),
            outputs=kedro_outputs if kedro_outputs else None,
            name=node_config.get("name"),
            tags=node_config.get("tags", [])
        )

    # ----------------------------------------------------------------------------
    # Caching helpers
    # ----------------------------------------------------------------------------
    def _fingerprint_object(self, obj: Any) -> str:
        """ç¨³å®šæŒ‡çº¹ï¼šDataFrame ä½¿ç”¨ sha256(shape + åˆ—å + å‰Nè¡Œæ ·æœ¬), å…¶å®ƒå¯¹è±¡ç”¨ç±»å‹+repræˆªæ–­ sha256."""
        try:
            if isinstance(obj, pd.DataFrame):
                h = hashlib.sha256()
                h.update(str(obj.shape).encode())
                h.update("|".join(map(str, obj.columns)).encode())
                # é‡‡æ ·å‰ 30 è¡Œï¼ˆé¿å…è¶…å¤§å†…å­˜ï¼‰
                sample = obj.head(30).to_csv(index=False).encode()
                h.update(sample)
                return f"df:{h.hexdigest()}"
            if isinstance(obj, (list, tuple)):
                h = hashlib.sha256()
                h.update(str(type(obj)).encode())
                h.update(str(len(obj)).encode())
                # é‡‡æ ·å‰ 10 ä¸ªå…ƒç´ çš„ repr
                for x in list(obj)[:10]:
                    h.update(repr(type(x)).encode())
                return f"seq:{h.hexdigest()}"
            if isinstance(obj, dict):
                h = hashlib.sha256()
                h.update(str(len(obj)).encode())
                for k in sorted(list(obj.keys())[:20]):
                    h.update(str(k).encode())
                    h.update(str(type(obj[k])).encode())
                return f"dict:{h.hexdigest()}"
            # fallback
            h = hashlib.sha256()
            rep = repr(obj)
            if len(rep) > 500:
                rep = rep[:500]
            h.update(rep.encode())
            h.update(str(type(obj)).encode())
            return f"obj:{h.hexdigest()}"
        except Exception:
            return "fingerprint:error"

    def _log_cache_diff(self, step_name: str, old_sig: str, new_sig: str, upstream_fps: List[str]):
        try:
            # ç®€å•æ‹†åˆ†ç­¾åï¼šmethod_chain#params_repr#upstream_fp_join
            def split_sig(sig: str):
                parts = sig.split('#', 2)
                while len(parts) < 3:
                    parts.append('')
                return parts
            old_m, old_p, old_u = split_sig(old_sig)
            new_m, new_p, new_u = split_sig(new_sig)
            diffs = []
            if old_m != new_m:
                diffs.append('method_chain')
            if old_p != new_p:
                diffs.append('parameters')
            if old_u != new_u:
                diffs.append('upstream')
            reason = ','.join(diffs) or 'unknown'
            self.logger.info(f"â™»ï¸ Cache miss (signature changed) step={step_name} reason=[{reason}] diffs={{methods:{old_m!r}->{new_m!r}, params_changed:{old_p!=new_p}, upstream_changed:{old_u!=new_u}}}")
        except Exception:
            self.logger.debug("æ— æ³•ç”Ÿæˆç¼“å­˜å·®å¼‚è¯´æ˜")

    # ----------------------------------------------------------------------------
    # Persistent cache helpers
    # ----------------------------------------------------------------------------
    def _load_persistent_cache(self):
        if not self.enable_persist:
            return
        sig_file = self.cache_base / 'node_signatures.json'
        idx_file = self.cache_base / 'datasets_index.json'
        if sig_file.exists():
            try:
                self.node_signatures = json.loads(sig_file.read_text(encoding='utf-8'))
            except Exception as e:
                self.logger.warning(f"ç­¾åæ–‡ä»¶è¯»å–å¤±è´¥: {e}")
        if idx_file.exists():
            try:
                idx = json.loads(idx_file.read_text(encoding='utf-8'))
                loaded = 0
                for ds, meta in idx.items():
                    file_rel = meta.get('file')
                    if not file_rel:
                        continue
                    fpath = self.cache_base / file_rel
                    if not fpath.exists():
                        continue
                    try:
                        with open(fpath, 'rb') as f:
                            obj = pickle.load(f)
                        self.global_catalog[ds] = obj
                        self.dataset_fingerprints[ds] = meta.get('fingerprint', '')
                        loaded += 1
                    except Exception as e:
                        self.logger.warning(f"æ•°æ®é›† {ds} è½½å…¥å¤±è´¥: {e}")
                if loaded:
                    self.logger.info(f"ğŸ“¦ æŒä¹…åŒ–ç¼“å­˜è½½å…¥: {loaded} datasets, {len(self.node_signatures)} node signatures")
            except Exception as e:
                self.logger.warning(f"æ•°æ®é›†ç´¢å¼•è¯»å–å¤±è´¥: {e}")

    def _persist_node_state(self, produced: List[str]):
        if not self.enable_persist:
            return
        try:
            self.cache_base.mkdir(parents=True, exist_ok=True)
            self.cache_datasets_dir.mkdir(parents=True, exist_ok=True)
            # å†™èŠ‚ç‚¹ç­¾å
            sig_file = self.cache_base / 'node_signatures.json'
            sig_file.write_text(json.dumps(self.node_signatures, ensure_ascii=False, indent=2), encoding='utf-8')
            # è¯»å–æ—§ç´¢å¼•
            idx_file = self.cache_base / 'datasets_index.json'
            if idx_file.exists():
                try:
                    idx = json.loads(idx_file.read_text(encoding='utf-8'))
                except Exception:
                    idx = {}
            else:
                idx = {}
            # å†™æ–°äº§ç”Ÿçš„æ•°æ®é›†
            for ds in produced:
                obj = self.global_catalog.get(ds)
                if obj is None:
                    continue
                fp = self.dataset_fingerprints.get(ds) or self._fingerprint_object(obj)
                self.dataset_fingerprints[ds] = fp
                file_name = f"{ds}.pkl"
                safe_path = self.cache_datasets_dir / file_name
                try:
                    with open(safe_path, 'wb') as f:
                        pickle.dump(obj, f)
                except Exception as e:
                    self.logger.warning(f"æ•°æ®é›† {ds} æŒä¹…åŒ–å¤±è´¥: {e}")
                rel_path = f"datasets/{file_name}"
                idx[ds] = {
                    'fingerprint': fp,
                    'type': type(obj).__name__,
                    'file': rel_path
                }
            idx_file.write_text(json.dumps(idx, ensure_ascii=False, indent=2), encoding='utf-8')
        except Exception as e:
            self.logger.warning(f"âš ï¸ æŒä¹…åŒ–å†™å…¥å¤±è´¥(å¿½ç•¥): {e}")

    def parse_pipeline_config(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """è§£æç®¡é“é…ç½®ï¼Œä¸ºKedroæ„å»ºåšå‡†å¤‡"""
        try:
            # ç›´æ¥è¿”å›Kedroç®¡é“é…ç½®éƒ¨åˆ†
            kedro_pipelines = config.get("pipeline", {}).get("kedro_pipelines", {})

            # æ„å»ºç®¡é“é…ç½®å¯¹è±¡
            pipeline_configs = {}
            for pipeline_name, pipeline_def in kedro_pipelines.items():
                pipeline_configs[pipeline_name] = type('PipelineConfig', (), {
                    'name': pipeline_def.get('name', pipeline_name),
                    'description': pipeline_def.get('description', ''),
                    'nodes': pipeline_def.get('nodes', []),
                    'depends_on': pipeline_def.get('depends_on', [])
                })()

            self.logger.info(f"âœ… Kedroé…ç½®è§£æå®Œæˆ: {len(pipeline_configs)} ä¸ªç®¡é“")
            return pipeline_configs

        except Exception as e:
            self.logger.error(f"âŒ Kedroé…ç½®è§£æå¤±è´¥: {e}")
            raise

    def get_pipeline_execution_order(self, pipeline_configs: Dict[str, Any]) -> List[str]:
        """ç¡®å®šç®¡é“æ‰§è¡Œé¡ºåºï¼ˆæ‹“æ‰‘æ’åºï¼‰"""
        try:
            # æ„å»ºä¾èµ–å›¾
            dependencies = {}
            for pipeline_name, config in pipeline_configs.items():
                dependencies[pipeline_name] = getattr(config, 'depends_on', [])

            # æ‹“æ‰‘æ’åº
            result = []
            visited = set()
            temp_visited = set()

            def visit(pipeline_name):
                if pipeline_name in temp_visited:
                    raise ValueError(f"æ£€æµ‹åˆ°å¾ªç¯ä¾èµ–: {pipeline_name}")
                if pipeline_name in visited:
                    return

                temp_visited.add(pipeline_name)
                for dep in dependencies.get(pipeline_name, []):
                    if dep in dependencies:  # ç¡®ä¿ä¾èµ–å­˜åœ¨
                        visit(dep)
                temp_visited.remove(pipeline_name)
                visited.add(pipeline_name)
                result.append(pipeline_name)

            for pipeline_name in dependencies:
                if pipeline_name not in visited:
                    visit(pipeline_name)

            self.logger.info(f"âœ… æ‰§è¡Œé¡ºåºç¡®å®š: {' -> '.join(result)}")
            return result

        except Exception as e:
            self.logger.error(f"âŒ æ‰§è¡Œé¡ºåºç¡®å®šå¤±è´¥: {e}")
            # è¿”å›ç®€å•çš„æŒ‰åç§°æ’åº
            return list(pipeline_configs.keys())
