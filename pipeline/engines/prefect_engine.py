#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Advanced Prefect-Kedro Hybrid Engine
===================================

é«˜çº§æ··åˆå¼•æ“ï¼šPrefectç¼–æ’Kedroç®¡é“

æ ¸å¿ƒç†å¿µï¼š
- ğŸ¯ Prefect è´Ÿè´£å·¥ä½œæµç¼–æ’ã€ç›‘æ§ã€é‡è¯•
- ğŸ—ï¸ Kedro è´Ÿè´£æ•°æ®å¤„ç†é€»è¾‘ã€è¡€ç¼˜ã€æµ‹è¯•
- ğŸ”— Prefect å°† Kedro ç®¡é“è§†ä¸ºé»‘ç®± Task
- ğŸ“Š ç»Ÿä¸€çš„ç›‘æ§ã€å®¹é”™ã€å±‚çº§è€—æ—¶ç»Ÿè®¡

æ¶æ„ä¼˜åŠ¿ï¼š
- ğŸ“ˆ Prefect çš„è°ƒåº¦ + Kedro çš„æ•°æ®å·¥ç¨‹æœ€ä½³å®è·µ
- â™»ï¸ é€šè¿‡ ConcurrentTaskRunner æ”¯æŒå±‚å†…å¹¶è¡Œï¼›max_workers=1 æ¨¡æ‹Ÿé¡ºåº
- ğŸ›¡ï¸ soft_fail å¯é€‰ï¼šå•ä»»åŠ¡å¤±è´¥ä¸å½±å“æ•´ä½“ï¼ˆä¾èµ–è‡ªåŠ¨ skippedï¼‰
- ğŸ“Š layer_metrics è¾“å‡ºæ¯å±‚ä»»åŠ¡æ•°ä¸è€—æ—¶

Author: AStock Team
Version: 2.0.1 - Hybrid Architecture (soft_fail, layer_metrics)
"""

import logging
import inspect
import json
from typing import Dict, Any, List, Optional, Callable, Union
from pathlib import Path
from datetime import datetime, timedelta
from dataclasses import dataclass

# Prefect-Kedro æ··åˆå¼•æ“å¿…éœ€å¯¼å…¥
from prefect import flow, task, get_run_logger
from prefect.states import Completed, Failed
from prefect.task_runners import ConcurrentTaskRunner
from pipeline.io.io_manager import IOManager


@dataclass
class HybridTaskConfig:
    """æ··åˆä»»åŠ¡é…ç½®"""
    name: str
    kedro_pipeline: str
    description: str
    retries: int = 3
    retry_delay: int = 30
    timeout: int = 600
    depends_on: List[str] = None


class PrefectEngine:
    """
    é«˜çº§Prefect-Kedroæ··åˆå¼•æ“

    å°†Kedroç®¡é“å°è£…ä¸ºPrefectä»»åŠ¡ï¼Œå®ç°å¼ºå¤§çš„æ··åˆæ¶æ„
    """

    def __init__(self, execute_manager):
        """
        åˆå§‹åŒ–é«˜çº§Prefectå¼•æ“

        Args:
            execute_manager: ExecuteManagerå®ä¾‹
        """
        self.execute_manager = execute_manager
        self.logger = execute_manager.logger

        # å¼•æ“çŠ¶æ€
        self.kedro_engine = None
        self.current_flow = None
        self.task_registry = {}

        # åˆå§‹åŒ–
        self.logger.info("âœ… Prefect-Kedroæ··åˆå¼•æ“åˆå§‹åŒ–æˆåŠŸ")
        self._initialize_kedro_integration()

    def _initialize_kedro_integration(self):
        """åˆå§‹åŒ–Kedroé›†æˆ"""
        try:
            # ä¼˜å…ˆå°è¯•åŒ…å†…ç›¸å¯¹å¯¼å…¥ï¼›å¤±è´¥åç»å¯¹å¯¼å…¥ï¼ˆå…¼å®¹ç›´æ¥è„šæœ¬æ‰§è¡Œï¼‰
            KedroEngine = None  # type: ignore
            try:
                from .kedro_engine import KedroEngine as _KE  # type: ignore
                KedroEngine = _KE
            except Exception:
                from pipeline.engines.kedro_engine import KedroEngine as _KE  # type: ignore
                KedroEngine = _KE
            if KedroEngine is None:
                raise ImportError("KedroEngine æœªæ‰¾åˆ°")
            self.kedro_engine = KedroEngine(self.execute_manager)
            self.logger.info("ğŸ”— Prefect-Kedroé›†æˆåˆå§‹åŒ–å®Œæˆ (import fallback OK)")
        except Exception as e:
            self.logger.error(f"âŒ Kedroé›†æˆåˆå§‹åŒ–å¤±è´¥: {e}")

    def parse_hybrid_config(self, config: Dict[str, Any]) -> Dict[str, HybridTaskConfig]:
        """
        è§£ææ··åˆé…ç½®:å°†Kedroç®¡é“æ˜ å°„ä¸ºPrefectä»»åŠ¡

        Args:
            config: å®Œæ•´é…ç½®

        Returns:
            æ··åˆä»»åŠ¡é…ç½®å­—å…¸
        """
        hybrid_tasks = {}

        # è·å–Kedroç®¡é“é…ç½®
        kedro_pipelines = config.get('pipeline', {}).get('kedro_pipelines', {})

        # è·å–Prefecté…ç½®
        prefect_config = config.get('prefect_flow', {})
        task_config = prefect_config.get('task_config', {})

        for pipeline_name, pipeline_def in kedro_pipelines.items():
            # ä¸ºæ¯ä¸ªKedroç®¡é“åˆ›å»ºä¸€ä¸ªPrefectä»»åŠ¡é…ç½®
            task_name = f"kedro_pipeline_{pipeline_name}"

            hybrid_task = HybridTaskConfig(
                name=task_name,
                kedro_pipeline=pipeline_name,
                description=pipeline_def.get('description', f"Execute {pipeline_name}"),
                retries=task_config.get('max_retries', 3),
                retry_delay=task_config.get('retry_delay', 30),
                timeout=task_config.get('timeout', 600),
                depends_on=pipeline_def.get('depends_on', [])
            )

            hybrid_tasks[pipeline_name] = hybrid_task

        self.logger.info(f"ğŸ¯ è§£æäº† {len(hybrid_tasks)} ä¸ªæ··åˆä»»åŠ¡é…ç½®")
        return hybrid_tasks

    def create_kedro_pipeline_task(self, hybrid_config: HybridTaskConfig, soft_fail: bool = False) -> Callable:
        """
        ä¸ºKedroç®¡é“åˆ›å»ºPrefectä»»åŠ¡åŒ…è£…å™¨

        è¿™æ˜¯æ ¸å¿ƒåŠŸèƒ½ï¼šå°†Kedroç®¡é“å°è£…ä¸ºPrefectä»»åŠ¡

        Args:
            hybrid_config: æ··åˆä»»åŠ¡é…ç½®

        Returns:
            Prefectä»»åŠ¡å‡½æ•°
        """
        pipeline_name = hybrid_config.kedro_pipeline

        @task(
            name=hybrid_config.name,
            description=hybrid_config.description,
            retries=hybrid_config.retries,
            retry_delay_seconds=hybrid_config.retry_delay,
            timeout_seconds=hybrid_config.timeout,
            tags=["kedro-pipeline"]  # ç‹¬ç«‹ç¼–æ’ç³»ç»Ÿæ ‡ç­¾
        )
        def execute_kedro_pipeline_task(**task_inputs):
            """
            Prefectä»»åŠ¡ï¼šæ‰§è¡ŒKedroç®¡é“

            è¿™ä¸ªä»»åŠ¡å°†æ•´ä¸ªKedroç®¡é“ä½œä¸ºé»‘ç®±æ‰§è¡Œ
            """
            logger = get_run_logger()
            logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡ŒKedroç®¡é“: {pipeline_name}")

            try:
                # ç¡®ä¿Kedroå¼•æ“å’Œç®¡é“å·²å‡†å¤‡å¥½
                if pipeline_name not in self.kedro_engine.pipelines:
                    raise ValueError(f"Kedroç®¡é“æœªæ‰¾åˆ°: {pipeline_name}")

                # è·å–Kedroç®¡é“ & pipeline å®šä¹‰
                pipeline = self.kedro_engine.pipelines[pipeline_name]
                catalog = self.kedro_engine.data_catalog
                pipeline_def = self.execute_manager.ctx.config.get('pipeline', {}).get('kedro_pipelines', {}).get(pipeline_name, {})
                node_defs = pipeline_def.get('nodes', []) or []
                declared_inputs = set()
                declared_outputs = set()
                for nd in node_defs:
                    # è¿‡æ»¤ node_defs å†…éƒ¨ outputsï¼šåªä¿ç•™ dataset å­—ç¬¦ä¸²ï¼Œå†™å›ä»¥å‡å°‘ Kedro æŠ¥é”™
                    cleaned_outs = []
                    for i in nd.get('inputs', []) or []:
                        if isinstance(i, str):
                            declared_inputs.add(i)
                    for o in nd.get('outputs', []) or []:
                        if isinstance(o, str) and '__param__' not in o:
                            declared_outputs.add(o)
                            cleaned_outs.append(o)
                    nd['outputs'] = cleaned_outs

                # ä½¿ç”¨ IOManager ç»Ÿä¸€å¤„ç†ä¸Šæ¸¸è¾“å…¥æ³¨å…¥
                io_manager = IOManager(self.kedro_engine.global_catalog, self.logger)
                io_manager.ingest_prefect_inputs(pipeline_name, list(declared_inputs), task_inputs, self.logger)
                # åŒæ­¥åˆ° Kedro catalogï¼ˆä»…å¯¼å…¥å£°æ˜éœ€è¦çš„ï¼‰
                for in_name in declared_inputs:
                    if in_name in task_inputs:
                        try:
                            if in_name not in catalog._datasets:  # type: ignore[attr-defined]
                                from kedro.io import MemoryDataset
                                catalog.add(in_name, MemoryDataset())  # type: ignore[attr-defined]
                            catalog.save(in_name, task_inputs[in_name])
                            logger.info(f"ğŸ“¥ æ³¨å…¥è¾“å…¥: {in_name}")
                        except Exception as _e:
                            logger.warning(f"ä¿å­˜è¾“å…¥ {in_name} å¤±è´¥: {_e}")

                # è‹¥å£°æ˜äº†è¾“å…¥ä½†æœªé€šè¿‡ task_inputs ä¼ é€’ï¼Œå°è¯•ä»å…¨å±€ç¼“å­˜è¡¥å…¨
                for miss in declared_inputs:
                    if miss in task_inputs:  # å·²ç”±ä¸Šæ¸¸ä¼ å…¥
                        continue
                    if miss in self.kedro_engine.global_catalog:
                        obj = self.kedro_engine.global_catalog[miss]
                        try:
                            if miss not in catalog._datasets:  # type: ignore[attr-defined]
                                from kedro.io import MemoryDataset
                                catalog.add(miss, MemoryDataset())  # type: ignore[attr-defined]
                            catalog.save(miss, obj)
                            logger.info(f"ğŸ“¥ å›å¡«ç¼“å­˜è¾“å…¥: {miss}")
                        except Exception as _e:
                            logger.warning(f"å›å¡«è¾“å…¥ {miss} å¤±è´¥: {_e}")

                # æ‰§è¡ŒKedroç®¡é“
                from kedro.runner import SequentialRunner
                runner = SequentialRunner()
                runner.run(pipeline, catalog)

                logger.info(f"âœ… Kedroç®¡é“æ‰§è¡ŒæˆåŠŸ: {pipeline_name}")

                # æ”¶é›†å£°æ˜è¾“å‡ºï¼ˆä¼˜å…ˆä½¿ç”¨ KedroEngine.global_catalog ä¸­çš„æŒä¹…åŒ–ï¼‰
                outputs = {}
                for out in declared_outputs:
                    if out in self.kedro_engine.global_catalog:
                        outputs[out] = self.kedro_engine.global_catalog[out]
                    else:
                        try:
                            if out in catalog._datasets:  # type: ignore[attr-defined]
                                outputs[out] = catalog.load(out)
                        except Exception as e:
                            logger.warning(f"æ— æ³•åŠ è½½å£°æ˜è¾“å‡º {out}: {e}")

                return {
                    'status': 'completed',
                    'pipeline_name': pipeline_name,
                    'outputs': outputs
                }

            except Exception as e:
                # soft_fail æƒ…å†µé™çº§ä¸º WARNINGï¼Œé¿å…è¯¯åˆ¤æ•´ä½“å¤±è´¥
                if soft_fail:
                    logger.warning(f"âš ï¸ Prefectä»»åŠ¡å¤±è´¥(soft_failå·²å¸æ”¶) {pipeline_name}: {e}")
                    return {
                        'status': 'failed',  # ä¿ç•™æ ‡è®°ä»¥ç»Ÿè®¡
                        'pipeline_name': pipeline_name,
                        'error': str(e),
                        'soft_fail': True
                    }
                else:
                    logger.error(f"âŒ Prefectä»»åŠ¡æ‰§è¡Œå¤±è´¥ {pipeline_name}: {e}")
                    raise

        return execute_kedro_pipeline_task

    def build_hybrid_flow(self, config: Dict[str, Any]) -> Callable:
        """
        æ„å»ºæ··åˆå·¥ä½œæµï¼šPrefect FlowåŒ…å«å¤šä¸ªKedro Pipelineä»»åŠ¡

        Args:
            config: å®Œæ•´é…ç½®

        Returns:
            Prefect Flowå‡½æ•°
        """
        # ç¡®ä¿Kedroç®¡é“å·²æ„å»º
        self.kedro_engine.build_all_pipelines(config)

        # åˆ¤æ–­æ˜¯å¦å¯ç”¨èŠ‚ç‚¹çº§ç²’åº¦
        orchestration = config.get('pipeline', {}).get('orchestration', {}) or {}
        granularity = orchestration.get('granularity', 'pipeline').lower()
        try:
            self.logger.info(f"ğŸ” Prefect granularity æ£€æµ‹: raw_orchestration_keys={list(orchestration.keys())} granularity={granularity}")
        except Exception:
            pass
        if granularity == 'node':
            return self._build_node_level_flow(config, orchestration)

        # è§£ææ··åˆé…ç½®
        hybrid_tasks = self.parse_hybrid_config(config)

        # è½¯å¤±è´¥é…ç½®ï¼ˆæ”¯æŒä¸¤ç§è·¯å¾„ï¼špipeline.orchestration.soft_fail ä¸ pipeline.options.orchestration.soft_failï¼‰
        options_block = config.get('pipeline', {}).get('options', {}) or {}
        opt_orch = options_block.get('orchestration', {}) if isinstance(options_block.get('orchestration', {}), dict) else {}
        soft_fail = bool(orchestration.get('soft_fail', False) or opt_orch.get('soft_fail', False))
        self.logger.info(f"ğŸ›¡ï¸ Soft-fail è§£æç»“æœ: {soft_fail} (direct={orchestration.get('soft_fail')}, options={opt_orch.get('soft_fail')})")

        # åˆ›å»ºä»»åŠ¡å‡½æ•°
        task_functions = {}
        for pipeline_name, hybrid_config in hybrid_tasks.items():
            task_func = self.create_kedro_pipeline_task(hybrid_config, soft_fail=soft_fail)
            task_functions[pipeline_name] = task_func
            self.task_registry[pipeline_name] = task_func

        # è·å–Flowé…ç½®
        flow_config = config.get('prefect_flow', {}).get('flow_config', {})

        # å¹¶è¡Œæ‰§è¡Œé…ç½®ï¼špipeline.orchestration.task_runner = 'concurrent' | 'sequential'
        orch_cfg = config.get('pipeline', {}).get('orchestration', {})
        task_runner_type = orch_cfg.get('task_runner', 'sequential').lower()
        max_workers = orch_cfg.get('max_workers', 4)
        if task_runner_type == 'concurrent':
            runner = ConcurrentTaskRunner(max_workers=max_workers)
            self.logger.info(f"âš™ï¸ ä½¿ç”¨å¹¶è¡Œä»»åŠ¡è¿è¡Œå™¨ ConcurrentTaskRunner(max_workers={max_workers})")
        else:
            # Prefect 3.x å·²æ—  SequentialTaskRunnerï¼Œä½¿ç”¨å¹¶å‘è¿è¡Œå™¨é™åˆ¶ä¸º1ä¸ªworkeræ¨¡æ‹Ÿé¡ºåº
            runner = ConcurrentTaskRunner(max_workers=1)
            self.logger.info("âš™ï¸ ä½¿ç”¨é¡ºåºæ¨¡æ‹Ÿè¿è¡Œå™¨ (ConcurrentTaskRunner(max_workers=1))")

        # åˆ›å»ºPrefectæµç¨‹ï¼ˆåŠ¨æ€é€‰æ‹©task_runnerï¼‰
        @flow(
            name=config.get('pipeline', {}).get('name', 'AStockæ··åˆå·¥ä½œæµ'),
            description=config.get('pipeline', {}).get('description', 'Prefectç¼–æ’Kedroç®¡é“'),
            log_prints=True,
            retries=orchestration.get('retry_count', 3),
            retry_delay_seconds=orchestration.get('retry_delay', 30),
            timeout_seconds=orchestration.get('timeout', 1800),
            task_runner=runner
        )
        def hybrid_workflow(**flow_inputs):
            """
            æ··åˆå·¥ä½œæµä¸»å‡½æ•°

            Prefectç®¡ç†æ•´ä½“ç¼–æ’ï¼ŒKedroå¤„ç†æ•°æ®é€»è¾‘
            """
            logger = get_run_logger()
            logger.info("ğŸ¯ å¯åŠ¨Prefect-Kedroæ··åˆå·¥ä½œæµ")

            # ç¡®å®šæ‰§è¡Œé¡ºåº
            pipeline_configs = self.kedro_engine.parse_pipeline_config(config)
            execution_order = self.kedro_engine.get_pipeline_execution_order(pipeline_configs)

            # ä»»åŠ¡ç»“æœå­˜å‚¨
            task_results = {}
            pipeline_outputs = {}

            # ä¾èµ–å±‚çº§å¹¶è¡Œæ‰§è¡Œï¼šåŒå±‚ï¼ˆæ— ç›¸äº’ä¾èµ–ï¼‰ä»»åŠ¡å¯å¹¶è¡Œ
            # æ„å»ºä¾èµ–æ˜ å°„
            dependencies = {p: hybrid_tasks[p].depends_on or [] for p in hybrid_tasks}
            remaining = set(execution_order)
            scheduled = {}
            completed = set()

            def ready(pname):
                return all(dep in completed for dep in dependencies.get(pname, []))

            layer_idx = 0
            layer_metrics = []  # è®°å½•æ¯å±‚è€—æ—¶ä¸ä»»åŠ¡æ•°é‡
            while remaining:
                # å½“å‰å±‚ï¼šæ‰€æœ‰ä¾èµ–å·²å®Œæˆä¸”æœªè°ƒåº¦
                current_layer = [p for p in list(remaining) if ready(p)]
                if not current_layer:
                    raise ValueError(f"æ— æ³•è§£æä¾èµ–ï¼ˆå¯èƒ½å¾ªç¯ï¼‰: å‰©ä½™ {remaining}")
                layer_idx += 1
                logger.info(f"ğŸ§© å¹¶è¡Œå±‚ {layer_idx}: {current_layer}")

                import time as _time
                layer_start = _time.time()

                # è°ƒåº¦æœ¬å±‚æ‰€æœ‰ä»»åŠ¡
                layer_futures = {}
                for pipeline_name in current_layer:
                    if pipeline_name not in task_functions:
                        logger.warning(f"è·³è¿‡æœªçŸ¥ç®¡é“: {pipeline_name}")
                        remaining.remove(pipeline_name)
                        completed.add(pipeline_name)
                        continue
                    # å¦‚æœä¾èµ–ä¸­æœ‰ failed ä¸”å¼€å¯ soft_failï¼Œåˆ™è·³è¿‡æ­¤ä»»åŠ¡
                    dep_failed = any(
                        (dep in task_results and task_results[dep].get('status') == 'failed')
                        for dep in dependencies.get(pipeline_name, [])
                    )
                    if dep_failed and soft_fail:
                        logger.warning(f"â­ï¸ è½¯è·³è¿‡ä»»åŠ¡ {pipeline_name} å› ä¾èµ–å¤±è´¥: {dependencies.get(pipeline_name, [])}")
                        task_results[pipeline_name] = {
                            'status': 'skipped',
                            'reason': 'dependency_failed',
                            'dependencies': dependencies.get(pipeline_name, [])
                        }
                        remaining.remove(pipeline_name)
                        completed.add(pipeline_name)
                        continue
                    logger.info(f"ğŸ“‹ è°ƒåº¦Kedroç®¡é“ä»»åŠ¡: {pipeline_name}")
                    task_inputs = {}
                    # å‡†å¤‡ä¾èµ–è¾“å‡ºä½œä¸ºè¾“å…¥
                    for dep in dependencies.get(pipeline_name, []):
                        dep_outputs = pipeline_outputs.get(dep, {})
                        for oname, odata in dep_outputs.items():
                            task_inputs[oname] = odata
                    task_func = task_functions[pipeline_name]
                    future = task_func(**task_inputs)
                    scheduled[pipeline_name] = future
                    layer_futures[pipeline_name] = future
                    remaining.remove(pipeline_name)

                # Prefect 3.x ä»»åŠ¡è°ƒç”¨è¿”å›çš„å¯¹è±¡æœ¬èº«å°±æ˜¯æœ€ç»ˆç»“æœ(åŒæ­¥æ‰§è¡Œ task_runner=Concurrent/1)
                # è¿™é‡Œç›´æ¥ä½¿ç”¨è¿”å›å€¼ï¼›å¦‚æœªæ¥åˆ‡æ¢å¼‚æ­¥æ‰§è¡Œå¯å†åˆ¤æ–­ .result å¯ç”¨æ€§
                for pipeline_name, fut in layer_futures.items():
                    res = fut  # fut å·²æ˜¯ç»“æœå­—å…¸æˆ–æ™®é€šå¯¹è±¡
                    if isinstance(res, dict):
                        status = res.get('status')
                        if status == 'completed':
                            pipeline_outputs[pipeline_name] = res.get('outputs', {})
                            task_results[pipeline_name] = res
                            logger.info(f"âœ… Kedroç®¡é“ä»»åŠ¡å®Œæˆ: {pipeline_name}")
                        elif status == 'failed':
                            task_results[pipeline_name] = res
                            if soft_fail:
                                logger.warning(f"âš ï¸ Kedroç®¡é“ä»»åŠ¡å¤±è´¥(soft_failä¿ç•™ç»§ç»­) : {pipeline_name}")
                            else:
                                logger.error(f"âŒ Kedroç®¡é“ä»»åŠ¡å¤±è´¥: {pipeline_name}")
                        elif status == 'skipped':
                            task_results[pipeline_name] = res
                            logger.warning(f"âš ï¸ Kedroç®¡é“ä»»åŠ¡è·³è¿‡: {pipeline_name}")
                        else:
                            task_results[pipeline_name] = {'status': 'completed', 'raw': res}
                            logger.info(f"âœ… Kedroç®¡é“ä»»åŠ¡å®Œæˆ(æœªçŸ¥çŠ¶æ€åŒ…è£…): {pipeline_name}")
                    else:
                        task_results[pipeline_name] = {'status': 'completed', 'raw': res}
                        logger.info(f"âœ… Kedroç®¡é“ä»»åŠ¡å®Œæˆ(éå­—å…¸è¿”å›åŒ…è£…): {pipeline_name}")
                    completed.add(pipeline_name)

                layer_elapsed = _time.time() - layer_start
                layer_metrics.append({
                    'layer': layer_idx,
                    'tasks': current_layer,
                    'task_count': len(current_layer),
                    'elapsed_sec': round(layer_elapsed, 4)
                })
                logger.info(f"â±ï¸ å±‚ {layer_idx} è€—æ—¶ {layer_elapsed:.3f}s (ä»»åŠ¡æ•°: {len(current_layer)})")

            logger.info("ğŸ‰ æ··åˆå·¥ä½œæµæ‰§è¡Œå®Œæˆ (å¹¶è¡Œå±‚æ•°: %d)" % layer_idx)
            # æ±‡æ€»å¤±è´¥/è·³è¿‡ç»Ÿè®¡
            failed_count = sum(1 for v in task_results.values() if v.get('status') == 'failed')
            skipped_count = sum(1 for v in task_results.values() if v.get('status') == 'skipped')
            overall_status = 'completed'
            if failed_count > 0 and not soft_fail:
                overall_status = 'failed'
            elif failed_count > 0 and soft_fail:
                overall_status = 'completed_with_failures'

            return {
                'status': 'completed',
                'engine': 'prefect-kedro-hybrid',
                'task_results': task_results,
                'execution_order': execution_order,
                'total_pipelines': len(execution_order),
                'layers': layer_idx,
                'layer_metrics': layer_metrics,
                'failed_count': failed_count,
                'skipped_count': skipped_count,
                'overall_status': overall_status,
                'soft_fail': soft_fail
            }

        self.current_flow = hybrid_workflow
        self.logger.info(f"ğŸ”— æ··åˆå·¥ä½œæµæ„å»ºå®Œæˆ: {len(task_functions)} ä¸ªKedroç®¡é“ä»»åŠ¡")

        return hybrid_workflow

    def _build_node_level_flow(self, config: Dict[str, Any], orchestration: Dict[str, Any]) -> Callable:
        """æ„å»ºèŠ‚ç‚¹çº§ç²’åº¦çš„ Prefect Flowï¼šæ¯ä¸ª Kedro Node ä¸€ä¸ª Prefect ä»»åŠ¡"""
        from prefect import flow, task, get_run_logger
        from prefect.task_runners import ConcurrentTaskRunner
        # è·å–æ‰€æœ‰ç®¡é“ï¼ˆå½“å‰ä¸»è¦æ˜¯ __auto__ï¼‰
        pipelines = self.kedro_engine.pipelines
        if not pipelines:
            raise ValueError("æœªå‘ç°å·²æ„å»ºçš„ Kedro ç®¡é“")
        # ä»…æ”¯æŒå•ç®¡é“æˆ–åˆå¹¶å¤šä¸ªç®¡é“èŠ‚ç‚¹
        all_nodes = []
        for pname, p in pipelines.items():
            for n in p.nodes:
                all_nodes.append((pname, n))
        # æ„å»ºæ•°æ®é›† -> ç”Ÿäº§èŠ‚ç‚¹æ˜ å°„
        dataset_producer = {}
        node_inputs_map = {}
        node_outputs_map = {}
        for _, nd in all_nodes:
            outs = list(nd.outputs) if isinstance(nd.outputs, (list, tuple, set)) else ([nd.outputs] if nd.outputs else [])
            for o in outs:
                dataset_producer[o] = nd.name
            ins = list(nd.inputs) if isinstance(nd.inputs, (list, tuple, set)) else ([nd.inputs] if nd.inputs else [])
            node_inputs_map[nd.name] = ins
            node_outputs_map[nd.name] = outs
        # ä¾èµ–ï¼šèŠ‚ç‚¹ä¾èµ–äºæ‰€æœ‰å…¶è¾“å…¥æ•°æ®é›†çš„ç”Ÿäº§èŠ‚ç‚¹
        node_deps = {}
        for _, nd in all_nodes:
            deps = set()
            for din in node_inputs_map[nd.name]:
                if din in dataset_producer:
                    deps.add(dataset_producer[din])
            deps.discard(nd.name)
            node_deps[nd.name] = sorted(deps)
        soft_fail = bool(orchestration.get('soft_fail', False))
        task_runner_type = orchestration.get('task_runner', 'sequential').lower()
        max_workers = orchestration.get('max_workers', 4)
        if task_runner_type == 'concurrent':
            runner = ConcurrentTaskRunner(max_workers=max_workers)
            self.logger.info(f"âš™ï¸ (Node) ä½¿ç”¨å¹¶è¡Œä»»åŠ¡è¿è¡Œå™¨ ConcurrentTaskRunner(max_workers={max_workers})")
        else:
            runner = ConcurrentTaskRunner(max_workers=1)
            self.logger.info("âš™ï¸ (Node) ä½¿ç”¨é¡ºåºæ¨¡æ‹Ÿè¿è¡Œå™¨ (ConcurrentTaskRunner(max_workers=1))")

        # ä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ›å»º Prefect ä»»åŠ¡
        prefect_tasks = {}
        def make_task(node_name, kedro_node):
            @task(name=f"kedro_node_{node_name}", retries=orchestration.get('retry_count', 0), retry_delay_seconds=orchestration.get('retry_delay', 5), tags=["kedro-node"], timeout_seconds=orchestration.get('timeout', 900))
            def _exec_node(**up_inputs):  # noqa
                logger = get_run_logger()
                logger.info(f"ğŸš€ æ‰§è¡ŒKedroèŠ‚ç‚¹: {node_name}")
                # å‡†å¤‡è¾“å…¥ï¼ˆæŒ‰ kedro_node.inputs åºï¼‰
                args = []
                for in_name in node_inputs_map[node_name]:
                    if in_name in self.kedro_engine.global_catalog:
                        args.append(self.kedro_engine.global_catalog[in_name])
                    else:
                        # å°è¯•ä» data_catalog åŠ è½½ï¼ˆç¼“å­˜/æŒä¹…åŒ–ï¼‰
                        loaded = False
                        if self.kedro_engine.data_catalog and in_name in getattr(self.kedro_engine.data_catalog, '_data_sets', {}):  # type: ignore
                            try:
                                val = self.kedro_engine.data_catalog.load(in_name)  # type: ignore
                                self.kedro_engine.global_catalog[in_name] = val
                                args.append(val)
                                loaded = True
                            except Exception:
                                pass
                        if not loaded:
                            if in_name in up_inputs:
                                args.append(up_inputs[in_name])
                            else:
                                args.append(None)  # å ä½
                # --------------- ç¼“å­˜åˆ¤å®š ---------------
                cached = False
                signature_components = []
                try:
                    upstream_fps = []
                    for in_name, in_val in zip(node_inputs_map[node_name], args):
                        if in_val is not None:
                            upstream_fps.append(f"{in_name}:{self.kedro_engine._fingerprint_object(in_val)}")
                    signature_components.append("|".join(upstream_fps))
                    signature_components.append(node_name)
                    node_signature = "#".join(signature_components)
                    last_sig = self.kedro_engine.node_signatures.get(node_name)
                    outs_list = node_outputs_map[node_name]
                    if last_sig == node_signature and outs_list and all(o in self.kedro_engine.global_catalog for o in outs_list):
                        cached = True
                        result = tuple(self.kedro_engine.global_catalog[o] for o in outs_list) if len(outs_list) > 1 else (self.kedro_engine.global_catalog[outs_list[0]],)
                        logger.info(f"ğŸ§© (NodeCache) å‘½ä¸­: {node_name}")
                    else:
                        result = kedro_node.func(*args)
                        self.kedro_engine.node_signatures[node_name] = node_signature
                except Exception as e:
                    if soft_fail:
                        logger.warning(f"âš ï¸ èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥(soft_fail): {node_name}: {e}")
                        return { 'status': 'failed', 'node': node_name, 'error': str(e), 'soft_fail': True }
                    raise
                # å°†è¾“å‡ºå†™å…¥ data_catalog (MemoryDataset) ä»¥ä¾›ä¸‹æ¸¸
                outs = node_outputs_map[node_name]
                produced = {}
                # result ç»“æ„: å¤šè¾“å‡º -> tuple, å•è¾“å‡º -> (obj,) per wrapper
                if outs:
                    if isinstance(result, tuple):
                        out_values = list(result)
                    else:
                        out_values = [result]
                    for ds_name, val in zip(outs, out_values):
                        self.kedro_engine.global_catalog[ds_name] = val
                        try:
                            from kedro.io import MemoryDataset
                            if ds_name not in self.kedro_engine.data_catalog._data_sets:  # type: ignore
                                self.kedro_engine.data_catalog.add(ds_name, MemoryDataset())  # type: ignore
                            self.kedro_engine.data_catalog.save(ds_name, val)  # type: ignore
                        except Exception:
                            pass
                        produced[ds_name] = val
                logger.info(f"âœ… èŠ‚ç‚¹å®Œæˆ: {node_name} -> {list(produced.keys())} {'(cached)' if cached else ''}")
                return { 'status': 'completed', 'node': node_name, 'outputs': produced, 'cached': cached }
            return _exec_node
        for pname, kedro_node in all_nodes:
            prefect_tasks[kedro_node.name] = make_task(kedro_node.name, kedro_node)

        @flow(
            name=config.get('pipeline', {}).get('name', 'AStockèŠ‚ç‚¹çº§å·¥ä½œæµ'),
            description='Prefect èŠ‚ç‚¹çº§ç¼–æ’ Kedro pipeline',
            log_prints=True,
            task_runner=runner,
            retries=orchestration.get('retry_count', 0),
            retry_delay_seconds=orchestration.get('retry_delay', 5),
            timeout_seconds=orchestration.get('timeout', 1800)
        )
        def node_level_flow():
            logger = get_run_logger()
            logger.info("ğŸ¯ å¯åŠ¨èŠ‚ç‚¹çº§ Prefect-Kedro å·¥ä½œæµ")
            # æ‹“æ‰‘å±‚æ‰§è¡Œ
            remaining = set(prefect_tasks.keys())
            completed = set()
            deps = node_deps
            layer_metrics = []
            import time as _time
            layer_idx = 0
            results = {}
            dataset_to_value = {}
            cached_nodes = []
            while remaining:
                ready_nodes = [n for n in list(remaining) if all(d in completed for d in deps.get(n, []))]
                if not ready_nodes:
                    raise ValueError(f"å­˜åœ¨å¾ªç¯ä¾èµ–ï¼Œå‰©ä½™: {remaining}")
                layer_idx += 1
                logger.info(f"ğŸ§© Nodeå±‚ {layer_idx}: {ready_nodes}")
                start_layer = _time.time()
                futures = {}
                for n in ready_nodes:
                    # èšåˆå…¶ä¾èµ– outputs ä½œä¸º kwargs è¾“å…¥ï¼ˆä»…åŒ…å«æ•°æ®é›†ï¼‰
                    upstream_kwargs = {}
                    for din in node_inputs_map[n]:
                        if din in dataset_to_value:
                            upstream_kwargs[din] = dataset_to_value[din]
                        elif din in self.kedro_engine.global_catalog:
                            upstream_kwargs[din] = self.kedro_engine.global_catalog[din]
                    futures[n] = prefect_tasks[n](**upstream_kwargs)
                    remaining.remove(n)
                # æ”¶é›†ç»“æœ
                for n, fut in futures.items():
                    res = fut
                    if isinstance(res, dict):
                        results[n] = res
                        if res.get('status') == 'completed':
                            for ds, val in (res.get('outputs') or {}).items():
                                dataset_to_value[ds] = val
                            if res.get('cached'):
                                cached_nodes.append(n)
                        elif res.get('status') == 'failed' and not soft_fail:
                            logger.error(f"âŒ èŠ‚ç‚¹å¤±è´¥ç»ˆæ­¢: {n}")
                            raise RuntimeError(res.get('error'))
                    else:
                        results[n] = {'status': 'completed', 'raw': res}
                    completed.add(n)
                elapsed = _time.time() - start_layer
                layer_metrics.append({'layer': layer_idx, 'nodes': ready_nodes, 'node_count': len(ready_nodes), 'elapsed_sec': round(elapsed, 4)})
            overall_status = 'completed'
            failed_nodes = [n for n, r in results.items() if r.get('status') == 'failed']
            if failed_nodes and not soft_fail:
                overall_status = 'failed'
            elif failed_nodes and soft_fail:
                overall_status = 'completed_with_failures'
            # åˆå¹¶ KedroEngine lineage / metricsï¼ˆèŠ‚ç‚¹åä¸€è‡´æ—¶æŠ½å–ï¼‰
            lineage = getattr(self.kedro_engine, 'lineage', {})
            metrics = getattr(self.kedro_engine, 'node_metrics', {})
            return {
                'status': overall_status,
                'engine': 'prefect-kedro-node',
                'nodes': list(prefect_tasks.keys()),
                'node_results': results,
                'layers': layer_idx,
                'layer_metrics': layer_metrics,
                'failed_nodes': failed_nodes,
                'soft_fail': soft_fail,
                'cached_nodes': cached_nodes,
                'lineage': {k: v for k, v in lineage.items() if k in results},
                'node_metrics': {k: v for k, v in metrics.items() if k in results}
            }
        self.current_flow = node_level_flow
        self.logger.info(f"ğŸ”— èŠ‚ç‚¹çº§æ··åˆå·¥ä½œæµæ„å»ºå®Œæˆ: {len(prefect_tasks)} ä¸ªèŠ‚ç‚¹ä»»åŠ¡")
        return node_level_flow

    def execute_pipeline(self, execution_graph: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ··åˆç®¡é“ç³»ç»Ÿï¼ˆä¸ºExecuteManageræä¾›çš„æ¥å£ï¼‰

        Args:
            execution_graph: æ‰§è¡Œå›¾ï¼ˆå®é™…ä½¿ç”¨configä¸­çš„æ··åˆé…ç½®ï¼‰

        Returns:
            æ‰§è¡Œç»“æœ
        """
        try:
            config = self.execute_manager.ctx.config

            self.logger.info("ğŸš€ å¯åŠ¨Prefect-Kedroæ··åˆæ‰§è¡Œ")

            # æ„å»ºæ··åˆå·¥ä½œæµ
            hybrid_flow = self.build_hybrid_flow(config)

            # æ‰§è¡Œå·¥ä½œæµ
            start_time = datetime.now()

            # ä½¿ç”¨Prefectæ‰§è¡Œ
            flow_result = hybrid_flow()

            # ç­‰å¾…ç»“æœï¼ˆåŒæ­¥æ‰§è¡Œï¼‰
            if hasattr(flow_result, 'result'):
                result = flow_result.result()
            else:
                result = flow_result

            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()

            self.logger.info(f"âœ… æ··åˆç®¡é“æ‰§è¡Œå®Œæˆ ({execution_time:.2f}s)")

            # ç¡®ä¿è¿”å›æ ‡å‡†æ ¼å¼
            if isinstance(result, dict):
                result['execution_time'] = execution_time
                result['start_time'] = start_time.isoformat()
                result['end_time'] = end_time.isoformat()
                # Phase3: enrich with lineage & node metrics if available
                if hasattr(self.kedro_engine, 'lineage'):
                    result['lineage'] = self.kedro_engine.lineage
                if hasattr(self.kedro_engine, 'node_metrics'):
                    result['node_metrics'] = self.kedro_engine.node_metrics
                if hasattr(self.kedro_engine, 'dataset_producers'):
                    result['dataset_producers'] = self.kedro_engine.dataset_producers
                return result
            else:
                return {
                    'status': 'completed',
                    'engine': 'prefect-kedro-hybrid',
                    'result': result,
                    'execution_time': execution_time,
                    'start_time': start_time.isoformat(),
                    'end_time': end_time.isoformat()
                }

        except Exception as e:
            self.logger.error(f"âŒ æ··åˆç®¡é“æ‰§è¡Œå¤±è´¥: {e}")
            return {
                'status': 'failed',
                'engine': 'prefect-kedro-hybrid',
                'error': str(e)
            }

    def get_flow_status(self) -> Dict[str, Any]:
        """è·å–å·¥ä½œæµçŠ¶æ€"""
        return {
            'available': True,
            'current_flow': self.current_flow.__name__ if self.current_flow else None,
            'registered_tasks': len(self.task_registry),
            'kedro_integration': self.kedro_engine is not None,
            'kedro_pipelines': len(self.kedro_engine.pipelines) if self.kedro_engine else 0
        }

    def visualize_hybrid_workflow(self, output_path: str = None) -> str:
        """
        å¯è§†åŒ–æ··åˆå·¥ä½œæµ

        Args:
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„

        Returns:
            å¯è§†åŒ–æè¿°
        """
        if not self.current_flow:
            return "å·¥ä½œæµä¸å¯ç”¨æˆ–æœªæ„å»º"

        try:
            description = []
            description.append("Prefect-Kedroæ··åˆå·¥ä½œæµå¯è§†åŒ–")
            description.append("=" * 40)
            description.append(f"å·¥ä½œæµåç§°: {self.current_flow.__name__}")
            description.append(f"Kedroç®¡é“æ•°é‡: {len(self.kedro_engine.pipelines)}")
            description.append(f"Prefectä»»åŠ¡æ•°é‡: {len(self.task_registry)}")
            description.append("")

            # æ˜¾ç¤ºç®¡é“ç»“æ„
            if self.kedro_engine:
                config = self.execute_manager.ctx.config
                pipeline_configs = self.kedro_engine.parse_pipeline_config(config)
                execution_order = self.kedro_engine.get_pipeline_execution_order(pipeline_configs)

                description.append("æ‰§è¡Œé¡ºåº:")
                for i, pipeline_name in enumerate(execution_order, 1):
                    config_obj = pipeline_configs[pipeline_name]
                    description.append(f"{i}. {pipeline_name}")
                    description.append(f"   æè¿°: {config_obj.description}")
                    description.append(f"   èŠ‚ç‚¹æ•°: {len(config_obj.nodes)}")
                    if config_obj.depends_on:
                        description.append(f"   ä¾èµ–: {', '.join(config_obj.depends_on)}")
                    description.append("")

            viz_text = "\\n".join(description)

            # ä¿å­˜åˆ°æ–‡ä»¶
            if output_path:
                output_file = Path(output_path)
                output_file.parent.mkdir(parents=True, exist_ok=True)
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(viz_text)
                self.logger.info(f"ğŸ“ˆ æ··åˆå·¥ä½œæµå¯è§†åŒ–å·²ä¿å­˜: {output_path}")
                return str(output_path)
            else:
                return viz_text

        except Exception as e:
            self.logger.error(f"âŒ å·¥ä½œæµå¯è§†åŒ–å¤±è´¥: {e}")
            return f"å¯è§†åŒ–å¤±è´¥: {e}"

    def parse_pipeline_config(self, pipeline_config: Dict[str, Any]) -> Dict[str, Any]:
        """è§£æç®¡é“é…ç½®ï¼Œä¸ºPrefectä¼˜åŒ–"""
        try:
            parsed_config = {
                'name': pipeline_config.get('name', 'default_pipeline'),
                'description': pipeline_config.get('description', ''),
                'kedro_pipelines': pipeline_config.get('kedro_pipelines', {}),
                'prefect_settings': {
                    'flow_run_name': f"run_{pipeline_config.get('name', 'default')}_{int(datetime.now().timestamp())}",
                    'task_runner': 'sequential',  # å¯ä»¥é…ç½®ä¸ºå¹¶è¡Œrunner
                    'retries': pipeline_config.get('retries', 0),
                    'retry_delay': pipeline_config.get('retry_delay', 0)
                }
            }

            self.logger.info(f"âœ… Prefecté…ç½®è§£æå®Œæˆ: {parsed_config['name']}")
            return parsed_config

        except Exception as e:
            self.logger.error(f"âŒ Prefecté…ç½®è§£æå¤±è´¥: {e}")
            raise


